{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c50685-98c4-4de5-8a73-2a5e4937b383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7703c131-0c2d-42f9-969b-cae145f1a013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawitj/anaconda3/envs/aics/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dawitj/anaconda3/envs/aics/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_image.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m text_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Lions versus the Packers (2007).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath_to_image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your image path\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m image_input \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Preprocess image to fit ResNet input format\u001b[39;00m\n\u001b[1;32m    109\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m    110\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m256\u001b[39m),\n\u001b[1;32m    111\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mCenterCrop(\u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m    112\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m    113\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m    114\u001b[0m ])\n",
      "File \u001b[0;32m~/anaconda3/envs/aics/lib/python3.9/site-packages/PIL/Image.py:3465\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3462\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[1;32m   3464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3465\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3466\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_image.jpg'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Initialize BERT tokenizer and model for textual encoding\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize ResNet model for image encoding (using ResNet-50 here)\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "resnet_model = nn.Sequential(*list(resnet_model.children())[:-1])  # Remove final classification layer\n",
    "\n",
    "# Cross-Attention Layer\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_attention_heads)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, text_features, image_features):\n",
    "        # Cross-attention: text to image and image to text\n",
    "        text_features = text_features.unsqueeze(0)  # Add batch dimension\n",
    "        image_features = image_features.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Perform attention between text and image features\n",
    "        attn_output_text, _ = self.attention(text_features, image_features, image_features)\n",
    "        attn_output_image, _ = self.attention(image_features, text_features, text_features)\n",
    "        \n",
    "        # Combine the outputs (you can experiment with different strategies like sum, concat, etc.)\n",
    "        combined_output = attn_output_text + attn_output_image\n",
    "        combined_output = self.fc(combined_output)  # Feedforward layer after attention\n",
    "        return combined_output\n",
    "\n",
    "# Entity Disambiguation Head\n",
    "class EntityDisambiguationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_candidates):\n",
    "        super(EntityDisambiguationHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_candidates)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = F.relu(self.fc1(features))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Model combining Text and Image features\n",
    "class MultimodalEntityLinkingModel(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, num_candidates):\n",
    "        super(MultimodalEntityLinkingModel, self).__init__()\n",
    "        self.text_encoder = bert_model\n",
    "        self.image_encoder = resnet_model\n",
    "        self.cross_attention_layer = CrossAttentionLayer(hidden_size, num_attention_heads)\n",
    "        self.disambiguation_head = EntityDisambiguationHead(hidden_size, num_candidates)\n",
    "\n",
    "    def forward(self, text_input, image_input):\n",
    "        # Textual feature extraction using BERT\n",
    "        encoded_input = tokenizer(text_input, return_tensors='pt', padding=True, truncation=True)\n",
    "        text_output = self.text_encoder(**encoded_input).last_hidden_state  # shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Image feature extraction using ResNet\n",
    "        image_input = image_input.unsqueeze(0)  # Add batch dimension\n",
    "        image_features = self.image_encoder(image_input)  # shape: (batch_size, channels, 1, 1)\n",
    "        image_features = image_features.view(image_features.size(0), -1)  # Flatten to (batch_size, feature_size)\n",
    "\n",
    "        # Apply cross-attention to fuse text and image features\n",
    "        combined_features = self.cross_attention_layer(text_output, image_features)\n",
    "\n",
    "        # Disambiguate and predict the entity\n",
    "        entity_scores = self.disambiguation_head(combined_features.squeeze(0))  # Removing batch dimension\n",
    "        return entity_scores\n",
    "\n",
    "# Helper function for calculating loss and accuracy\n",
    "def calculate_loss_and_accuracy(model, text_input, image_input, correct_entity_index):\n",
    "    # Forward pass through the model\n",
    "    entity_scores = model(text_input, image_input)\n",
    "\n",
    "    # Compute the cross-entropy loss for entity disambiguation\n",
    "    labels = torch.tensor([correct_entity_index])  # The index of the correct entity in the candidate list\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(entity_scores, labels)\n",
    "    \n",
    "    # Get the predicted entity\n",
    "    predicted_entity = torch.argmax(entity_scores, dim=1)\n",
    "    accuracy = (predicted_entity == labels).float().mean()\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "# Example Usage\n",
    "# Initialize model parameters\n",
    "hidden_size = 768  # BERT hidden size\n",
    "num_attention_heads = 8\n",
    "num_candidates = 10  # Number of candidates for entity linking\n",
    "\n",
    "# Initialize the model\n",
    "model = MultimodalEntityLinkingModel(hidden_size, num_attention_heads, num_candidates)\n",
    "\n",
    "# Example input (text and image)\n",
    "text_input = \"The Lions versus the Packers (2007).\"\n",
    "image_path = \"path_to_image.jpg\"  # Replace with your image path\n",
    "image_input = Image.open(image_path)\n",
    "\n",
    "# Preprocess image to fit ResNet input format\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "image_input = preprocess(image_input)\n",
    "\n",
    "# Assume we have the index of the correct entity (for example purposes)\n",
    "correct_entity_index = 0\n",
    "\n",
    "# Calculate loss and accuracy\n",
    "loss, accuracy = calculate_loss_and_accuracy(model, text_input, image_input, correct_entity_index)\n",
    "print(f\"Loss: {loss.item()}, Accuracy: {accuracy.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f184bf0-91a8-439c-8034-b26d455de7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
