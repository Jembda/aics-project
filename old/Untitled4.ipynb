{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba63597-1f0e-4a90-bd8d-34e4d55b99c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b4fd8b6-5b1f-4a8f-b983-4576bf6336ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 139\u001b[0m\n\u001b[1;32m    136\u001b[0m model \u001b[38;5;241m=\u001b[39m MultimodalEntityLinkingModel(hidden_size, num_attention_heads, num_candidates)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Preprocess the dataset\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m preprocessed_data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Training Example\u001b[39;00m\n\u001b[1;32m    142\u001b[0m sample \u001b[38;5;241m=\u001b[39m preprocessed_data[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[25], line 105\u001b[0m, in \u001b[0;36mpreprocess_dataset\u001b[0;34m(dataset_path)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_dataset\u001b[39m(dataset_path):\n\u001b[1;32m    103\u001b[0m     preprocessed_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    106\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tqdm(data, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing Dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/aics/lib/python3.9/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths to dataset and image folder\n",
    "#DATASET_PATH = \"path_to_wikidiverse.json\"  # Replace with your dataset file\n",
    "#DATASET_PATH = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse.json\"\n",
    "DATASET_PATH = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_data\\images/062ce5e341a566a4208d801e53557538.jpg\"\n",
    "\n",
    "DATASET_PATH = r\"\"\n",
    "IMAGES_FOLDER = \"downloaded_images/\"\n",
    "os.makedirs(IMAGES_FOLDER, exist_ok=True)\n",
    "\n",
    "# Initialize BERT tokenizer and model for textual encoding\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize ResNet model for image encoding (using ResNet-50 here)\n",
    "resnet_model = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "resnet_model = nn.Sequential(*list(resnet_model.children())[:-1])  # Remove final classification layer\n",
    "\n",
    "# Image preprocessing pipeline\n",
    "image_preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Cross-Attention Layer\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_attention_heads)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, text_features, image_features):\n",
    "        text_features = text_features.unsqueeze(0)  # Add batch dimension\n",
    "        image_features = image_features.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        attn_output_text, _ = self.attention(text_features, image_features, image_features)\n",
    "        attn_output_image, _ = self.attention(image_features, text_features, text_features)\n",
    "\n",
    "        combined_output = attn_output_text + attn_output_image\n",
    "        combined_output = self.fc(combined_output)\n",
    "        return combined_output\n",
    "\n",
    "# Entity Disambiguation Head\n",
    "class EntityDisambiguationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_candidates):\n",
    "        super(EntityDisambiguationHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_candidates)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = F.relu(self.fc1(features))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Model combining Text and Image features\n",
    "class MultimodalEntityLinkingModel(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, num_candidates):\n",
    "        super(MultimodalEntityLinkingModel, self).__init__()\n",
    "        self.text_encoder = bert_model\n",
    "        self.image_encoder = resnet_model\n",
    "        self.cross_attention_layer = CrossAttentionLayer(hidden_size, num_attention_heads)\n",
    "        self.disambiguation_head = EntityDisambiguationHead(hidden_size, num_candidates)\n",
    "\n",
    "    def forward(self, text_input, image_input):\n",
    "        encoded_input = tokenizer(text_input, return_tensors='pt', padding=True, truncation=True)\n",
    "        text_output = self.text_encoder(**encoded_input).last_hidden_state\n",
    "\n",
    "        image_input = image_input.unsqueeze(0)  # Add batch dimension\n",
    "        image_features = self.image_encoder(image_input)\n",
    "        image_features = image_features.view(image_features.size(0), -1)\n",
    "\n",
    "        combined_features = self.cross_attention_layer(text_output, image_features)\n",
    "        entity_scores = self.disambiguation_head(combined_features.squeeze(0))\n",
    "        return entity_scores\n",
    "\n",
    "# Helper Functions for Preprocessing\n",
    "def download_image(image_url):\n",
    "    image_name = image_url.split(\"/\")[-1]\n",
    "    local_path = os.path.join(IMAGES_FOLDER, image_name)\n",
    "\n",
    "    if not os.path.exists(local_path):\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "    return local_path\n",
    "\n",
    "def preprocess_dataset(dataset_path):\n",
    "    preprocessed_data = []\n",
    "\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for item in tqdm(data, desc=\"Preprocessing Dataset\"):\n",
    "        text = item.get(\"caption\")\n",
    "        image_url = item.get(\"image_url\")\n",
    "        entities = item.get(\"entities\")\n",
    "\n",
    "        tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        try:\n",
    "            local_image_path = download_image(image_url)\n",
    "            image = Image.open(local_image_path).convert(\"RGB\")\n",
    "            image_tensor = image_preprocess(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_url}: {e}\")\n",
    "            continue\n",
    "\n",
    "        preprocessed_data.append({\n",
    "            \"text\": tokenized_text,\n",
    "            \"image\": image_tensor,\n",
    "            \"entities\": entities,\n",
    "        })\n",
    "\n",
    "    return preprocessed_data\n",
    "\n",
    "# Example Usage\n",
    "hidden_size = 768\n",
    "num_attention_heads = 8\n",
    "num_candidates = 10\n",
    "\n",
    "model = MultimodalEntityLinkingModel(hidden_size, num_attention_heads, num_candidates)\n",
    "\n",
    "# Preprocess the dataset\n",
    "preprocessed_data = preprocess_dataset(DATASET_PATH)\n",
    "\n",
    "# Training Example\n",
    "sample = preprocessed_data[0]\n",
    "text_input = sample[\"text\"]\n",
    "image_input = sample[\"image\"]\n",
    "correct_entity_index = 0  # Example correct index\n",
    "\n",
    "# Calculate loss and accuracy\n",
    "def calculate_loss_and_accuracy(model, text_input, image_input, correct_entity_index):\n",
    "    entity_scores = model(text_input, image_input)\n",
    "    labels = torch.tensor([correct_entity_index])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(entity_scores, labels)\n",
    "    predicted_entity = torch.argmax(entity_scores, dim=1)\n",
    "    accuracy = (predicted_entity == labels).float().mean()\n",
    "    return loss, accuracy\n",
    "\n",
    "loss, accuracy = calculate_loss_and_accuracy(model, text_input, image_input, correct_entity_index)\n",
    "print(f\"Loss: {loss.item()}, Accuracy: {accuracy.item()}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2184f53-6c4e-4bad-a6f0-ee0689ce9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Download and load\n",
    "dataset = load_dataset(\"wikidiverse\")\n",
    "print(dataset)\n",
    "\n",
    "# Preprocessing (example: tokenizing text)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoded_dataset = dataset.map(lambda x: tokenizer(x['text'], truncation=True, padding='max_length'), batched=True)\n",
    "\n",
    "# Save preprocessed data locally\n",
    "encoded_dataset.save_to_disk('./preprocessed_wikidiverse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9b381-1a6e-433f-87a8-45251bcfbd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "df = datasets.load_boston()\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d8232c-5b50-452a-b753-10b9a9cf8fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.utils.Bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a35216d-4e9b-4b53-9c7f-3ff0b445e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.DataFrame(data=df.data, columns=df.feature_names)\n",
    "boston['target'] = df.target\n",
    "\n",
    "botson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88277e2-80f3-406f-85ab-aafcc159f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Example to load a dataset from Hugging Face's Datasets library\n",
    "dataset = load_dataset(\"wikidiverse\")  # Replace \"wikidiverse\" with the actual dataset ID\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf21983-8a48-4380-bd3b-3359e59cd51a",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x7f0369abd090>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Download and process the image\u001b[39;00m\n\u001b[1;32m     16\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(image_url)\n\u001b[0;32m---> 17\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load the BERT tokenizer and process text\u001b[39;00m\n\u001b[1;32m     20\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/aics/lib/python3.9/site-packages/PIL/Image.py:3532\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3530\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[1;32m   3531\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[0;32m-> 3532\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7f0369abd090>"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import BertTokenizer, CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "# Sample data\n",
    "text = \"The Lions versus the Packers (2007).\"\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/0/06/DetroitLionsRunningPlay-2007.jpg\"\n",
    "entities = [\n",
    "    (\"Lions\", \"Organization\", 4, 9, \"https://en.wikipedia.org/wiki/Detroit_Lions\"),\n",
    "    (\"Packers\", \"Organization\", 21, 28, \"https://en.wikipedia.org/wiki/Green_Bay_Packers\")\n",
    "]\n",
    "\n",
    "# Download and process the image\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Load the BERT tokenizer and process text\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoded_text = tokenizer(text, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "# Load the CLIP processor and model for multimodal processing\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Process image using CLIP\n",
    "inputs = clip_processor(text=[text], images=image, return_tensors=\"pt\", padding=True)\n",
    "outputs = clip_model(**inputs)\n",
    "\n",
    "# Extract image features\n",
    "image_features = outputs.image_embeds\n",
    "\n",
    "# Example to combine text and image features (via concatenation or other techniques)\n",
    "# For simplicity, we concatenate the text and image features\n",
    "text_features = outputs.text_embeds\n",
    "\n",
    "# Combine text and image features (a simple concatenation for example)\n",
    "combined_features = torch.cat((text_features, image_features), dim=1)\n",
    "\n",
    "# Example entity prediction or further processing could go here\n",
    "print(\"Combined Text and Image Features:\", combined_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e583b9-2e22-4ef8-9a0c-462d795b12f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve image. Status code: 403\n",
      "b'<!DOCTYPE html>\\n<html lang=\"en\">\\n<meta charset=\"utf-8\">\\n<title>Wikimedia Error</title>\\n<style>\\n* { m'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/0/06/DetroitLionsRunningPlay-2007.jpg\"\n",
    "\n",
    "# Download the image\n",
    "response = requests.get(image_url)\n",
    "\n",
    "# Check the response status code\n",
    "if response.status_code == 200:\n",
    "    print(\"Image successfully retrieved!\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve image. Status code: {response.status_code}\")\n",
    "\n",
    "# Optionally, check the first few bytes of the response content\n",
    "print(response.content[:100])  # Print first 100 bytes to inspect the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e012dbd0-4223-4a4f-b2cc-5f967d3e21de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text/html; charset=utf-8\n"
     ]
    }
   ],
   "source": [
    "# Check the content type of the response\n",
    "print(response.headers['Content-Type'])\n",
    "\n",
    "# If it's an image, the content type should be something like \"image/jpeg\" or \"image/png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a629b-ee49-46ba-9d56-08c1e6d2c5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5ef6239-6422-4df9-b112-6548f29c2fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(eog:62948): EOG-WARNING **: 01:10:05.933: Couldn't load icon: Icon 'image-loading' not present in theme Adwaita\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/0/06/DetroitLionsRunningPlay-2007.jpg\"\n",
    "\n",
    "# Add headers to mimic a browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Make the request with headers\n",
    "response = requests.get(image_url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        image.show()  # Display the image\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening image: {e}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve image. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c447079-bb69-45aa-a650-6f110c7fb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('wikidiverse_data.csv')  # Change the filename if needed\n",
    "\n",
    "# Example of inspecting the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Preprocess text data (e.g., tokenization)\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoded_texts = tokenizer(df['text_column_name'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Save the preprocessed data locally\n",
    "torch.save(encoded_texts, 'encoded_texts.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109754d7-8d7a-41cd-82ac-594d427e57ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83c1561c-5cd4-4723-9905-08a350adca35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved: path_to_wikinewsImgs/062ce5e341a566a4208d801e53557538.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import re\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Your data (This should be a list of tuples containing the image URL and other data)\n",
    "data = [\n",
    "    [\"The Lions versus the Packers (2007).\", \"https://upload.wikimedia.org/wikipedia/commons/0/06/DetroitLionsRunningPlay-2007.jpg\", \"sports\"],\n",
    "    # Add more items here\n",
    "]\n",
    "\n",
    "# Define the base directory to save images\n",
    "base_img_dir = 'path_to_wikinewsImgs/'  # Define your image storage path\n",
    "\n",
    "# Ensure the base directory exists\n",
    "os.makedirs(base_img_dir, exist_ok=True)\n",
    "\n",
    "# Function to download and save the image\n",
    "def download_and_save_image(image_url, save_path):\n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        if response.status_code == 200:\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            img.save(save_path)\n",
    "            print(f\"Image saved: {save_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve image from: {image_url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image from {image_url}: {e}\")\n",
    "\n",
    "# Loop through each item in the data and process images\n",
    "for item in data:\n",
    "    image_url = item[1]  # Extract image URL\n",
    "    \n",
    "    # Extract image name and process it\n",
    "    m_img = image_url.split('/')[-1]\n",
    "    prefix = hashlib.md5(m_img.encode()).hexdigest()  # Generate unique hash prefix\n",
    "    suffix = re.sub(r'(\\S+(?=\\.(jpg|JPG|png|PNG|svg|SVG)))|(\\S+(?=\\.(jpeg|JPEG)))', '', m_img)\n",
    "    m_img = os.path.join(base_img_dir, prefix + suffix)\n",
    "    \n",
    "    # Ensure .svg or .SVG images are converted to .png\n",
    "    m_img = m_img.replace('.svg', '.png').replace('.SVG', '.png')\n",
    "    \n",
    "    # Download and save the image\n",
    "    download_and_save_image(image_url, m_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "864b4922-0c45-4879-a241-e6bf1623b518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved at C:\\Users\\Min Dator\\aics-project\\wikidiverse_data\\images/062ce5e341a566a4208d801e53557538.jpg\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Path to store images\n",
    "DATASET_PATH = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_data\\images\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    os.makedirs(DATASET_PATH)\n",
    "\n",
    "# Function to download and process the image\n",
    "def download_image(url):\n",
    "    try:\n",
    "        # Get the image content\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            m_img = url.split('/')[-1]\n",
    "            \n",
    "            # Create a unique file name using MD5 hash\n",
    "            prefix = hashlib.md5(m_img.encode()).hexdigest()\n",
    "            suffix = re.sub(r'(\\S+(?=\\.(jpg|JPG|png|PNG|svg|SVG)))|(\\S+(?=\\.(jpeg|JPEG)))', '', m_img)\n",
    "            \n",
    "            # Construct the file path for the image\n",
    "            file_path = os.path.join(DATASET_PATH, prefix + suffix)\n",
    "            file_path = file_path.replace('.svg', '.png').replace('.SVG', '.png')  # Replace .svg with .png\n",
    "\n",
    "            # Open the image and save it\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "            image.save(file_path)\n",
    "\n",
    "            print(f\"Image saved at {file_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve image. Status code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "\n",
    "# Example usage with data (replace 'data' with the actual dataset)\n",
    "data = [\n",
    "    [\"The Lions versus the Packers (2007).\", \"https://upload.wikimedia.org/wikipedia/commons/0/06/DetroitLionsRunningPlay-2007.jpg\", \"sports\", [\n",
    "        [\"Lions\", \"Organization\", 4, 9, \"https://en.wikipedia.org/wiki/Detroit_Lions\"],\n",
    "        [\"Packers\", \"Organization\", 21, 28, \"https://en.wikipedia.org/wiki/Green_Bay_Packers\"]\n",
    "    ]]\n",
    "]\n",
    "\n",
    "# Iterate over the data and download images\n",
    "for item in data:\n",
    "    image_url = item[1]  # Get the image URL (second element in the data)\n",
    "    download_image(image_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e91434-6fa8-41a1-888e-e32a15baf637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
