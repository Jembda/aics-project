{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31081c9b-4734-4018-95d0-6dd3a9d26bb6",
   "metadata": {},
   "source": [
    "# Overview\n",
    "An implementation of the WikiDiverse entity linking system using attention mechanisms, thereby leveraging a multimodal attention-based approach to fuse both text and image information for entity disambiguation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad71a973-9cdb-43ab-87c0-835d0f906d3b",
   "metadata": {},
   "source": [
    "# 1. Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cdb6fa-787b-4ae8-be08-255f0b31dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel\n",
    "import torch.nn.functional as F\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84476a-286f-43d4-9987-a048ea2fd7c2",
   "metadata": {},
   "source": [
    "# 2. Textual Encoder (BERT-based) and Image Encoder (CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45880c2b-a8f4-49f2-a1cb-e23f7b51a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT tokenizer and model for textual encoding\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize CLIP processor and model for image encoding\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f21724a-0a4b-4e4f-9cad-fabb27ded8da",
   "metadata": {},
   "source": [
    "# 3. Cross-Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb4fef1-886d-446c-a3b8-f429272709a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_attention_heads)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, text_features, image_features):\n",
    "        # Cross-attention: text to image and image to text\n",
    "        text_features = text_features.unsqueeze(0)  \n",
    "        image_features = image_features.unsqueeze(0)\n",
    "\n",
    "        # Perform attention between text and image features\n",
    "        attn_output_text, _ = self.attention(text_features, image_features, image_features)\n",
    "        attn_output_image, _ = self.attention(image_features, text_features, text_features)\n",
    "        \n",
    "        # Combine the outputs (you can experiment with different strategies like sum, concat, etc.)\n",
    "        combined_output = attn_output_text + attn_output_image\n",
    "        combined_output = self.fc(combined_output)  \n",
    "        return combined_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15205d24-c3af-4c4b-a838-d1cd7ba91d85",
   "metadata": {},
   "source": [
    "# 4. Entity Disambiguation Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da7b30-fbba-40b8-aebf-c771d934dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDisambiguationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_candidates):\n",
    "        super(EntityDisambiguationHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_candidates)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = F.relu(self.fc1(features))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e42259-ba6a-4f1d-b3fc-d125c3091cb6",
   "metadata": {},
   "source": [
    "# 4. Multimodal Entity Linking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11943774-8c72-4641-9838-c67171cbb9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalEntityLinkingModel(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, num_candidates):\n",
    "        super(MultimodalEntityLinkingModel, self).__init__()\n",
    "        self.text_encoder = bert_model\n",
    "        self.image_encoder = clip_model\n",
    "        self.cross_attention_layer = CrossAttentionLayer(hidden_size, num_attention_heads)\n",
    "        self.disambiguation_head = EntityDisambiguationHead(hidden_size, num_candidates)\n",
    "\n",
    "    def forward(self, text_input, image_input):\n",
    "        # Textual feature extraction using BERT\n",
    "        encoded_input = tokenizer(text_input, return_tensors='pt', padding=True, truncation=True)\n",
    "        text_output = self.text_encoder(**encoded_input).last_hidden_state  # shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Image feature extraction using CLIP\n",
    "        inputs = clip_processor(text=[text_input], images=image_input, return_tensors=\"pt\", padding=True)\n",
    "        outputs = self.image_encoder(**inputs)\n",
    "        image_features = outputs.image_embeds  # shape: (batch_size, embedding_dim)\n",
    "\n",
    "        # Apply cross-attention to fuse text and image features\n",
    "        combined_features = self.cross_attention_layer(text_output, image_features)\n",
    "\n",
    "        # Disambiguate and predict the entity\n",
    "        entity_scores = self.disambiguation_head(combined_features.squeeze(0))  # Removing batch dimension\n",
    "        return entity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e1fb4-9840-4518-af7f-dff4633a9d25",
   "metadata": {},
   "source": [
    "# 5. Help function for calculating Loss and accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e65a2-eea8-4133-81fd-4678d1921265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_and_accuracy(model, text_input, image_input, correct_entity_index):\n",
    "    # Forward pass through the model\n",
    "    entity_scores = model(text_input, image_input)\n",
    "\n",
    "    # Compute the cross-entropy loss for entity disambiguation\n",
    "    labels = torch.tensor([correct_entity_index])  # The index of the correct entity in the candidate list\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(entity_scores, labels)\n",
    "    \n",
    "    # Get the predicted entity\n",
    "    predicted_entity = torch.argmax(entity_scores, dim=1)\n",
    "    accuracy = (predicted_entity == labels).float().mean()\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf464f-7cec-48bd-923e-6f8f40900626",
   "metadata": {},
   "source": [
    "# 6. Iniitialize Model and Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f55b6e-1bda-4e67-9d05-04c568f8ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "hidden_size = 768  # BERT hidden size\n",
    "num_attention_heads = 8\n",
    "num_candidates = 10  # Number of candidates for entity linking\n",
    "\n",
    "# Initialize the model\n",
    "model = MultimodalEntityLinkingModel(hidden_size, num_attention_heads, num_candidates)\n",
    "\n",
    "# Input (text and image) ???\n",
    "# text_input = \"The Lions versus the Packers (2007).\"\n",
    "# image_path = \"path_to_image.jpg\"\n",
    "image_input = Image.open(image_path)\n",
    "\n",
    "# Assume we have the index of the correct entity (for example purposes)\n",
    "correct_entity_index = 0\n",
    "\n",
    "# Calculate loss and accuracy\n",
    "loss, accuracy = calculate_loss_and_accuracy(model, text_input, image_input, correct_entity_index)\n",
    "print(f\"Loss: {loss.item()}, Accuracy: {accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec08341-b689-4c41-9ddb-cfc574e462f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
