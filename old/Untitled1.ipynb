{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b8a5d7-237c-4f53-943d-b391661aba56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c7c128e-202f-4e5a-9168-db54831fb07d",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a963fe26-d999-4b27-b8b5-b10de7d2e933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 08:07:00.289982: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-02 08:07:04.571938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-02 08:07:08.308389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image path_to_image_1.jpg: [Errno 2] No such file or directory: 'path_to_image_1.jpg'\n",
      "Error loading image path_to_image_2.jpg: [Errno 2] No such file or directory: 'path_to_image_2.jpg'\n",
      "Error loading image path_to_image_3.jpg: [Errno 2] No such file or directory: 'path_to_image_3.jpg'\n",
      "Error loading image path_to_image_4.jpg: [Errno 2] No such file or directory: 'path_to_image_4.jpg'\n",
      "Error loading image path_to_image_5.jpg: [Errno 2] No such file or directory: 'path_to_image_5.jpg'\n",
      "Error loading image path_to_image_6.jpg: [Errno 2] No such file or directory: 'path_to_image_6.jpg'\n",
      "Error loading image path_to_image_7.jpg: [Errno 2] No such file or directory: 'path_to_image_7.jpg'\n",
      "Error loading image path_to_image_8.jpg: [Errno 2] No such file or directory: 'path_to_image_8.jpg'\n",
      "Error loading image path_to_image_9.jpg: [Errno 2] No such file or directory: 'path_to_image_9.jpg'\n",
      "Error loading image path_to_image_10.jpg: [Errno 2] No such file or directory: 'path_to_image_10.jpg'\n",
      "Error loading image path_to_image_11.jpg: [Errno 2] No such file or directory: 'path_to_image_11.jpg'\n",
      "Error loading image path_to_image_12.jpg: [Errno 2] No such file or directory: 'path_to_image_12.jpg'\n",
      "Error loading image path_to_image_13.jpg: [Errno 2] No such file or directory: 'path_to_image_13.jpg'\n",
      "Error loading image path_to_image_14.jpg: [Errno 2] No such file or directory: 'path_to_image_14.jpg'\n",
      "Error loading image path_to_image_15.jpg: [Errno 2] No such file or directory: 'path_to_image_15.jpg'\n",
      "Error loading image path_to_image_16.jpg: [Errno 2] No such file or directory: 'path_to_image_16.jpg'\n",
      "Error loading image path_to_image_17.jpg: [Errno 2] No such file or directory: 'path_to_image_17.jpg'\n",
      "Error loading image path_to_image_18.jpg: [Errno 2] No such file or directory: 'path_to_image_18.jpg'\n",
      "Error loading image path_to_image_19.jpg: [Errno 2] No such file or directory: 'path_to_image_19.jpg'\n",
      "Error loading image path_to_image_20.jpg: [Errno 2] No such file or directory: 'path_to_image_20.jpg'\n",
      "Error loading image path_to_image_21.jpg: [Errno 2] No such file or directory: 'path_to_image_21.jpg'\n",
      "Error loading image path_to_image_22.jpg: [Errno 2] No such file or directory: 'path_to_image_22.jpg'\n",
      "Error loading image path_to_image_23.jpg: [Errno 2] No such file or directory: 'path_to_image_23.jpg'\n",
      "Error loading image path_to_image_24.jpg: [Errno 2] No such file or directory: 'path_to_image_24.jpg'\n",
      "Error loading image path_to_image_25.jpg: [Errno 2] No such file or directory: 'path_to_image_25.jpg'\n",
      "Error loading image path_to_image_26.jpg: [Errno 2] No such file or directory: 'path_to_image_26.jpg'\n",
      "Error loading image path_to_image_27.jpg: [Errno 2] No such file or directory: 'path_to_image_27.jpg'\n",
      "Error loading image path_to_image_28.jpg: [Errno 2] No such file or directory: 'path_to_image_28.jpg'\n",
      "Error loading image path_to_image_29.jpg: [Errno 2] No such file or directory: 'path_to_image_29.jpg'\n",
      "Error loading image path_to_image_30.jpg: [Errno 2] No such file or directory: 'path_to_image_30.jpg'\n",
      "Error loading image path_to_image_31.jpg: [Errno 2] No such file or directory: 'path_to_image_31.jpg'\n",
      "Error loading image path_to_image_32.jpg: [Errno 2] No such file or directory: 'path_to_image_32.jpg'\n",
      "Error loading image path_to_image_33.jpg: [Errno 2] No such file or directory: 'path_to_image_33.jpg'\n",
      "Error loading image path_to_image_34.jpg: [Errno 2] No such file or directory: 'path_to_image_34.jpg'\n",
      "Error loading image path_to_image_35.jpg: [Errno 2] No such file or directory: 'path_to_image_35.jpg'\n",
      "Error loading image path_to_image_36.jpg: [Errno 2] No such file or directory: 'path_to_image_36.jpg'\n",
      "Error loading image path_to_image_37.jpg: [Errno 2] No such file or directory: 'path_to_image_37.jpg'\n",
      "Error loading image path_to_image_38.jpg: [Errno 2] No such file or directory: 'path_to_image_38.jpg'\n",
      "Error loading image path_to_image_39.jpg: [Errno 2] No such file or directory: 'path_to_image_39.jpg'\n",
      "Error loading image path_to_image_40.jpg: [Errno 2] No such file or directory: 'path_to_image_40.jpg'\n",
      "Data preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Load the WikiDiverse dataset\n",
    "dataset_path = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_dataset.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Extract columns\n",
    "image_paths1 = data['image_path1'].values\n",
    "image_paths2 = data['image_path2'].values\n",
    "text_data1 = data['text1'].values\n",
    "text_data2 = data['text2'].values\n",
    "labels = data['label'].values\n",
    "\n",
    "# Tokenize and pad text\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(np.concatenate([text_data1, text_data2]))\n",
    "\n",
    "text_sequences1 = tokenizer.texts_to_sequences(text_data1)\n",
    "text_sequences2 = tokenizer.texts_to_sequences(text_data2)\n",
    "\n",
    "text_input1 = pad_sequences(text_sequences1, maxlen=max_sequence_length)\n",
    "text_input2 = pad_sequences(text_sequences2, maxlen=max_sequence_length)\n",
    "\n",
    "# Image preprocessing function\n",
    "def preprocess_image(img_path):\n",
    "    try:\n",
    "        img = load_img(img_path, target_size=(224, 224))\n",
    "        img_array = img_to_array(img)\n",
    "        return preprocess_input(img_array)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {img_path}: {e}\")\n",
    "        return np.zeros((224, 224, 3))  # Fallback for missing images\n",
    "\n",
    "image_input1 = np.array([preprocess_image(img) for img in image_paths1])\n",
    "image_input2 = np.array([preprocess_image(img) for img in image_paths2])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_image1, X_test_image1, X_train_image2, X_test_image2, \\\n",
    "X_train_text1, X_test_text1, X_train_text2, X_test_text2, \\\n",
    "y_train, y_test = train_test_split(\n",
    "    image_input1, image_input2, text_input1, text_input2, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae992f-1361-4e5a-b75b-e0ad27f53a66",
   "metadata": {},
   "source": [
    "# 2. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b459c7-7420-4f2c-83d3-81236e3c0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# Image Sub-Network\n",
    "def create_image_model(input_shape=(224, 224, 3)):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    return models.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Text Sub-Network\n",
    "def create_text_model(input_shape=(max_sequence_length,)):\n",
    "    input_text = layers.Input(shape=input_shape)\n",
    "    x = layers.Embedding(input_dim=10000, output_dim=128, input_length=max_sequence_length)(input_text)\n",
    "    x = layers.LSTM(256, return_sequences=True)(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)  # Global max pooling for compact representation\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    return models.Model(inputs=input_text, outputs=x)\n",
    "\n",
    "# Cross-Attention Layer\n",
    "def cross_attention_layer(image_embedding, text_embedding):\n",
    "    \"\"\"\n",
    "    Cross-attention mechanism where text guides the focus on the image embedding.\n",
    "    \"\"\"\n",
    "    text_expanded = tf.expand_dims(text_embedding, axis=1)  # Expand for broadcasting\n",
    "    image_expanded = tf.expand_dims(image_embedding, axis=2)\n",
    "    \n",
    "    # Attention scores\n",
    "    attention_scores = tf.matmul(image_expanded, text_expanded)  # Shape: (batch_size, image_dim, text_dim)\n",
    "    attention_weights = tf.nn.softmax(attention_scores, axis=1)\n",
    "    \n",
    "    # Weighted sum\n",
    "    attended_image = tf.matmul(attention_weights, text_embedding)\n",
    "    return layers.Flatten()(attended_image)\n",
    "\n",
    "# Siamese Network\n",
    "def create_siamese_network(image_shape=(224, 224, 3), text_shape=(max_sequence_length,)):\n",
    "    # Image and Text Models\n",
    "    image_model = create_image_model(input_shape=image_shape)\n",
    "    text_model = create_text_model(input_shape=text_shape)\n",
    "    \n",
    "    # Inputs\n",
    "    input_image1 = layers.Input(shape=image_shape)\n",
    "    input_image2 = layers.Input(shape=image_shape)\n",
    "    input_text1 = layers.Input(shape=text_shape)\n",
    "    input_text2 = layers.Input(shape=text_shape)\n",
    "    \n",
    "    # Get Embeddings\n",
    "    image_embedding1 = image_model(input_image1)\n",
    "    image_embedding2 = image_model(input_image2)\n",
    "    text_embedding1 = text_model(input_text1)\n",
    "    text_embedding2 = text_model(input_text2)\n",
    "    \n",
    "    # Apply Cross-Attention\n",
    "    cross_embedding1 = cross_attention_layer(image_embedding1, text_embedding1)\n",
    "    cross_embedding2 = cross_attention_layer(image_embedding2, text_embedding2)\n",
    "    \n",
    "    # Combine Embeddings\n",
    "    combined_embedding1 = layers.concatenate([cross_embedding1, text_embedding1])\n",
    "    combined_embedding2 = layers.concatenate([cross_embedding2, text_embedding2])\n",
    "    \n",
    "    # Compute Similarity\n",
    "    distance = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([combined_embedding1, combined_embedding2])\n",
    "    output = layers.Dense(1, activation='sigmoid')(distance)\n",
    "    \n",
    "    # Build Model\n",
    "    model = models.Model(inputs=[input_image1, input_image2, input_text1, input_text2], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e8532b-25d9-4d0b-adbb-ae0325921aa2",
   "metadata": {},
   "source": [
    "# 3. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9ed5f20-fc0d-4848-8e3f-c2689796c3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 08:09:38.239809: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-01-02 08:09:51.201969: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-01-02 08:09:51.208344: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-01-02 08:09:51.210439: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-01-02 08:09:52.522003: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-01-02 08:09:52.524996: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-01-02 08:09:52.527804: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-01-02 08:09:52.795753: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-01-02 08:09:52.800417: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-01-02 08:09:52.804845: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 08:09:54.293358: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-01-02 08:09:54.296759: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-01-02 08:09:54.303655: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-01-02 08:09:55.256749: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-01-02 08:09:55.258562: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-01-02 08:09:55.261014: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/dawitj/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_45302/2090342518.py\", line 7, in contrastive_loss  *\n        loss = (y_true * square_pred) + ((1 - y_true) * tf.maximum(0.0, margin - tf.sqrt(square_pred + 1e-6))**2)\n\n    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int64 of argument 'x'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m siamese_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m), loss\u001b[38;5;241m=\u001b[39mcontrastive_loss, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train Model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m siamese_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     16\u001b[0m     [X_train_image1, X_train_image2, X_train_text1, X_train_text2], y_train,\n\u001b[1;32m     17\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Evaluate Model\u001b[39;00m\n\u001b[1;32m     21\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m siamese_model\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m     22\u001b[0m     [X_test_image1, X_test_image2, X_test_text1, X_test_text2], y_test\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileo0imaqub.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileao82lfuc.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__contrastive_loss\u001b[0;34m(y_true, y_pred, margin)\u001b[0m\n\u001b[1;32m     10\u001b[0m square_pred \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39msquare, (ag__\u001b[38;5;241m.\u001b[39mld(y_pred),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     11\u001b[0m square_true \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39msquare, (ag__\u001b[38;5;241m.\u001b[39mld(y_true),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 12\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(y_true) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(square_pred) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(y_true)) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mmaximum, (\u001b[38;5;241m0.0\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(margin) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39msqrt, (ag__\u001b[38;5;241m.\u001b[39mld(square_pred) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-06\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/dawitj/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_45302/2090342518.py\", line 7, in contrastive_loss  *\n        loss = (y_true * square_pred) + ((1 - y_true) * tf.maximum(0.0, margin - tf.sqrt(square_pred + 1e-6))**2)\n\n    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int64 of argument 'x'.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Loss Function\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    square_pred = tf.square(y_pred)\n",
    "    square_true = tf.square(y_true)\n",
    "    loss = (y_true * square_pred) + ((1 - y_true) * tf.maximum(0.0, margin - tf.sqrt(square_pred + 1e-6))**2)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Create and Compile Model\n",
    "siamese_model = create_siamese_network()\n",
    "siamese_model.compile(optimizer=Adam(learning_rate=1e-4), loss=contrastive_loss, metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "history = siamese_model.fit(\n",
    "    [X_train_image1, X_train_image2, X_train_text1, X_train_text2], y_train,\n",
    "    batch_size=16, epochs=10, validation_split=0.1\n",
    ")\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_accuracy = siamese_model.evaluate(\n",
    "    [X_test_image1, X_test_image2, X_test_text1, X_test_text2], y_test\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baac1c0-480c-42a3-84c4-d5a13be2b700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
