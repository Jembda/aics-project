{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40026f0c-3a35-4731-ab7b-5c621b4a4b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1097cac-3913-4822-a57a-7e533681359c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Min Dator\\\\aics-project\\\\wikidiverse.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m model \u001b[38;5;241m=\u001b[39m MultimodalEntityLinkingModel(hidden_size, num_attention_heads, num_candidates)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Preprocess the dataset\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m preprocessed_data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Example Training\u001b[39;00m\n\u001b[1;32m    139\u001b[0m sample \u001b[38;5;241m=\u001b[39m preprocessed_data[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[5], line 102\u001b[0m, in \u001b[0;36mpreprocess_dataset\u001b[0;34m(dataset_path)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_dataset\u001b[39m(dataset_path):\n\u001b[1;32m    100\u001b[0m     preprocessed_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    103\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tqdm(data, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing Dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/aics/lib/python3.9/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Min Dator\\\\aics-project\\\\wikidiverse.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Paths to dataset and image folder\n",
    "#DATASET_PATH = \"path_to_wikidiverse.json\"  \n",
    "DATASET_PATH = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse.json\"\n",
    "IMAGES_FOLDER = \"downloaded_images/\"\n",
    "os.makedirs(IMAGES_FOLDER, exist_ok=True)\n",
    "\n",
    "# Initialize BERT tokenizer and model for textual encoding\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize ResNet model for image encoding (using ResNet-50 here)\n",
    "resnet_model = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "resnet_model = nn.Sequential(*list(resnet_model.children())[:-1])  # Remove final classification layer\n",
    "\n",
    "# Image preprocessing pipeline\n",
    "image_preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Cross-Attention Layer\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_attention_heads)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, text_features, image_features):\n",
    "        text_features = text_features.unsqueeze(0)  # Add batch dimension\n",
    "        image_features = image_features.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        attn_output_text, _ = self.attention(text_features, image_features, image_features)\n",
    "        attn_output_image, _ = self.attention(image_features, text_features, text_features)\n",
    "\n",
    "        combined_output = attn_output_text + attn_output_image\n",
    "        combined_output = self.fc(combined_output)\n",
    "        return combined_output\n",
    "\n",
    "# Entity Disambiguation Head\n",
    "class EntityDisambiguationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_candidates):\n",
    "        super(EntityDisambiguationHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_candidates)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = F.relu(self.fc1(features))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Model combining Text and Image features\n",
    "class MultimodalEntityLinkingModel(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, num_candidates):\n",
    "        super(MultimodalEntityLinkingModel, self).__init__()\n",
    "        self.text_encoder = bert_model\n",
    "        self.image_encoder = resnet_model\n",
    "        self.cross_attention_layer = CrossAttentionLayer(hidden_size, num_attention_heads)\n",
    "        self.disambiguation_head = EntityDisambiguationHead(hidden_size, num_candidates)\n",
    "\n",
    "    def forward(self, text_input, image_input):\n",
    "        encoded_input = tokenizer(text_input, return_tensors='pt', padding=True, truncation=True)\n",
    "        text_output = self.text_encoder(**encoded_input).last_hidden_state\n",
    "\n",
    "        image_input = image_input.unsqueeze(0)  # Add batch dimension\n",
    "        image_features = self.image_encoder(image_input)\n",
    "        image_features = image_features.view(image_features.size(0), -1)\n",
    "\n",
    "        combined_features = self.cross_attention_layer(text_output, image_features)\n",
    "        entity_scores = self.disambiguation_head(combined_features.squeeze(0))\n",
    "        return entity_scores\n",
    "\n",
    "# Helper Functions for Preprocessing\n",
    "def download_image(image_url):\n",
    "    image_name = image_url.split(\"/\")[-1]\n",
    "    local_path = os.path.join(IMAGES_FOLDER, image_name)\n",
    "\n",
    "    if not os.path.exists(local_path):\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "    return local_path\n",
    "\n",
    "def preprocess_dataset(dataset_path):\n",
    "    preprocessed_data = []\n",
    "\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for item in tqdm(data, desc=\"Preprocessing Dataset\"):\n",
    "        text = item.get(\"caption\")\n",
    "        image_url = item.get(\"image_url\")\n",
    "        entities = item.get(\"entities\")\n",
    "\n",
    "        tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        try:\n",
    "            local_image_path = download_image(image_url)\n",
    "            image = Image.open(local_image_path).convert(\"RGB\")\n",
    "            image_tensor = image_preprocess(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_url}: {e}\")\n",
    "            continue\n",
    "\n",
    "        preprocessed_data.append({\n",
    "            \"text\": tokenized_text,\n",
    "            \"image\": image_tensor,\n",
    "            \"entities\": entities,\n",
    "        })\n",
    "\n",
    "    return preprocessed_data\n",
    "\n",
    "# Example Usage\n",
    "hidden_size = 768\n",
    "num_attention_heads = 8\n",
    "num_candidates = 10\n",
    "\n",
    "model = MultimodalEntityLinkingModel(hidden_size, num_attention_heads, num_candidates)\n",
    "\n",
    "# Preprocess the dataset\n",
    "preprocessed_data = preprocess_dataset(DATASET_PATH)\n",
    "\n",
    "# Example Training\n",
    "sample = preprocessed_data[0]\n",
    "text_input = sample[\"text\"]\n",
    "image_input = sample[\"image\"]\n",
    "correct_entity_index = 0  # Example correct index\n",
    "\n",
    "# Calculate loss and accuracy\n",
    "def calculate_loss_and_accuracy(model, text_input, image_input, correct_entity_index):\n",
    "    entity_scores = model(text_input, image_input)\n",
    "    labels = torch.tensor([correct_entity_index])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(entity_scores, labels)\n",
    "    predicted_entity = torch.argmax(entity_scores, dim=1)\n",
    "    accuracy = (predicted_entity == labels).float().mean()\n",
    "    return loss, accuracy\n",
    "\n",
    "loss, accuracy = calculate_loss_and_accuracy(model, text_input, image_input, correct_entity_index)\n",
    "print(f\"Loss: {loss.item()}, Accuracy: {accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5304d1-44ec-47ee-b8c1-ee25a78233fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacad24-632b-4762-941d-e88ecb86812d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d460aa-ae2b-4da0-8b14-2368fbd24b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54382e-4450-4269-bcc9-4f5c5ef9fad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
