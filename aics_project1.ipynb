{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd4516a-7cc7-47fd-b481-597c06f239b5",
   "metadata": {},
   "source": [
    "# An overview\n",
    "Siamese networks consist of two identical sub-networks that share weights and learn to compute the similarity between two input samples. The goal is to learn embeddings such that similar inputs are close in the embedding space, while dissimilar inputs are far apart. For the WikiDiverse dataset, where we have image-caption pairs, we can build a Siamese network that processes text and image data (or just one modality like text or image) and learns to compute similarity between two entities from the knowledge base.\n",
    "* Siamese Network Structure: Two identical sub-networks that compute embeddings for input pairs and learn their similarity\n",
    "* Application: For WikiDiverse, compute similarity between image-caption pairs to link knowledge-base entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47d21e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import Compose, ColorJitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3424641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "# 1. Dataset Path and Hyperparameter\n",
    "#-----------------------------------\n",
    "DATASET_PATH = r'C:\\Users\\Min Dator\\aics-project\\wikidiverse_w_cands\\wikidiverse_w_cands'\n",
    "IMAGE_DIR = os.path.join(DATASET_PATH, \"wikinewsImgs\")\n",
    "JSON_PATH = os.path.join(DATASET_PATH, \"train_w_10cands.json\")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "EMBED_DIM = 256\n",
    "VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb99d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# 2. Helper Functions\n",
    "#-------------------------------\n",
    "# Helper Functions to generate the local image path:\n",
    "def get_image_path(url, img_dir):\n",
    "    filename = url.split('/')[-1]\n",
    "    prefix = hashlib.md5(filename.encode()).hexdigest()\n",
    "    suffix = re.sub(r'(\\S+(?=\\.(jpg|jpeg|png|svg)))', '', filename, flags=re.IGNORECASE)\n",
    "    #suffix = filename.replace('.svg', '.png') if filename.lower().endswith('.svg') else filename\n",
    "    local_path = os.path.join(img_dir, f\"{prefix}{suffix}\".replace('.svg', '.png'))\n",
    "    return local_path\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(json_path, img_dir):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_data = []\n",
    "    for item in data:\n",
    "        # Extract fields from the dataset\n",
    "        sentence = item[0]\n",
    "        img_url = item[1]\n",
    "        mention = item[2]\n",
    "        mention_type = item[3]\n",
    "        left_context = item[4]\n",
    "        right_context = item[5]\n",
    "        entity_url = item[6]\n",
    "\n",
    "        # Generate the local image path\n",
    "        img_path = get_image_path(img_url, img_dir)\n",
    "\n",
    "        # Add processed data\n",
    "        processed_data.append({\n",
    "            'sentence': sentence,\n",
    "            'mention': mention,\n",
    "            'mention_type': mention_type,\n",
    "            'left_context': left_context,\n",
    "            'right_context': right_context,\n",
    "            'entity_url': entity_url,\n",
    "            'img_path': img_path\n",
    "        })\n",
    "    return processed_data\n",
    "\n",
    "#img_dir = r\"C:\\Users\\Min Dator\\aics-project\\wikinewsImgs\"\n",
    "#json_path = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_w_cands\\wikidiverse_w_cands\\train_w_10cands.json\"\n",
    "#dataset = load_dataset(json_path, img_dir)\n",
    "# Print a sample\n",
    "#print(dataset[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee400cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------\n",
    "# 3. WikiDiverse Dataset Class \n",
    "#-----------------------------\n",
    "class WikiDiverseDataset(Dataset):\n",
    "    def __init__(self, data, img_dir, transform=None, text_tokenizer=None):\n",
    "        \"\"\"\n",
    "        Initialize the WikiDiverseDataset.\n",
    "\n",
    "        Args:\n",
    "            data (list): List of processed data entries.\n",
    "            img_dir (str): Directory containing the images.\n",
    "            transform (callable, optional): Transformations to apply to images.\n",
    "            text_tokenizer (callable, optional): Tokenizer to apply to text data.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        try:\n",
    "            img1_path = os.path.join(self.img_dir, entry['img1'])\n",
    "            img2_path = os.path.join(self.img_dir, entry['img2'])\n",
    "            img1 = Image.open(img1_path).convert('RGB')\n",
    "            img2 = Image.open(img2_path).convert('RGB')\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Missing key {e} in dataset entry at index {idx}: {entry}\")\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        text1 = entry['text1']\n",
    "        text2 = entry['text2']\n",
    "        if self.text_tokenizer:\n",
    "            text1 = self.text_tokenizer(\n",
    "                text1, return_tensors=\"pt\", padding=True, truncation=True, max_length=128\n",
    "            )\n",
    "            text2 = self.text_tokenizer(\n",
    "                text2, return_tensors=\"pt\", padding=True, truncation=True, max_length=128\n",
    "            )\n",
    "\n",
    "        label = torch.tensor(entry['label', 0], dtype=torch.float32)\n",
    "\n",
    "        return img1, text1, img2, text2, label\n",
    "\n",
    "\n",
    "#Revision 2\n",
    "# class WikiDiverseDataset:\n",
    "#     def __init__(self, data, img_dir, transform=None, text_tokenizer=None):\n",
    "#         self.data = data\n",
    "#         self.img_dir = img_dir\n",
    "#         self.transform = transform\n",
    "#         self.text_tokenizer = text_tokenizer\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         entry1 = self.data[idx]\n",
    "#         entry2 = self.data[np.random.randint(len(self.data))]  # Randomly pick a second entry\n",
    "        \n",
    "#         # Extract paths and text\n",
    "#         img1_path = os.path.join(self.img_dir, entry1['img_path'])\n",
    "#         img2_path = os.path.join(self.img_dir, entry2['img_path'])\n",
    "#         text1 = entry1['mention']\n",
    "#         text2 = entry2['mention']\n",
    "        \n",
    "#         # Apply transformations\n",
    "#         from PIL import Image\n",
    "#         img1 = self.transform(Image.open(img1_path).convert('RGB')) if self.transform else img1_path\n",
    "#         img2 = self.transform(Image.open(img2_path).convert('RGB')) if self.transform else img2_path\n",
    "\n",
    "#         # Tokenize text\n",
    "#         if self.text_tokenizer:\n",
    "#             text1 = self.text_tokenizer(text1, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "#             text2 = self.text_tokenizer(text2, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "#         # Define label (1: similar, 0: dissimilar)\n",
    "#         label = 1 if entry1['mention'] == entry2['mention'] else 0\n",
    "\n",
    "#         return img1, text1, img2, text2, torch.tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5cdbf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# 4. Data Transformations \n",
    "#----------------------------\n",
    "transform = Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cb216b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# 5. Cross-Attention Mechanism\n",
    "#----------------------------\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, dropout=0.1):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return self.layer_norm(query + attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "269920f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "# 6. Sub-Networks for Image and Text\n",
    "#-----------------------------------\n",
    "class ImageSubNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=4):\n",
    "        super(ImageSubNetworkWithAttention, self).__init__()\n",
    "        base_model = models.resnet50(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.cross_attention = CrossAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x, text_features):\n",
    "        x = self.features(x).view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        if text_features is not None:\n",
    "            x = self.cross_attention(x.unsqueeze(1), text_features, text_features).squeeze(1)\n",
    "        return x\n",
    "\n",
    "class TextSubNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_heads=4):\n",
    "        super(TextSubNetworkWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.cross_attention = CrossAttention(hidden_dim, num_heads)\n",
    "\n",
    "    def forward(self, x, image_features):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        if image_features is not None:\n",
    "            hidden = self.cross_attention(hidden.unsqueeze(1), image_features, image_features).squeeze(1)\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c66ce6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "# 7. Siamese Network\n",
    "#------------------------------\n",
    "class SiameseNetworkWithCrossAttention(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SiameseNetworkWithCrossAttention, self).__init__()\n",
    "        self.image_net = ImageSubNetworkWithAttention()\n",
    "        self.text_net = TextSubNetworkWithAttention(vocab_size)\n",
    "\n",
    "    def forward(self, img1, img2, text1, text2):\n",
    "        img_embedding1 = self.image_net(img1, None)\n",
    "        img_embedding2 = self.image_net(img2, None)\n",
    "\n",
    "        text_embedding1 = self.text_net(text1, img_embedding1)\n",
    "        text_embedding2 = self.text_net(text2, img_embedding2)\n",
    "\n",
    "        combined_embedding1 = torch.cat([img_embedding1, text_embedding1], dim=1)\n",
    "        combined_embedding2 = torch.cat([img_embedding2, text_embedding2], dim=1)\n",
    "\n",
    "        return combined_embedding1, combined_embedding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a03464e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# 8. Contrastive Loss\n",
    "#-------------------------------\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                          label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26287679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "import numpy as np\n",
    "\n",
    "def train(model, device, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img1, text1, img2, text2, labels in loader:\n",
    "        # Move to device\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "        text1 = {key: val.to(device) for key, val in text1.items()}\n",
    "        text2 = {key: val.to(device) for key, val in text2.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(img1, img2, text1, text2)\n",
    "        loss = criterion(output1, output2, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for img1, text1, img2, text2, labels in loader:\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "            text1 = {key: val.to(device) for key, val in text1.items()}\n",
    "            text2 = {key: val.to(device) for key, val in text2.items()}\n",
    "            \n",
    "            output1, output2 = model(img1, img2, text1, text2)\n",
    "            similarity = F.cosine_similarity(output1, output2)\n",
    "            predictions.extend(similarity.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, (np.array(predictions) > 0.5).astype(int)) * 100\n",
    "    precision = precision_score(true_labels, (np.array(predictions) > 0.5).astype(int))\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    return accuracy\n",
    "    print(accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57f7ac32",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing key 'img1' in dataset entry at index 267: {'sentence': \"(From left) Harry Potter, Star Lord (from Marvel's Guardians of the Galaxy, Elsa (from Disney's Frozen) and Harley Quinn from DC/Warner Bros. Suicide Squad.\", 'mention': \"Marvel's\", 'mention_type': 'Organization', 'left_context': ['from', 'left', 'harry', 'potter', 'star', 'lord', 'from'], 'right_context': [' ', 'guardian', 'of', 'the', 'galaxy', 'elsa', 'from', 'disney', 'frozen', 'and', 'harley', 'quinn', 'from', 'dcwarner', 'bros', 'suicide', 'squad'], 'entity_url': 'https://en.wikipedia.org/wiki/Marvel_Comics', 'img_path': 'C:\\\\Users\\\\Min Dator\\\\aics-project\\\\wikidiverse_w_cands\\\\wikidiverse_w_cands\\\\wikinewsImgs\\\\a29bbfb533d09c5165b3dea9ea11d881.JPG'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 26\u001b[0m, in \u001b[0;36mWikiDiverseDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     img1_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, \u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     27\u001b[0m     img2_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'img1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m best_val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     val_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, device, val_loader)\n",
      "Cell \u001b[1;32mIn[40], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     24\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img1, text1, img2, text2, labels \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     26\u001b[0m     img1, text1, img2, text2, labels \u001b[38;5;241m=\u001b[39m img1\u001b[38;5;241m.\u001b[39mto(device), text1\u001b[38;5;241m.\u001b[39mto(device), img2\u001b[38;5;241m.\u001b[39mto(device), text2\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[30], line 31\u001b[0m, in \u001b[0;36mWikiDiverseDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m     img2 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img2_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in dataset entry at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     34\u001b[0m     img1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img1)\n",
      "\u001b[1;31mValueError\u001b[0m: Missing key 'img1' in dataset entry at index 267: {'sentence': \"(From left) Harry Potter, Star Lord (from Marvel's Guardians of the Galaxy, Elsa (from Disney's Frozen) and Harley Quinn from DC/Warner Bros. Suicide Squad.\", 'mention': \"Marvel's\", 'mention_type': 'Organization', 'left_context': ['from', 'left', 'harry', 'potter', 'star', 'lord', 'from'], 'right_context': [' ', 'guardian', 'of', 'the', 'galaxy', 'elsa', 'from', 'disney', 'frozen', 'and', 'harley', 'quinn', 'from', 'dcwarner', 'bros', 'suicide', 'squad'], 'entity_url': 'https://en.wikipedia.org/wiki/Marvel_Comics', 'img_path': 'C:\\\\Users\\\\Min Dator\\\\aics-project\\\\wikidiverse_w_cands\\\\wikidiverse_w_cands\\\\wikinewsImgs\\\\a29bbfb533d09c5165b3dea9ea11d881.JPG'}"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(JSON_PATH, IMAGE_DIR)\n",
    "train_data, val_test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(WikiDiverseDataset(train_data, IMAGE_DIR, transform, text_tokenizer=tokenizer), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(WikiDiverseDataset(val_data, IMAGE_DIR, transform, text_tokenizer=tokenizer), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(WikiDiverseDataset(test_data, IMAGE_DIR, transform, text_tokenizer=tokenizer), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CosineEmbeddingLoss()  # or ContrastiveLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Training and evaluation loop\n",
    "# best_val_accuracy = 0\n",
    "# for epoch in range(EPOCHS):\n",
    "#     train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "#     val_accuracy = evaluate(model, device, val_loader)\n",
    "#     scheduler.step(train_loss)  \n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "#     # Save the best model\n",
    "#     if val_accuracy > best_val_accuracy:\n",
    "#         best_val_accuracy = val_accuracy\n",
    "#         torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# # Test the model\n",
    "# model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "# test_accuracy = evaluate(model, device, test_loader)\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "best_val_accuracy = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_accuracy = evaluate(model, device, val_loader)\n",
    "    scheduler.step(train_loss)  # Adjust learning rate based on training loss\n",
    "\n",
    "    # Log results\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(f\"New best model saved with Val Accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "# Test the best model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "test_accuracy = evaluate(model, device, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07f9d5ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing key 'img1' in dataset entry at index 2855: {'sentence': 'Location of Helmand Province within Afghanistan where Daniele Mastrogiacomo was taken hostage.', 'mention': 'Helmand Province', 'mention_type': 'Location', 'left_context': ['location', 'of'], 'right_context': [' ', 'within', 'afghanistan', 'where', 'daniele', 'mastrogiacomo', 'be', 'take', 'hostage'], 'entity_url': 'https://en.wikipedia.org/wiki/Helmand_Province', 'img_path': 'C:\\\\Users\\\\Min Dator\\\\aics-project\\\\wikidiverse_w_cands\\\\wikidiverse_w_cands\\\\wikinewsImgs\\\\6a9d4a5a25582015c8d9a65c8afe365d.png'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 26\u001b[0m, in \u001b[0;36mWikiDiverseDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     img1_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, \u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     27\u001b[0m     img2_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'img1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m criterion \u001b[38;5;241m=\u001b[39m ContrastiveLoss()\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m---> 77\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m     val_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, device, val_loader)\n",
      "Cell \u001b[1;32mIn[40], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     24\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img1, text1, img2, text2, labels \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     26\u001b[0m     img1, text1, img2, text2, labels \u001b[38;5;241m=\u001b[39m img1\u001b[38;5;241m.\u001b[39mto(device), text1\u001b[38;5;241m.\u001b[39mto(device), img2\u001b[38;5;241m.\u001b[39mto(device), text2\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[30], line 31\u001b[0m, in \u001b[0;36mWikiDiverseDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m     img2 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img2_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in dataset entry at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     34\u001b[0m     img1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img1)\n",
      "\u001b[1;31mValueError\u001b[0m: Missing key 'img1' in dataset entry at index 2855: {'sentence': 'Location of Helmand Province within Afghanistan where Daniele Mastrogiacomo was taken hostage.', 'mention': 'Helmand Province', 'mention_type': 'Location', 'left_context': ['location', 'of'], 'right_context': [' ', 'within', 'afghanistan', 'where', 'daniele', 'mastrogiacomo', 'be', 'take', 'hostage'], 'entity_url': 'https://en.wikipedia.org/wiki/Helmand_Province', 'img_path': 'C:\\\\Users\\\\Min Dator\\\\aics-project\\\\wikidiverse_w_cands\\\\wikidiverse_w_cands\\\\wikinewsImgs\\\\6a9d4a5a25582015c8d9a65c8afe365d.png'}"
     ]
    }
   ],
   "source": [
    "#-----------------------------\n",
    "# 9. Training and Evaluation\n",
    "#-----------------------------\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Define transformations for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def train(model, device, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img1, text1, img2, text2, labels in loader:\n",
    "        img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(img1, img2, text1, text2)\n",
    "        loss = criterion(output1, output2, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "#def train(model, device, loader, optimizer, criterion, scheduler):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for img1, text1, img2, text2, labels in loader:\n",
    "#         img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output1, output2 = model(img1, img2, text1, text2)\n",
    "#         loss = criterion(output1, output2, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step(loss)\n",
    "#         total_loss += loss.item()\n",
    "#     return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for img1, text1, img2, text2, labels in loader:\n",
    "            img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "            output1, output2 = model(img1, img2, text1, text2)\n",
    "            similarity = F.cosine_similarity(output1, output2)\n",
    "            predictions.extend(similarity.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    return accuracy_score(true_labels, (np.array(predictions) > 0.5).astype(int)) * 100\n",
    "\n",
    "# Main Execution\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = load_dataset(JSON_PATH, IMAGE_DIR)\n",
    "train_data, val_test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(WikiDiverseDataset(train_data, IMAGE_DIR, transform, text_tokenizer=tokenizer), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(WikiDiverseDataset(val_data, IMAGE_DIR, transform, text_tokenizer=tokenizer), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(WikiDiverseDataset(test_data, IMAGE_DIR, transform, text_tokenizer=tokenizer), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = ContrastiveLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}\")\n",
    "    val_accuracy = evaluate(model, device, val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "test_accuracy = evaluate(model, device, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd5ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--------------------\n",
    "# # evaluation metrics\n",
    "# #--------------------\n",
    "\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "# metrics['accuracy'] = accuracy_score(true_labels, (np.array(predictions) > 0.5).astype(int)) * 100\n",
    "\n",
    "# def evaluate(model, device, loader):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     true_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for img1, text1, img2, text2, labels in loader:\n",
    "#             img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "#             output1, output2 = model(img1, img2, text1, text2)\n",
    "#             similarity = F.cosine_similarity(output1, output2)\n",
    "#             predictions.extend(similarity.cpu().numpy())\n",
    "#             true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "#     return np.array(predictions), np.array(true_labels)\n",
    "\n",
    "# # After running the evaluation\n",
    "# test_predictions, test_true_labels = evaluate(model, device, test_loader)\n",
    "\n",
    "# # Print results\n",
    "# print(\"Test Metrics:\")\n",
    "# print(f\"Accuracy: {accuracy_score(test_true_labels, (test_predictions > 0.5).astype(int)) * 100:.4f}\")\n",
    "# print(f\"AUC: {roc_auc_score(test_true_labels, test_predictions):.4f}\")\n",
    "# print(f\"Precision: {precision_score(test_true_labels, (test_predictions > 0.5).astype(int)):.4f}\")\n",
    "# print(f\"Recall: {recall_score(test_true_labels, (test_predictions > 0.5).astype(int)):.4f}\")\n",
    "# print(f\"F1 Score: {f1_score(test_true_labels, (test_predictions > 0.5).astype(int)):.4f}\")\n",
    "\n",
    "# # Plot confusion matrix\n",
    "# plot_confusion_matrix(test_true_labels, (test_predictions > 0.5).astype(int))\n",
    "\n",
    "# # Plot ROC curve\n",
    "# plot_roc_curve(test_true_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9921e3b",
   "metadata": {},
   "source": [
    "### END ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7746a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2536f5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab067d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, device, loader, optimizer, criterion):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for img1, text1, img2, text2, labels in loader:\n",
    "#         # Move to device\n",
    "#         img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "#         text1 = {key: val.to(device) for key, val in text1.items()}\n",
    "#         text2 = {key: val.to(device) for key, val in text2.items()}\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         output1, output2 = model(img1, img2, text1, text2)\n",
    "#         loss = criterion(output1, output2, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     return total_loss / len(loader)\n",
    "# import numpy as np\n",
    "# def evaluate(model, device, loader):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     true_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for img1, text1, img2, text2, labels in loader:\n",
    "#             img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "#             text1 = {key: val.to(device) for key, val in text1.items()}\n",
    "#             text2 = {key: val.to(device) for key, val in text2.items()}\n",
    "            \n",
    "#             output1, output2 = model(img1, img2, text1, text2)\n",
    "#             similarity = F.cosine_similarity(output1, output2)\n",
    "#             predictions.extend(similarity.cpu().numpy())\n",
    "#             true_labels.extend(labels.cpu().numpy())\n",
    "#     return accuracy_score(true_labels, (np.array(predictions) > 0.5).astype(int)) * 100\n",
    "# print(precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f379a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #-----------------------------\n",
    "# # 9. Training and Evaluation\n",
    "# #-----------------------------\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "# # Initialize tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# #scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# # Define transformations for images\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "\n",
    "# def train(model, device, loader, optimizer, criterion):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for img1, text1, img2, text2, labels in loader:\n",
    "#         img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output1, output2 = model(img1, img2, text1, text2)\n",
    "#         loss = criterion(output1, output2, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     return total_loss / len(loader)\n",
    "\n",
    "# #def train(model, device, loader, optimizer, criterion, scheduler):\n",
    "# #     model.train()\n",
    "# #     total_loss = 0\n",
    "# #     for img1, text1, img2, text2, labels in loader:\n",
    "# #         img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "# #         optimizer.zero_grad()\n",
    "# #         output1, output2 = model(img1, img2, text1, text2)\n",
    "# #         loss = criterion(output1, output2, labels)\n",
    "# #         loss.backward()\n",
    "# #         optimizer.step()\n",
    "# #         scheduler.step(loss)\n",
    "# #         total_loss += loss.item()\n",
    "# #     return total_loss / len(loader)\n",
    "\n",
    "# def evaluate(model, device, loader):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     true_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for img1, text1, img2, text2, labels in loader:\n",
    "#             img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "#             output1, output2 = model(img1, img2, text1, text2)\n",
    "#             similarity = F.cosine_similarity(output1, output2)\n",
    "#             predictions.extend(similarity.cpu().numpy())\n",
    "#             true_labels.extend(labels.cpu().numpy())\n",
    "#     return accuracy_score(true_labels, (np.array(predictions) > 0.5).astype(int)) * 100\n",
    "\n",
    "# # Main Execution\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# dataset = load_dataset(JSON_PATH, IMAGE_DIR)\n",
    "# train_data, val_test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "# val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# train_loader = DataLoader(WikiDiverseDataset(train_data, IMAGE_DIR, transform, text_tokenizer=tokenizer), batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_loader = DataLoader(WikiDiverseDataset(val_data, IMAGE_DIR, transform, text_tokenizer=tokenizer), batch_size=BATCH_SIZE, shuffle=False)\n",
    "# test_loader = DataLoader(WikiDiverseDataset(test_data, IMAGE_DIR, transform, text_tokenizer=tokenizer), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# #--------\n",
    "# # train_loader = DataLoader(\n",
    "# #     WikiDiverseDataset(train_data, IMAGE_DIR, transform=transform, text_tokenizer=tokenizer),\n",
    "# #     batch_size=BATCH_SIZE,\n",
    "# #     shuffle=True,\n",
    "# # )\n",
    "# # val_loader = DataLoader(\n",
    "# #     WikiDiverseDataset(val_data, IMAGE_DIR, transform=transform, text_tokenizer=tokenizer),\n",
    "# #     batch_size=BATCH_SIZE,\n",
    "# #     shuffle=False,\n",
    "# # )\n",
    "# # test_loader = DataLoader(\n",
    "# #     WikiDiverseDataset(test_data, IMAGE_DIR, transform=transform, text_tokenizer=tokenizer),\n",
    "# #     batch_size=BATCH_SIZE,\n",
    "# #     shuffle=False,\n",
    "# # )\n",
    "\n",
    "# #-----------\n",
    "\n",
    "# # # Create datasets\n",
    "# # train_dataset = WikiDiverseDataset(\n",
    "# #     json_path=\"C:/Users/Min Dator/aics-project/wikidiverse_w_cands/wikidiverse_w_cands/train_w_10cands.json\",\n",
    "# #     image_dir=\"C:/Users/Min Dator/aics-project/wikinewsImgs\",\n",
    "# #     tokenizer=tokenizer,\n",
    "# #     transform=transform\n",
    "# # )\n",
    "\n",
    "# # valid_dataset = WikiDiverseDataset(\n",
    "# #     json_path=\"C:/Users/Min Dator/aics-project/wikidiverse_w_cands/wikidiverse_w_cands/valid_w_10cands.json\",\n",
    "# #     image_dir=\"C:/Users/Min Dator/aics-project/wikinewsImgs\",\n",
    "# #     tokenizer=tokenizer,\n",
    "# #     transform=transform\n",
    "# # )\n",
    "\n",
    "# # test_dataset = WikiDiverseDataset(\n",
    "# #     json_path=\"C:/Users/Min Dator/aics-project/wikidiverse_w_cands/wikidiverse_w_cands/test_w_10cands.json\",\n",
    "# #     image_dir=\"C:/Users/Min Dator/aics-project/wikinewsImgs\",\n",
    "# #     tokenizer=tokenizer,\n",
    "# #     transform=transform\n",
    "# # )\n",
    "\n",
    "# # # Create DataLoaders\n",
    "# # train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# # valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "# # test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# criterion = ContrastiveLoss()\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}\")\n",
    "#     val_accuracy = evaluate(model, device, valid_loader)\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# test_accuracy = evaluate(model, device, test_loader)\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8b7d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import numpy as np\n",
    "# from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "# # metrics['accuracy'] = accuracy_score(true_labels, (np.array(predictions) > 0.5).astype(int)) * 100\n",
    "\n",
    "\n",
    "# def evaluate(model, device, loader):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     true_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for img1, text1, img2, text2, labels in loader:\n",
    "#             img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "#             output1, output2 = model(img1, img2, text1, text2)\n",
    "#             similarity = F.cosine_similarity(output1, output2)\n",
    "#             predictions.extend(similarity.cpu().numpy())\n",
    "#             true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "#     return np.array(predictions), np.array(true_labels)\n",
    "\n",
    "# # After running the evaluation\n",
    "# test_predictions, test_true_labels = evaluate(model, device, test_loader)\n",
    "\n",
    "# # Print results\n",
    "# print(\"Test Metrics:\")\n",
    "# print(f\"Accuracy: {accuracy_score(test_true_labels, (test_predictions > 0.5).astype(int)) * 100:.4f}\")\n",
    "# #print(f\"AUC: {roc_auc_score(test_true_labels, test_predictions):.4f}\")\n",
    "# print(f\"Precision: {precision_score(test_true_labels, (test_predictions > 0.5).astype(int)):.4f}\")\n",
    "# print(f\"Recall: {recall_score(test_true_labels, (test_predictions > 0.5).astype(int)):.4f}\")\n",
    "# print(f\"F1 Score: {f1_score(test_true_labels, (test_predictions > 0.5).astype(int)):.4f}\")\n",
    "\n",
    "# # Plot confusion matrix\n",
    "# #plot_confusion_matrix(test_true_labels, (test_predictions > 0.5).astype(int))\n",
    "\n",
    "# # Plot ROC curve\n",
    "# #plot_roc_curve(test_true_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "deb38234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "# metrics['accuracy'] = accuracy_score(true_labels, (np.array(predictions) > 0.5).astype(int)) * 100\n",
    "\n",
    "\n",
    "# def evaluate(model, device, loader):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     true_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for img1, text1, img2, text2, labels in loader:\n",
    "#             img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "#             output1, output2 = model(img1, img2, text1, text2)\n",
    "#             similarity = F.cosine_similarity(output1, output2)\n",
    "#             predictions.extend(similarity.cpu().numpy())\n",
    "#             true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "#     return np.array(predictions), np.array(true_labels)\n",
    "\n",
    "# # After running the evaluation\n",
    "# test_predictions, test_true_labels = evaluate(model, device, test_loader)\n",
    "\n",
    "# # Now you can create the confusion matrix\n",
    "# plot_confusion_matrix(test_true_labels, (test_predictions > 0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f0e0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--------------------\n",
    "# # Implement confusion matrix calculation\n",
    "# #--------------------\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_confusion_matrix(y_true, y_pred):\n",
    "#     cm = confusion_matrix(y_true, y_pred)\n",
    "#     plt.figure(figsize=(10,8))\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "#     plt.xlabel('Predicted labels')\n",
    "#     plt.ylabel('True labels')\n",
    "#     plt.title('Confusion Matrix')\n",
    "#     plt.show()\n",
    "\n",
    "# # Calculate confusion matrix\n",
    "# y_pred = (np.array(test_predictions) > 0.5).astype(int)\n",
    "# plot_confusion_matrix(test_true_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "b167923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation\n",
    "# test_metrics, test_predictions, test_true_labels = evaluate(model, device, test_loader)\n",
    "# print(\"Test Metrics:\")\n",
    "# for metric, value in test_metrics.items():\n",
    "#     print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# # Confusion Matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "\n",
    "# def plot_confusion_matrix(y_true, y_pred):\n",
    "#     cm = confusion_matrix(y_true, y_pred)\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "#     plt.xlabel('Predicted labels')\n",
    "#     plt.ylabel('True labels')\n",
    "#     plt.title('Confusion Matrix')\n",
    "#     plt.show()\n",
    "\n",
    "# # Calculate and plot confusion matrix\n",
    "# y_pred = (np.array(test_predictions) > 0.5).astype(int)\n",
    "# plot_confusion_matrix(test_true_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa531f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # Main Execution\n",
    "# # -----------------------------\n",
    "# data = load_dataset(JSON_PATH, IMAGE_DIR)\n",
    "# print(f\"Loaded {len(data)} entries from the dataset.\")\n",
    "\n",
    "# # Initialize tokenizer (if needed)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Create dataset and dataloader\n",
    "# dataset = WikiDiverseDataset(data, transform=transform, text_tokenizer=tokenizer.encode)\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# # Create model\n",
    "# model = SiameseNetworkWithCrossAttention(VOCAB_SIZE)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# # Set up optimizer and loss function\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# criterion = ContrastiveLoss(margin=1.0)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(EPOCHS):\n",
    "#     loss = train(model, device, dataloader, optimizer, criterion)\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "af9a5690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WikiDiverseDataset(Dataset):\n",
    "#     def __init__(self, data, img_dir, transform=None, text_tokenizer=None):\n",
    "#         self.data = data  # Directly set the data passed to the constructor\n",
    "#         self.img_dir = img_dir\n",
    "#         self.transform = transform\n",
    "#         self.text_tokenizer = text_tokenizer\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         entry = self.data[idx]\n",
    "\n",
    "#         # Load images\n",
    "#         img1_path = os.path.join(self.img_dir, entry['img1'])\n",
    "#         img2_path = os.path.join(self.img_dir, entry['img2'])\n",
    "#         img1 = Image.open(img1_path).convert('RGB')\n",
    "#         img2 = Image.open(img2_path).convert('RGB')\n",
    "\n",
    "#         # Apply image transformations\n",
    "#         if self.transform:\n",
    "#             img1 = self.transform(img1)\n",
    "#             img2 = self.transform(img2)\n",
    "\n",
    "#         # Tokenize text\n",
    "#         text1 = entry['text1']\n",
    "#         text2 = entry['text2']\n",
    "#         if self.text_tokenizer:\n",
    "#             text1 = self.text_tokenizer(text1, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "#             text2 = self.text_tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "#         label = torch.tensor(entry['label'], dtype=torch.float32)\n",
    "\n",
    "#         return img1, text1, img2, text2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "c92ef2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "\n",
    "# class WikiDiverseDataset(Dataset):\n",
    "#     def __init__(self, json_path, img_dir, transform=None, text_tokenizer=None):\n",
    "#         self.data = []\n",
    "#         self.img_dir = img_dir\n",
    "#         self.transform = transform\n",
    "#         self.text_tokenizer = text_tokenizer\n",
    "        \n",
    "#         try:\n",
    "#             with open(json_path, 'r') as f:\n",
    "#                 for entry in json.load(f):\n",
    "#                     try:\n",
    "#                         img1_path = os.path.join(img_dir, entry['image1'])\n",
    "#                         img2_path = os.path.join(img_dir, entry['image2'])\n",
    "                        \n",
    "#                         if os.path.exists(img1_path) and os.path.exists(img2_path):\n",
    "#                             self.data.append(entry)\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error processing entry: {e}\")\n",
    "#         except PermissionError:\n",
    "#             print(f\"Permission denied for file: {json_path}. Please ensure you have read permissions.\")\n",
    "        \n",
    "#         print(f\"Total valid entries: {len(self.data)}\")\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         entry = self.data[idx]\n",
    "\n",
    "#         # Load images\n",
    "#         img1_path = os.path.join(self.img_dir, entry['image1'])\n",
    "#         img2_path = os.path.join(self.img_dir, entry['image2'])\n",
    "#         img1 = Image.open(img1_path).convert('RGB')\n",
    "#         img2 = Image.open(img2_path).convert('RGB')\n",
    "\n",
    "#         # Apply image transformations\n",
    "#         if self.transform:\n",
    "#             img1 = self.transform(img1)\n",
    "#             img2 = self.transform(img2)\n",
    "\n",
    "#         # Tokenize text\n",
    "#         text1 = entry['text1']\n",
    "#         text2 = entry['text2']\n",
    "#         if self.text_tokenizer:\n",
    "#             text1 = self.text_tokenizer(text1, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "#             text2 = self.text_tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "#         label = torch.tensor(entry['label'], dtype=torch.float32)\n",
    "\n",
    "#         return img1, text1, img2, text2, label\n",
    "\n",
    "# # Example: Define the transformation pipeline\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# # Now you can pass this transform to your dataset\n",
    "# dataset = WikiDiverseDataset(json_path='path_to_json', img_dir='path_to_images', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "6554e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_data(raw_data):\n",
    "#     \"\"\"\n",
    "#     Convert list entries in the dataset to dictionaries with expected keys.\n",
    "#     \"\"\"\n",
    "#     processed_data = []\n",
    "#     for idx, entry in enumerate(raw_data):\n",
    "#         if isinstance(entry, list):\n",
    "#             try:\n",
    "#                 # Ensure the list follows the expected structure\n",
    "#                 processed_entry = {\n",
    "#                     'text1': entry[0],  # Descriptive text\n",
    "#                     'img1': entry[1],   # Path or URL for the first image\n",
    "#                     'text2': entry[2],  # Second text (if applicable)\n",
    "#                     'img2': entry[3],   # Path or URL for the second image\n",
    "#                     'label': entry[-1], # Label or class\n",
    "#                 }\n",
    "#                 processed_data.append(processed_entry)\n",
    "#             except IndexError:\n",
    "#                 print(f\"Skipping malformed list entry at index {idx}: {entry}\")\n",
    "#         elif isinstance(entry, dict):\n",
    "#             # Keep valid dictionary entries\n",
    "#             processed_data.append(entry)\n",
    "#         else:\n",
    "#             print(f\"Skipping unsupported entry type at index {idx}: {entry}\")\n",
    "#     return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "b529aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WikiDiverseDataset(Dataset):\n",
    "#     def __init__(self, data, img_dir, transform=None, text_tokenizer=None):\n",
    "#         \"\"\"\n",
    "#         Initialize the dataset with preloaded data.\n",
    "\n",
    "#         :param data: List of dictionary entries containing dataset information.\n",
    "#         :param img_dir: Path to the directory containing images.\n",
    "#         :param transform: Image transformations to apply (optional).\n",
    "#         :param text_tokenizer: Tokenizer function for text (optional).\n",
    "#         \"\"\"\n",
    "#         self.data = data  # Use the preloaded data directly\n",
    "#         self.img_dir = img_dir\n",
    "#         self.transform = transform\n",
    "#         self.text_tokenizer = text_tokenizer\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         entry = self.data[idx]\n",
    "\n",
    "#         # Load images\n",
    "#         img1_path = os.path.join(self.img_dir, entry['image1'])\n",
    "#         img2_path = os.path.join(self.img_dir, entry['image2'])\n",
    "#         img1 = Image.open(img1_path).convert('RGB')\n",
    "#         img2 = Image.open(img2_path).convert('RGB')\n",
    "\n",
    "#         # Apply image transformations\n",
    "#         if self.transform:\n",
    "#             img1 = self.transform(img1)\n",
    "#             img2 = self.transform(img2)\n",
    "\n",
    "#         # Tokenize text\n",
    "#         text1 = entry['text1']\n",
    "#         text2 = entry['text2']\n",
    "#         if self.text_tokenizer:\n",
    "#             text1 = self.text_tokenizer(\n",
    "#                 text1, return_tensors=\"pt\", padding=True, truncation=True, max_length=128\n",
    "#             )\n",
    "#             text2 = self.text_tokenizer(\n",
    "#                 text2, return_tensors=\"pt\", padding=True, truncation=True, max_length=128\n",
    "#             )\n",
    "\n",
    "#         label = torch.tensor(entry['label'], dtype=torch.float32)\n",
    "\n",
    "#         return img1, text1, img2, text2, label   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "4759fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(json_path, img_dir, batch_size):\n",
    "#     dataset = WikiDiverseDataset(json_path, img_dir)\n",
    "    \n",
    "#     # Split the dataset into train, validation, and test\n",
    "#     train_data, val_test_data = train_test_split(dataset.data, test_size=0.2, random_state=42)\n",
    "#     val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)\n",
    "    \n",
    "#     # Ensure split data retains correct structure (list of dictionaries)\n",
    "#     def ensure_data_structure(data):\n",
    "#         if isinstance(data[0], str):  # If data entries are strings, parse them as JSON\n",
    "#             import json\n",
    "#             return [json.loads(entry) for entry in data]\n",
    "#         return data\n",
    "\n",
    "#     train_data = ensure_data_structure(train_data)\n",
    "#     val_data = ensure_data_structure(val_data)\n",
    "#     test_data = ensure_data_structure(test_data)\n",
    "    \n",
    "#     # Create dataset objects for each set with corresponding splits\n",
    "#     train_dataset = WikiDiverseDataset(json_path, img_dir)\n",
    "#     train_dataset.data = train_data  # Assign parsed data to dataset\n",
    "#     val_dataset = WikiDiverseDataset(json_path, img_dir)\n",
    "#     val_dataset.data = val_data\n",
    "#     test_dataset = WikiDiverseDataset(json_path, img_dir)\n",
    "#     test_dataset.data = test_data\n",
    "    \n",
    "#     # Create DataLoader for each set\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "#     return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "f3a7e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #------------------------\n",
    "# # 4. Data Transformations \n",
    "# #------------------------\n",
    "# transform = Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "#     transforms.RandomRotation(degrees=30),\n",
    "#     ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "042c3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #---------------------------\n",
    "# # 5. Cross-Attention Mechanism \n",
    "# #---------------------------\n",
    "# class CrossAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads=4, dropout=0.1):\n",
    "#         super(CrossAttention, self).__init__()\n",
    "#         self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "#         self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, query, key, value):\n",
    "#         attn_output, _ = self.multihead_attn(query, key, value)\n",
    "#         attn_output = self.dropout(attn_output)\n",
    "#         return self.layer_norm(query + attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "5faed25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #-----------------------------------\n",
    "# # 6. Sub-Networks for Image and Text\n",
    "# #-----------------------------------\n",
    "# class ImageSubNetworkWithAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim=256, num_heads=4):\n",
    "#         super(ImageSubNetworkWithAttention, self).__init__()\n",
    "#         base_model = models.resnet50(pretrained=True)\n",
    "#         self.features = nn.Sequential(*list(base_model.children())[:-1])  # Remove FC layer\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(2048, embed_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5)\n",
    "#         )\n",
    "#         self.cross_attention = CrossAttention(embed_dim, num_heads)\n",
    "\n",
    "#     def forward(self, x, text_features):\n",
    "#         x = self.features(x).view(x.size(0), -1)  # Flatten\n",
    "#         x = self.fc(x)\n",
    "#         if text_features is not None:\n",
    "#             x = self.cross_attention(x.unsqueeze(1), text_features, text_features).squeeze(1)\n",
    "#         return x\n",
    "\n",
    "# class TextSubNetworkWithAttention(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_heads=4):\n",
    "#         super(TextSubNetworkWithAttention, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "#         self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, dropout=0.5)\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5)\n",
    "#         )\n",
    "#         self.cross_attention = CrossAttention(hidden_dim, num_heads)\n",
    "\n",
    "#     def forward(self, x, image_features):\n",
    "#         x = self.embedding(x)\n",
    "#         _, (hidden, _) = self.lstm(x)\n",
    "#         hidden = hidden.squeeze(0)\n",
    "#         if image_features is not None:\n",
    "#             hidden = self.cross_attention(hidden.unsqueeze(1), image_features, image_features).squeeze(1)\n",
    "#         return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "f56f14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #-----------------\n",
    "# # 7. Siamese Network\n",
    "# #----------------\n",
    "# class SiameseNetworkWithCrossAttention(nn.Module):\n",
    "#     def __init__(self, vocab_size):\n",
    "#         super(SiameseNetworkWithCrossAttention, self).__init__()\n",
    "#         self.image_net = ImageSubNetworkWithAttention()\n",
    "#         self.text_net = TextSubNetworkWithAttention(vocab_size)\n",
    "\n",
    "#     def forward(self, img1, img2, text1, text2):\n",
    "#         img_embedding1 = self.image_net(img1, None)\n",
    "#         img_embedding2 = self.image_net(img2, None)\n",
    "\n",
    "#         text_embedding1 = self.text_net(text1, img_embedding1)\n",
    "#         text_embedding2 = self.text_net(text2, img_embedding2)\n",
    "\n",
    "#         combined_embedding1 = torch.cat([img_embedding1, text_embedding1], dim=1)\n",
    "#         combined_embedding2 = torch.cat([img_embedding2, text_embedding2], dim=1)\n",
    "\n",
    "#         return combined_embedding1, combined_embedding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "96f6919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #---------------------\n",
    "# # 8. Contrastive Loss\n",
    "# #---------------------\n",
    "# class ContrastiveLoss(nn.Module):\n",
    "#     def __init__(self, margin=1.0):\n",
    "#         super(ContrastiveLoss, self).__init__()\n",
    "#         self.margin = margin\n",
    "\n",
    "#     def forward(self, output1, output2, label):\n",
    "#         euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "#         loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "#                           label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "4c8e8e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #-------------------------\n",
    "# # 9. Training and Evaluation\n",
    "# #------------------------\n",
    "# def train(model, device, loader, optimizer, criterion):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     total_correct = 0\n",
    "#     for img1, text1, img2, text2, labels in loader:\n",
    "#         img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output1, output2 = model(img1, img2, text1, text2)\n",
    "#         loss = criterion(output1, output2, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "f3e93f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Training loop\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "# def train(model, device, loader, optimizer, criterion):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch in loader:\n",
    "#         try:\n",
    "#             img1, text1, img2, text2, labels = batch\n",
    "#             img1, text1, img2, text2, labels = (\n",
    "#                 img1.to(device),\n",
    "#                 text1.to(device),\n",
    "#                 img2.to(device),\n",
    "#                 text2.to(device),\n",
    "#                 labels.to(device),\n",
    "#             )\n",
    "            \n",
    "#             # Forward pass\n",
    "#             combined_embedding1, combined_embedding2 = model(img1, img2, text1, text2)\n",
    "#             labels = labels.unsqueeze(1).float()  # Ensure labels have correct shape\n",
    "            \n",
    "#             loss = criterion(combined_embedding1, combined_embedding2, labels)\n",
    "#             total_loss += loss.item()\n",
    "            \n",
    "#             # Backward pass\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in batch processing: {e}\")\n",
    "    \n",
    "#     return total_loss / len(loader)\n",
    "\n",
    "# def evaluate(model, device, loader):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     true_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in loader:\n",
    "#             try:\n",
    "#                 img1, text1, img2, text2, labels = batch\n",
    "#                 img1, text1, img2, text2, labels = (\n",
    "#                     img1.to(device),\n",
    "#                     text1.to(device),\n",
    "#                     img2.to(device),\n",
    "#                     text2.to(device),\n",
    "#                     labels.to(device),\n",
    "#                 )\n",
    "                \n",
    "#                 combined_embedding1, combined_embedding2 = model(img1, img2, text1, text2)\n",
    "#                 similarity = F.cosine_similarity(combined_embedding1, combined_embedding2)\n",
    "                \n",
    "#                 predictions.extend(similarity.cpu().numpy())\n",
    "#                 true_labels.extend(labels.cpu().numpy())\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error during evaluation: {e}\")\n",
    "    \n",
    "#     # Compute metrics\n",
    "#     accuracy = accuracy_score(true_labels, (np.array(predictions) > 0.5).astype(int)) * 100\n",
    "#     auc = roc_auc_score(true_labels, predictions) if len(set(true_labels)) > 1 else None\n",
    "    \n",
    "#     return accuracy, auc\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(EPOCHS):\n",
    "#     train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "#     # Evaluate on validation set\n",
    "#     val_accuracy, val_auc = evaluate(model, device, val_loader)\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Val Accuracy: {val_accuracy:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "# # Final evaluation on test set\n",
    "# test_accuracy, test_auc = evaluate(model, device, test_loader)\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}, Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "5004fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #9. Training and Evaluation:\n",
    "# #Train the model and track performance using metrics like ROC AUC.\n",
    "# # Training and Evaluation\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import torch.nn.functional as F\n",
    "# import torch.nn.functional as F\n",
    "# from torchvision.transforms import Compose, ColorJitter\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # # Load data\n",
    "\n",
    "# train_loader, val_loader, test_loader = load_data(json_path, img_dir, BATCH_SIZE)\n",
    "\n",
    "# # Training loop\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# criterion = nn.CosineEmbeddingLoss()\n",
    "# #labels = labels.unsqueeze(1).float()\n",
    "\n",
    "# def train(model, device, loader, optimizer, criterion):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch in loader:\n",
    "#         valid_batch = [sample for sample in batch if sample is not None]\n",
    "#         if len(valid_batch) == 5:  # Check if all elements are valid\n",
    "#             img1, text1, img2, text2, labels = valid_batch\n",
    "#             img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "            \n",
    "#             combined_embedding1, combined_embedding2 = model(img1, img2, text1, text2)\n",
    "#             labels = labels.unsqueeze(1).float()\n",
    "            \n",
    "#             loss = criterion(combined_embedding1, combined_embedding2, labels)\n",
    "#             total_loss += loss.item()\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#         else:\n",
    "#             print(f\"Warning: Invalid batch encountered. Skipping this iteration.\")\n",
    "    \n",
    "#     return total_loss / len(loader)  \n",
    "\n",
    "# def evaluate(model, device, loader):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     true_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in loader:\n",
    "#             img1, text1, img2, text2 = batch\n",
    "#             img1, text1, img2, text2 = img1.to(device), text1.to(device), img2.to(device), text2.to(device)\n",
    "            \n",
    "#             combined_embedding1, combined_embedding2 = model(img1, img2, text1, text2)\n",
    "#             similarity = F.cosine_similarity(combined_embedding1, combined_embedding2)\n",
    "#             predictions.extend(similarity.cpu().numpy())\n",
    "#             true_labels.extend(batch[-1].cpu().numpy())\n",
    "    \n",
    "#     return accuracy_score(true_labels, (predictions > 0.5).astype(int)) * 100  \n",
    "\n",
    "# # Training loop\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     train_loss = train(model, device, train_loader, optimizer, criterion)    \n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "#     # Evaluate on validation set\n",
    "#     val_accuracy = evaluate(model, device, val_loader)\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# # Final evaluation on test set\n",
    "# test_accuracy = evaluate(model, device, test_loader)\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "9885e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #---------------\n",
    "# # Main Excution\n",
    "# #---------------\n",
    "# data = load_dataset(json_path, img_dir)\n",
    "# print(f\"Loaded {len(data)} entries from the dataset.\")\n",
    "\n",
    "# # Initialize tokenizer (if needed)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Create dataset and dataloader\n",
    "# dataset = WikiDiverseDataset(data, img_dir, transform=transform, text_tokenizer=tokenizer.encode)\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# # Create model\n",
    "# model = SiameseNetworkWithCrossAttention(VOCAB_SIZE)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# # Set up optimizer and loss function\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# criterion = ContrastiveLoss(margin=1.0)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(EPOCHS):\n",
    "#     loss = train(model, device, dataloader, optimizer, criterion)\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "873d2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main Execution\n",
    "# data = load_dataset(json_path, img_dir)  # This function will load and process your data\n",
    "# print(f\"Loaded {len(data)} entries from the dataset.\")\n",
    "\n",
    "# # Initialize tokenizer (if needed)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Create dataset and dataloader\n",
    "# dataset = WikiDiverseDataset(data, img_dir, transform=transform, text_tokenizer=tokenizer.encode)\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# # Create model\n",
    "# model = SiameseNetworkWithCrossAttention(VOCAB_SIZE)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# # Set up optimizer and loss function\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# criterion = ContrastiveLoss(margin=1.0)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(EPOCHS):\n",
    "#     loss = train(model, device, dataloader, optimizer, criterion)\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----\n",
    "# Revised \n",
    "#-------\n",
    "# Overview\n",
    "# Siamese networks consist of two identical sub-networks that share weights and learn to compute similarity between two inputs.\n",
    "# For the WikiDiverse dataset, the goal is to learn embeddings such that similar inputs are close in embedding space, while dissimilar ones are far apart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "d3d38cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Imports and Required Libraries\n",
    "# import os\n",
    "# import json\n",
    "# import hashlib\n",
    "# import re\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import models, transforms\n",
    "# from transformers import BertTokenizer\n",
    "# import torch.nn.functional as F\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from PIL import Image\n",
    "# from torchvision.transforms import Compose, ColorJitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "e72451e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Dataset Path and Hyperparameters\n",
    "# 1 cccccc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "02ccd59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 CCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "426d2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 CCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "9bf0df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4. Data Transformations\n",
    "# transform = Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "#     transforms.RandomRotation(degrees=30),\n",
    "#     ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "00195fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5. Cross-Attention Mechanism\n",
    "# class CrossAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads=4, dropout=0.1):\n",
    "#         super(CrossAttention, self).__init__()\n",
    "#         self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "#         self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, query, key, value):\n",
    "#         attn_output, _ = self.multihead_attn(query, key, value)\n",
    "#         attn_output = self.dropout(attn_output)\n",
    "#         return self.layer_norm(query + attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "c165c340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6. Sub-Networks for Image and Text\n",
    "# class ImageSubNetworkWithAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim=256, num_heads=4):\n",
    "#         super(ImageSubNetworkWithAttention, self).__init__()\n",
    "#         base_model = models.resnet50(pretrained=True)\n",
    "#         self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(2048, embed_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5)\n",
    "#         )\n",
    "#         self.cross_attention = CrossAttention(embed_dim, num_heads)\n",
    "\n",
    "#     def forward(self, x, text_features):\n",
    "#         x = self.features(x).view(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "#         if text_features is not None:\n",
    "#             x = self.cross_attention(x.unsqueeze(1), text_features, text_features).squeeze(1)\n",
    "#         return x\n",
    "\n",
    "# class TextSubNetworkWithAttention(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_heads=4):\n",
    "#         super(TextSubNetworkWithAttention, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "#         self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, dropout=0.5)\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5)\n",
    "#         )\n",
    "#         self.cross_attention = CrossAttention(hidden_dim, num_heads)\n",
    "\n",
    "#     def forward(self, x, image_features):\n",
    "#         x = self.embedding(x)\n",
    "#         _, (hidden, _) = self.lstm(x)\n",
    "#         hidden = hidden.squeeze(0)\n",
    "#         if image_features is not None:\n",
    "#             hidden = self.cross_attention(hidden.unsqueeze(1), image_features, image_features).squeeze(1)\n",
    "#         return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "05cd5dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 7. Siamese Network\n",
    "# class SiameseNetworkWithCrossAttention(nn.Module):\n",
    "#     def __init__(self, vocab_size):\n",
    "#         super(SiameseNetworkWithCrossAttention, self).__init__()\n",
    "#         self.image_net = ImageSubNetworkWithAttention()\n",
    "#         self.text_net = TextSubNetworkWithAttention(vocab_size)\n",
    "\n",
    "#     def forward(self, img1, img2, text1, text2):\n",
    "#         img_embedding1 = self.image_net(img1, None)\n",
    "#         img_embedding2 = self.image_net(img2, None)\n",
    "\n",
    "#         text_embedding1 = self.text_net(text1, img_embedding1)\n",
    "#         text_embedding2 = self.text_net(text2, img_embedding2)\n",
    "\n",
    "#         combined_embedding1 = torch.cat([img_embedding1, text_embedding1], dim=1)\n",
    "#         combined_embedding2 = torch.cat([img_embedding2, text_embedding2], dim=1)\n",
    "\n",
    "#         return combined_embedding1, combined_embedding2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "153226dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 8. Contrastive Loss\n",
    "# class ContrastiveLoss(nn.Module):\n",
    "#     def __init__(self, margin=1.0):\n",
    "#         super(ContrastiveLoss, self).__init__()\n",
    "#         self.margin = margin\n",
    "\n",
    "#     def forward(self, output1, output2, label):\n",
    "#         euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "#         loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "#                           label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "b822a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 9. Training and Evaluation\n",
    "# def train(model, device, loader, optimizer, criterion):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for img1, text1, img2, text2, labels in loader:\n",
    "#         img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output1, output2 = model(img1, img2, text1, text2)\n",
    "#         loss = criterion(output1, output2, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     return total_loss / len(loader)\n",
    "\n",
    "# def evaluate(model, device, loader):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     true_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for img1, text1, img2, text2, labels in loader:\n",
    "#             img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "#             output1, output2 = model(img1, img2, text1, text2)\n",
    "#             similarity = F.cosine_similarity(output1, output2)\n",
    "#             predictions.extend(similarity.cpu().numpy())\n",
    "#             true_labels.extend(labels.cpu().numpy())\n",
    "#     return accuracy_score(true_labels, (np.array(predictions) > 0.5).astype(int)) * 100\n",
    "\n",
    "# # Main Execution\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# dataset = load_dataset(JSON_PATH, IMAGE_DIR)\n",
    "# train_data, val_test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "# val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# # train_loader = DataLoader(WikiDiverseDataset(train_data, IMAGE_DIR, transform), batch_size=BATCH_SIZE, shuffle=True)\n",
    "# # val_loader = DataLoader(WikiDiverseDataset(val_data, IMAGE_DIR, transform), batch_size=BATCH_SIZE, shuffle=False)\n",
    "# # test_loader = DataLoader(WikiDiverseDataset(test_data, IMAGE_DIR, transform), batch_size=BATCH_SIZE, shuffle=False)\n",
    "# train_loader = DataLoader(\n",
    "#     WikiDiverseDataset(train_data, IMAGE_DIR, transform=transform, text_tokenizer=tokenizer),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=True,\n",
    "# )\n",
    "# val_loader = DataLoader(\n",
    "#     WikiDiverseDataset(val_data, IMAGE_DIR, transform=transform, text_tokenizer=tokenizer),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False,\n",
    "# )\n",
    "# test_loader = DataLoader(\n",
    "#     WikiDiverseDataset(test_data, IMAGE_DIR, transform=transform, text_tokenizer=tokenizer),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False,\n",
    "# )\n",
    "\n",
    "# model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# criterion = ContrastiveLoss()\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}\")\n",
    "#     val_accuracy = evaluate(model, device, val_loader)\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# test_accuracy = evaluate(model, device, test_loader)\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f546c176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86566b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88f5bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #------------------------\n",
    "# # ???. WikiDiverse Dataset Class \n",
    "# #------------------------\n",
    "# class WikiDiverseDataset(Dataset):\n",
    "#     def __init__(self, json_path, img_dir, transform=None, text_tokenizer=None):\n",
    "#         self.data = []\n",
    "#         self.img_dir = img_dir\n",
    "#         self.transform = transform\n",
    "#         self.text_tokenizer = text_tokenizer\n",
    "        \n",
    "#         try:\n",
    "#             with open(json_path, 'r') as f:\n",
    "#                 for entry in json.load(f):\n",
    "#                     try:\n",
    "#                         img1_path = os.path.join(img_dir, entry['image1'])\n",
    "#                         img2_path = os.path.join(img_dir, entry['image2'])\n",
    "                        \n",
    "#                         if os.path.exists(img1_path) and os.path.exists(img2_path):\n",
    "#                             self.data.append(entry)\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error processing entry: {e}\")\n",
    "#         except PermissionError:\n",
    "#             print(f\"Permission denied for file: {json_path}. Please ensure you have read permissions.\")\n",
    "        \n",
    "#         print(f\"Total valid entries: {len(self.data)}\")\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         entry = self.data[idx]\n",
    "\n",
    "#         # Load images\n",
    "#         img1_path = os.path.join(self.img_dir, entry['image1'])\n",
    "#         img2_path = os.path.join(self.img_dir, entry['image2'])\n",
    "#         img1 = Image.open(img1_path).convert('RGB')\n",
    "#         img2 = Image.open(img2_path).convert('RGB')\n",
    "\n",
    "#         # Apply image transformations\n",
    "#         if self.transform:\n",
    "#             img1 = self.transform(img1)\n",
    "#             img2 = self.transform(img2)\n",
    "\n",
    "#         # Tokenize text\n",
    "#         text1 = entry['text1']\n",
    "#         text2 = entry['text2']\n",
    "#         if self.text_tokenizer:\n",
    "#             text1 = self.text_tokenizer(text1, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "#             text2 = self.text_tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "#         label = torch.tensor(entry['label'], dtype=torch.float32)\n",
    "\n",
    "#         return img1, text1, img2, text2, label\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     batch = [sample for sample in batch if sample is not None]\n",
    "#     return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "d10a7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# img_dir = r\"C:\\Users\\Min Dator\\aics-project\\wikinewsImgs\"\n",
    "\n",
    "# if not os.path.exists(img_dir):\n",
    "#     print(f\"Image directory does not exist: {img_dir}\")\n",
    "# else:\n",
    "#     print(f\"Image directory exists: {img_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "3a714cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# img_files = glob.glob(os.path.join(img_dir, \"*\"))\n",
    "# if len(img_files) == 0:\n",
    "#     print(\"No image files found in the directory.\")\n",
    "# else:\n",
    "#     print(f\"Found {len(img_files)} image files.\")\n",
    "#     print(f\"Example files: {img_files[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54964f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# class WikiDiverseDataset(Dataset):\n",
    "#     # Existing __init__ method here...\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         entry = self.data[idx]\n",
    "\n",
    "#         img1_path = os.path.join(self.img_dir, entry['image1'])\n",
    "#         img2_path = os.path.join(self.img_dir, entry['image2'])\n",
    "\n",
    "#         try:\n",
    "#             img1 = Image.open(img1_path).convert('RGB')\n",
    "#             img2 = Image.open(img2_path).convert('RGB')\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading image: {e} for {img1_path} or {img2_path}\")\n",
    "#             return None\n",
    "\n",
    "#         # Rest of the __getitem__ code...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "812fe984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_json_path = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_w_cands\\wikidiverse_w_cands\\train_w_10cands.json\"\n",
    "# valid_json_path = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_w_cands\\wikidiverse_w_cands\\valid_w_10cands.json\"\n",
    "# test_json_path = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_w_cands\\wikidiverse_w_cands\\test_w_10cands.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "82440ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = WikiDiverseDataset(train_json_path, img_dir, transform, text_tokenizer)\n",
    "# val_loader = WikiDiverseDataset(valid_json_path, img_dir, transform, text_tokenizer)\n",
    "# test_loader = WikiDiverseDataset(test_json_path, img_dir, transform, text_tokenizer)\n",
    "\n",
    "# # Wrap with DataLoader\n",
    "# train_loader = DataLoader(train_loader, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "# val_loader = DataLoader(val_loader, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "# test_loader = DataLoader(test_loader, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cc34978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_json_path = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_w_cands\\wikidiverse_w_cands\\train_w_10cands.json\"\n",
    "# valid_json_path = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_w_cands\\wikidiverse_w_cands\\valid_w_10cands.json\"\n",
    "# test_json_path = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_w_cands\\wikidiverse_w_cands\\test_w_10cands.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "9e3045f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def load_json(json_file):\n",
    "#     try:\n",
    "#         with open(json_file, 'r') as f:\n",
    "#             data = json.load(f)\n",
    "#             print(f\"Loaded {len(data)} entries from {json_file}\")\n",
    "#             return data\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         print(f\"JSON Decode Error: {e}\")\n",
    "#         raise\n",
    "#     except FileNotFoundError as e:\n",
    "#         print(f\"File not found: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Load JSON files\n",
    "# train_data = load_json(train_json_path)\n",
    "# test_data = load_json(test_json_path)\n",
    "# valid_data = load_json(valid_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "f69f9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_entry(entry):\n",
    "#     if not isinstance(entry, dict):\n",
    "#         print(\"Invalid entry format, skipping:\", entry)\n",
    "#         return None\n",
    "    \n",
    "#     # Check required keys\n",
    "#     required_keys = ['img_path', 'mention']\n",
    "#     for key in required_keys:\n",
    "#         if key not in entry:\n",
    "#             print(f\"Missing key {key} in entry:\", entry)\n",
    "#             return None\n",
    "    \n",
    "#     # Process image path\n",
    "#     img_path = entry['img_path']\n",
    "#     print(f\"Processing image: {img_path}\")\n",
    "#     return entry\n",
    "\n",
    "# # Validate dataset\n",
    "# processed_data = [process_entry(entry) for entry in train_data if process_entry(entry)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "de35f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def split_dataset(data, test_size=0.2):\n",
    "#     if len(data) == 0:\n",
    "#         raise ValueError(\"Cannot split an empty dataset!\")\n",
    "\n",
    "#     train, test = train_test_split(data, test_size=test_size, random_state=42)\n",
    "#     print(f\"Split into {len(train)} train and {len(test)} test entries.\")\n",
    "#     return train, test\n",
    "\n",
    "# train_split, val_split = split_dataset(train_data, test_size=0.2)\n",
    "# val_split, test_split = split_dataset(val_split, test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a1e8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# # Placeholder for missing variables\n",
    "# text_tokenizer = lambda x: x  # Replace with a real tokenizer if needed\n",
    "# transform = None  # Define image transformations if necessary\n",
    "\n",
    "\n",
    "# class WikiDiverseDataset(Dataset):\n",
    "#     def __init__(self, data, transform=None, tokenizer=None):\n",
    "#         self.data = data\n",
    "#         self.transform = transform\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = self.data[idx]\n",
    "#         img_path = item['img_path']\n",
    "#         text = item['mention']\n",
    "        \n",
    "#         # Apply tokenizer\n",
    "#         if self.tokenizer:\n",
    "#             text = self.tokenizer(text)\n",
    "        \n",
    "#         # Apply transform\n",
    "#         if self.transform:\n",
    "#             # Load and transform the image (placeholder)\n",
    "#             pass\n",
    "        \n",
    "#         return img_path, text\n",
    "\n",
    "# # Create datasets\n",
    "# train_dataset = WikiDiverseDataset(train_split, transform, text_tokenizer)\n",
    "# val_dataset = WikiDiverseDataset(val_split, transform, text_tokenizer)\n",
    "# test_dataset = WikiDiverseDataset(test_split, transform, text_tokenizer)\n",
    "\n",
    "# # Wrap datasets with DataLoader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137abdd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fda4a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import DataLoader\n",
    "# import os\n",
    "\n",
    "# # Assume WikiDiverseDataset is defined with proper constructor\n",
    "# class WikiDiverseDataset:\n",
    "#     def __init__(self, json_path, img_dir, transform=None):\n",
    "#         # Load JSON file and process\n",
    "#         self.json_path = json_path\n",
    "#         self.img_dir = img_dir\n",
    "#         self.transform = transform\n",
    "#         self.data = self.load_json_data()\n",
    "    \n",
    "#     def load_json_data(self):\n",
    "#         # Load the dataset from json_path\n",
    "#         import json\n",
    "#         with open(self.json_path, 'r') as file:\n",
    "#             data = json.load(file)\n",
    "#         return data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Get data from JSON\n",
    "#         entry = self.data[idx]\n",
    "#         img_path = os.path.join(self.img_dir, entry['img_path'])\n",
    "#         mention = entry['mention']\n",
    "        \n",
    "#         # Apply transform if any\n",
    "#         if self.transform:\n",
    "#             img = self.transform(img_path)\n",
    "        \n",
    "#         return img_path, mention\n",
    "\n",
    "# # Define collate_fn if needed\n",
    "# def collate_fn(batch):\n",
    "#     # Custom function to handle batching\n",
    "#     return batch\n",
    "\n",
    "# # Load data function\n",
    "# def load_data(json_path, img_dir, batch_size):\n",
    "#     dataset = WikiDiverseDataset(json_path, img_dir)\n",
    "    \n",
    "#     # Split the dataset into train, validation, and test\n",
    "#     train_data, val_test_data = train_test_split(dataset.data, test_size=0.2, random_state=42)\n",
    "#     val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)\n",
    "    \n",
    "#     # Create dataset objects for each set with corresponding splits\n",
    "#     train_dataset = WikiDiverseDataset(json_path, img_dir)\n",
    "#     train_dataset.data = train_data  # Assign the split data to the dataset\n",
    "#     val_dataset = WikiDiverseDataset(json_path, img_dir)\n",
    "#     val_dataset.data = val_data\n",
    "#     test_dataset = WikiDiverseDataset(json_path, img_dir)\n",
    "#     test_dataset.data = test_data\n",
    "    \n",
    "#     # Create DataLoader for each set\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "#     return train_loader, val_loader, test_loader\n",
    "\n",
    "# # Example usage\n",
    "# #json_path = os.path.join(os.getcwd(), 'wikidiverse_w_cands', 'wikidiverse_w_cands.json')\n",
    "# json_path = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_w_cands\\wikidiverse_w_cands\\train_w_10cands.json\"\n",
    "# img_dir = r'C:\\Users\\Min Dator\\aics-project\\wikinewsImgs'\n",
    "# BATCH_SIZE = 16\n",
    "\n",
    "# train_loader, val_loader, test_loader = load_data(json_path, img_dir, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d51f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---------------\n",
    "# 5. Load Data \n",
    "#---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d53c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# 6. Cross-Attention Mechanism \n",
    "#-------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5c4395-eaa0-4786-9053-fcbd93a81654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Dataset Path and Hyperparameters:\n",
    "# # -----------------------------\n",
    "# # Dataset Path and Hyperparameters\n",
    "# # -----------------------------\n",
    "# # Update paths based on your directory structure\n",
    "# DATASET_PATH = '\\\\home\\\\gusjembda@GU.GU.SE\\\\aics-project\\\\wikidiverse_w_cands\\\\wikidiverse_w_cands'\n",
    "# IMAGE_DIR = os.path.join(DATASET_PATH, \"wikinewsImgs\")\n",
    "# JSON_PATH = os.path.join(DATASET_PATH, \"train_w_10cands.json\")\n",
    "\n",
    "# #DATASET_PATH = '/home/gusjembda@GU.GU.SE/aics-project'\n",
    "# #IMAGE_DIR = os.path.join(DATASET_PATH, \"wikinewsImgs\")  # Make sure this directory exists\n",
    "# #JSON_PATH = os.path.join(DATASET_PATH, \"train_w_10cands.json\")\n",
    "\n",
    "# BATCH_SIZE = 16\n",
    "# EPOCHS = 10\n",
    "# LEARNING_RATE = 0.0001\n",
    "# EMBED_DIM = 256\n",
    "# VOCAB_SIZE = 10000\n",
    "\n",
    "# # Paths\n",
    "# #DATASET_PATH = '/home/gusjembda/aics-project/wikidiverse_w_cands/wikidiverse_w_cands'\n",
    "# #JSON_PATH = '/home/gusjembda@GU.GU.SE/aics-project/wikidiverse_w_cands/wikidiverse_w_cands/train_w_10cands.json'\n",
    "\n",
    "# #IMAGE_DIR = '/home/gusjembda/aics-project/wikinewsImgs'\n",
    "# #ENTITY_DESC_PATH = '/home/gusjembda/aics-project/wikipedia_entity2desc_filtered.tsv'\n",
    "# #ENTITY_IMGS_PATH = '/home/gusjembda/aics-project/wikipedia_entity2imgs.tsv'\n",
    "# #PROJECT_DIR = '/home/gusjembda/aics-project/WikiDiverse'\n",
    "\n",
    "# # usage:\n",
    "# #json_path = os.path.join(DATASET_PATH, 'train_w_10cands.json')\n",
    "# # entity_desc_df = pd.read_csv(ENTITY_DESC_PATH, sep='\\t', header=None)\n",
    "# # Sentity_imgs_df = pd.read_csv(ENTITY_IMGS_PATH, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4934cb03-ded6-4942-b832-3d49571a28f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhashlib\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m:\n\u001b[1;32m      5\u001b[0m     m_img \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m hashlib\u001b[38;5;241m.\u001b[39mmd5(m_img\u001b[38;5;241m.\u001b[39mencode())\u001b[38;5;241m.\u001b[39mhexdigest()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# import hashlib\n",
    "# import re\n",
    "\n",
    "# for item in data:\n",
    "#     m_img = item[1].split('/')[-1]\n",
    "#     prefix = hashlib.md5(m_img.encode()).hexdigest()\n",
    "#     suffix = re.sub(r'(\\S+(?=\\.(jpg|JPG|png|PNG|svg|SVG)))|(\\S+(?=\\.(jpeg|JPEG)))', '', m_img)\n",
    "#     m_img = 'path/to/wikinewsImgs/' + prefix + suffix\n",
    "#     m_img = m_img.replace('.svg', '.png').replace('.SVG', '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0770cce-2de0-4a4a-9140-34381e0003f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to the train_w_10cands.json: \\home\\gusjembda@GU.GU.SE\\aics-project\\wikidiverse_w_cands\\wikidiverse_w_cands/train_w_10cands.json\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# # Correct path for the directory you're working in\n",
    "# DATASET_PATH = '\\\\home\\\\gusjembda@GU.GU.SE\\\\aics-project\\\\wikidiverse_w_cands\\\\wikidiverse_w_cands'\n",
    "\n",
    "# # Define the path to the 'train_w_10cands.json' file\n",
    "# JSON_PATH = os.path.join(DATASET_PATH, 'train_w_10cands.json')\n",
    "\n",
    "# # Verify the path\n",
    "# print(f\"Path to the train_w_10cands.json: {JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc91fddf-f669-4926-88bf-024a51dde455",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json(JSON_PATH)\n",
    "\n",
    "# Print the first few rows of the dataset\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f0203bb-1393-473b-b78e-6ca5f5a4567e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /mnt/c/Users/Min Dator/aics-project\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.getcwd()\n",
    "# print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "961d3428-2098-473c-8b50-933203a38399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist!\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# JSON_PATH = \"/home/gusjembda@GU.GU.SE/aics-project/wikidiverse_w_cands/train_w_10cands.json\"\n",
    "\n",
    "# if os.path.exists(JSON_PATH):\n",
    "#     print(\"File exists!\")\n",
    "#     try:\n",
    "#         with open(JSON_PATH, 'r') as f:\n",
    "#             data = f.read()\n",
    "#             print(f\"File content length: {len(data)}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading the file: {e}\")\n",
    "# else:\n",
    "#     print(\"File does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d14101-3df7-4d12-9358-d310b08118d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train JSON not found: /home/gusjembda@GU.GU.SE/aics-project/WikiDiverse/train.json\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "# # Define the path to the train JSON file\n",
    "# train_json = '/home/gusjembda@GU.GU.SE/aics-project/WikiDiverse/train.json'\n",
    "\n",
    "# # Check if the file exists\n",
    "# if os.path.exists(train_json):\n",
    "#     print(f\"Train JSON found: {train_json}\")\n",
    "#     # Open and load the JSON file\n",
    "#     with open(train_json, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "#         print(\"First 5 records from the train JSON:\")\n",
    "#         print(data[:5])  # Display the first 5 records\n",
    "# else:\n",
    "#     print(f\"Train JSON not found: {train_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "738a21ce-994a-41a8-b8b4-5ae9c95cacd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: False\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# train_json = r'/home/gusjembda@GU.GU.SE/aics-project/WikiDiverse/train.json'\n",
    "# print(\"File exists:\", os.path.exists(train_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ed59ba2-b97f-499d-aa36-c3d00aad90a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File not found at /home/gusjembda\\@GU.GU.SE/aics-project/wikidiverse_w_cands/wikidiverse_w_cands/train_w_10cands.json\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "\n",
    "# # Path to your JSON file\n",
    "# #JSON_PATH = \"/home/gusjembda@GU.GU.SE/aics-project/wikidiverse_w_cands/wikidiverse_w_cands/train_w_10cands.json\"\n",
    "# JSON_PATH = \"/home/gusjembda\\\\@GU.GU.SE/aics-project/wikidiverse_w_cands/wikidiverse_w_cands/train_w_10cands.json\"\n",
    "# try:\n",
    "#     with open(JSON_PATH, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "#     print(\"Data loaded successfully!\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Error: File not found at {JSON_PATH}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2708509b-4796-4b11-8319-ee13fcee2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hashlib\n",
    "# import re\n",
    "# import os\n",
    "\n",
    "# # Example: process each data entry to get the local path for images\n",
    "# def get_image_path(item, img_dir):\n",
    "#     m_img = item[1].split('/')[-1]  # Extract image filename\n",
    "#     prefix = hashlib.md5(m_img.encode()).hexdigest()  # Generate a unique hash\n",
    "#     suffix = re.sub(r'(\\S+(?=\\.(jpg|JPG|png|PNG|svg|SVG)))|(\\S+(?=\\.(jpeg|JPEG)))', '', m_img)  # Remove image extension\n",
    "#     m_img = os.path.join(img_dir, prefix + suffix)  # Create the local path\n",
    "#     m_img = m_img.replace('.svg', '.png').replace('.SVG', '.png')  # Replace .svg with .png\n",
    "#     return m_img\n",
    "\n",
    "# # Example usage\n",
    "# img_dir = \"/path/to/wikinewsImgs\"\n",
    "# data = [...]  # Your dataset here\n",
    "# for item in data:\n",
    "#     image_path = get_image_path(item, img_dir)\n",
    "#     print(f\"Processed image path: {image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c45a7fe2-8ed2-4d9f-9f80-86c90494b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# json_file = \"/home/gusjembda@GU.GU.SE/aics-project/wikidiverse_w_cands/wikidiverse_w_cands/train_w_10cands.json\"\n",
    "\n",
    "# try:\n",
    "#     with open(json_file, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "#         print(\"Valid JSON\")\n",
    "# except json.JSONDecodeError as e:\n",
    "#     print(f\"Invalid JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d76c6ca-7f21-412d-b765-ec56efa3d03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found!\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# path = \"/home/gusjembda@GU.GU.SE/aics-project/wikidiverse_w_cands/wikidiverse_w_cands/train_w_10cands.json\"\n",
    "\n",
    "# if os.path.exists(path):\n",
    "#     print(\"File exists and is accessible!\")\n",
    "# else:\n",
    "#     print(\"File not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b17407a-cb8c-4a0c-a5bb-ac514621baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Path to the parent directory\n",
    "# DATASET_PATH = '/home/gusjembda@GU.GU.SE/aics-project/wikidiverse_w_cands/wikidiverse_w_cands'\n",
    "\n",
    "# # File paths\n",
    "# TRAIN_JSON = os.path.join(DATASET_PATH, 'train_w_10cands.json')\n",
    "# TEST_JSON = os.path.join(DATASET_PATH, 'test_w_10cands.json')\n",
    "# VALID_JSON = os.path.join(DATASET_PATH, 'valid_w_10cands.json')\n",
    "\n",
    "# # Verify paths\n",
    "# print(f\"Train JSON: {TRAIN_JSON}\")\n",
    "# print(f\"Test JSON: {TEST_JSON}\")\n",
    "# print(f\"Valid JSON: {VALID_JSON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66d23b4c-de1b-43c2-a75d-288d365e794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# base_dir = '\\\\home\\\\gusjembda@GU.GU.SE\\\\aics-project\\\\wikidiverse_w_cands\\\\wikidiverse_w_cands'\n",
    "# json_file = 'train_w_10cands.json'\n",
    "\n",
    "# JSON_PATH = os.path.join(base_dir, json_file)\n",
    "\n",
    "# with open(JSON_PATH, 'r') as f:\n",
    "#     data = json.load(f)\n",
    "#     print(data[:5])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a475afd-f2c7-4b80-94c5-94694b2d0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "# # Use forward slashes for Linux paths\n",
    "# base_dir = '/home/gusjembda@GU.GU.SE/aics-project/wikidiverse_w_cands/wikidiverse_w_cands'\n",
    "# json_file = 'train_w_10cands.json'\n",
    "\n",
    "# JSON_PATH = os.path.join(base_dir, json_file)\n",
    "\n",
    "# # Check if the file exists\n",
    "# if not os.path.exists(JSON_PATH):\n",
    "#     print(f\"File not found: {JSON_PATH}\")\n",
    "# else:\n",
    "#     with open(JSON_PATH, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "#         # Print the first 5 items (assuming the JSON is a list)\n",
    "#         print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7bd88154-77ae-4ab3-b800-ac10efedad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "# # Use forward slashes for Linux\n",
    "# base_dir = '/home/gusjembda@GU.GU.SE/aics-project/wikidiverse_w_cands/wikidiverse_w_cands'\n",
    "# json_file = 'train_w_10cands.json'\n",
    "\n",
    "# # Construct the full path\n",
    "# JSON_PATH = os.path.join(base_dir, json_file)\n",
    "\n",
    "# # Check if the file exists before trying to open it\n",
    "# if not os.path.exists(JSON_PATH):\n",
    "#     print(f\"File not found: {JSON_PATH}\")\n",
    "# else:\n",
    "#     # Safely open and load the JSON file\n",
    "#     with open(JSON_PATH, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "#         print(data[:5])  # Print the first 5 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcfee7aa-8710-42fe-94d9-d5eca26c9b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # Helper Functions\n",
    "# # -----------------------------\n",
    "# # Generate the local image path\n",
    "# def get_image_path(url, img_dir):\n",
    "#     try:\n",
    "#         filename = url.split('/')[-1]\n",
    "#         prefix = hashlib.md5(filename.encode()).hexdigest()\n",
    "#         suffix = re.sub(r'(\\S+(?=\\.(jpg|jpeg|png|svg)))', '', filename, flags=re.IGNORECASE)\n",
    "#         local_path = os.path.join(img_dir, f\"{prefix}{suffix}\".replace('.svg', '.png'))\n",
    "#         return local_path\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error generating image path: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Load and preprocess the dataset\n",
    "# def load_dataset(json_path, img_dir):\n",
    "#     try:\n",
    "#         with open(json_path, 'r') as f:\n",
    "#             data = json.load(f)\n",
    "#         processed_data = []\n",
    "#         for item in data:\n",
    "#             img_path = get_image_path(item[1], img_dir)\n",
    "#             if img_path and os.path.exists(img_path):\n",
    "#                 processed_data.append({\n",
    "#                     'sentence': item[0],\n",
    "#                     'mention': item[2],\n",
    "#                     'mention_type': item[3],\n",
    "#                     'left_context': item[4],\n",
    "#                     'right_context': item[5],\n",
    "#                     'entity_url': item[6],\n",
    "#                     'img_path': img_path\n",
    "#                 })\n",
    "        \n",
    "#         if not processed_data:\n",
    "#             print(\"No valid data entries found in the dataset.\")\n",
    "        \n",
    "#         return processed_data\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading dataset: {e}\")\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8cdb247-4b89-48a3-b56e-eadd728aea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # Data Transformations\n",
    "# # -----------------------------\n",
    "# transform = Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "#     transforms.RandomRotation(degrees=30),\n",
    "#     ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d12dd4fe-f39b-44a8-8287-b8988d84a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # Dataset Class\n",
    "# # -----------------------------\n",
    "# class WikiDiverseDataset(Dataset):\n",
    "#     def __init__(self, data, transform=None, text_tokenizer=None):\n",
    "#         self.data = data\n",
    "#         self.transform = transform\n",
    "#         self.text_tokenizer = text_tokenizer\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         entry = self.data[idx]\n",
    "#         try:\n",
    "#             img = Image.open(entry['img_path']).convert('RGB')\n",
    "#             if self.transform:\n",
    "#                 img = self.transform(img)\n",
    "\n",
    "#             text = entry['sentence']\n",
    "#             if self.text_tokenizer:\n",
    "#                 text = self.text_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "#             return img, text\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing data at index {idx}: {e}\")\n",
    "#             return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21ddd7d2-938c-4b3e-a5d0-d4fd223495d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # Cross-Attention Mechanism\n",
    "# # -----------------------------\n",
    "# class CrossAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads=4, dropout=0.1):\n",
    "#         super(CrossAttention, self).__init__()\n",
    "#         self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "#         self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, query, key, value):\n",
    "#         attn_output, _ = self.multihead_attn(query, key, value)\n",
    "#         attn_output = self.dropout(attn_output)\n",
    "#         return self.layer_norm(query + attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "096eacfe-21bf-4ec2-a6a5-3d0bc63975bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # Sub-Networks for Image and Text\n",
    "# # -----------------------------\n",
    "# class ImageSubNetworkWithAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim=256, num_heads=4):\n",
    "#         super(ImageSubNetworkWithAttention, self).__init__()\n",
    "#         base_model = models.resnet50(pretrained=True)\n",
    "#         self.features = nn.Sequential(*list(base_model.children())[:-1])  # Remove FC layer\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(2048, embed_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5)\n",
    "#         )\n",
    "#         self.cross_attention = CrossAttention(embed_dim, num_heads)\n",
    "\n",
    "#     def forward(self, x, text_features):\n",
    "#         x = self.features(x).view(x.size(0), -1)  # Flatten\n",
    "#         x = self.fc(x)\n",
    "#         if text_features is not None:\n",
    "#             x = self.cross_attention(x.unsqueeze(1), text_features, text_features).squeeze(1)\n",
    "#         return x\n",
    "\n",
    "# class TextSubNetworkWithAttention(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_heads=4):\n",
    "#         super(TextSubNetworkWithAttention, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "#         self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, dropout=0.5)\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5)\n",
    "#         )\n",
    "#         self.cross_attention = CrossAttention(hidden_dim, num_heads)\n",
    "\n",
    "#     def forward(self, x, image_features):\n",
    "#         x = self.embedding(x)\n",
    "#         _, (hidden, _) = self.lstm(x)\n",
    "#         hidden = hidden.squeeze(0)\n",
    "#         if image_features is not None:\n",
    "#             hidden = self.cross_attention(hidden.unsqueeze(1), image_features, image_features).squeeze(1)\n",
    "#         return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eab3c062-e94c-4148-bdcc-774341592213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # Siamese Network\n",
    "# # -----------------------------\n",
    "# class SiameseNetworkWithCrossAttention(nn.Module):\n",
    "#     def __init__(self, vocab_size):\n",
    "#         super(SiameseNetworkWithCrossAttention, self).__init__()\n",
    "#         self.image_net = ImageSubNetworkWithAttention()\n",
    "#         self.text_net = TextSubNetworkWithAttention(vocab_size)\n",
    "\n",
    "#     def forward(self, img1, img2, text1, text2):\n",
    "#         img_embedding1 = self.image_net(img1, None)\n",
    "#         img_embedding2 = self.image_net(img2, None)\n",
    "\n",
    "#         text_embedding1 = self.text_net(text1, img_embedding1)\n",
    "#         text_embedding2 = self.text_net(text2, img_embedding2)\n",
    "\n",
    "#         combined_embedding1 = torch.cat([img_embedding1, text_embedding1], dim=1)\n",
    "#         combined_embedding2 = torch.cat([img_embedding2, text_embedding2], dim=1)\n",
    "\n",
    "#         return combined_embedding1, combined_embedding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e3d766f-c825-4170-8bf8-5743cdf2468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # Contrastive Loss\n",
    "# # -----------------------------\n",
    "# class ContrastiveLoss(nn.Module):\n",
    "#     def __init__(self, margin=1.0):\n",
    "#         super(ContrastiveLoss, self).__init__()\n",
    "#         self.margin = margin\n",
    "\n",
    "#     def forward(self, output1, output2, label):\n",
    "#         euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "#         loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "#                           label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a976d23a-76c7-4101-9528-62cf46515f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # Training and Evaluation\n",
    "# # -----------------------------\n",
    "# def train(model, device, loader, optimizer, criterion):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     total_correct = 0\n",
    "#     for img1, text1, img2, text2, labels in loader:\n",
    "#         img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output1, output2 = model(img1, img2, text1, text2)\n",
    "#         loss = criterion(output1, output2, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     return total_loss / len(loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
