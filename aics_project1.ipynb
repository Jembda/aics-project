{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd4516a-7cc7-47fd-b481-597c06f239b5",
   "metadata": {},
   "source": [
    "# An overview\n",
    "Siamese networks consist of two identical sub-networks that share weights and learn to compute the similarity between two input samples. The goal is to learn embeddings such that similar inputs are close in the embedding space, while dissimilar inputs are far apart. For the WikiDiverse dataset, where we have image-caption pairs, we can build a Siamese network that processes text and image data (or just one modality like text or image) and learns to compute similarity between two entities from the knowledge base.\n",
    "* Siamese Network Structure: Two identical sub-networks that compute embeddings for input pairs and learn their similarity\n",
    "* Application: For WikiDiverse, compute similarity between image-caption pairs to link knowledge-base entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746e43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "1e5c4395-eaa0-4786-9053-fcbd93a81654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dataset Path and Hyperparameters:\n",
    "DATASET_PATH = r'C:\\Users\\Min Dator\\aics-project\\wikidiverse_w_cands'\n",
    "IMAGE_DIR = os.path.join(DATASET_PATH, \"wikinewsImgs\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "EMBED_DIM = 256\n",
    "VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "d3f97be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Helper Functions:\n",
    "def get_image_path(url, img_dir):\n",
    "    filename = url.split('/')[-1]\n",
    "    prefix = hashlib.md5(filename.encode()).hexdigest()\n",
    "    suffix = re.sub(r'(\\S+(?=\\.(jpg|jpeg|png|svg)))', '', filename, flags=re.IGNORECASE)\n",
    "    local_path = os.path.join(img_dir, f\"{prefix}{suffix}\".replace('.svg', '.png'))\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "074161e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. WikiDiverse Dataset Class:\n",
    "class WikiDiverseDataset(Dataset):\n",
    "    def __init__(self, json_path, img_dir, transform=None, text_tokenizer=None):\n",
    "        self.data = []\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        \n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                for entry in json.load(f):\n",
    "                    try:\n",
    "                        img1_path = os.path.join(img_dir, entry['image1'])\n",
    "                        img2_path = os.path.join(img_dir, entry['image2'])\n",
    "                        \n",
    "                        if os.path.exists(img1_path) and os.path.exists(img2_path):\n",
    "                            self.data.append(entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing entry: {e}\")\n",
    "        except PermissionError:\n",
    "            print(f\"Permission denied for file: {json_path}. Please ensure you have read permissions.\")\n",
    "        \n",
    "        print(f\"Total valid entries: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "\n",
    "        # Load images\n",
    "        img1_path = os.path.join(self.img_dir, entry['image1'])\n",
    "        img2_path = os.path.join(self.img_dir, entry['image2'])\n",
    "        img1 = Image.open(img1_path).convert('RGB')\n",
    "        img2 = Image.open(img2_path).convert('RGB')\n",
    "\n",
    "        # Apply image transformations\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        # Tokenize text\n",
    "        text1 = entry['text1']\n",
    "        text2 = entry['text2']\n",
    "        if self.text_tokenizer:\n",
    "            text1 = self.text_tokenizer(text1, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "            text2 = self.text_tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "        label = torch.tensor(entry['label'], dtype=torch.float32)\n",
    "\n",
    "        return img1, text1, img2, text2, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [sample for sample in batch if sample is not None]\n",
    "    return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a57c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(json_path, img_dir):\n",
    "    dataset = WikiDiverseDataset(json_path, img_dir)\n",
    "    train_dataset, val_test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    val_dataset, test_dataset = train_test_split(val_test_dataset, test_size=0.5, random_state=42)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Load data\n",
    "#train_loader, val_loader, test_loader = load_data(DATASET_PATH, IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99f3577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data Augmentation:\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import Compose, ColorJitter\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "\n",
    "transform = Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "701962c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Cross-Attention Mechanism:\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, dropout=0.1):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return self.layer_norm(attn_output + attn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "1af278cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Image and Text Sub-Networks:\n",
    "class ImageSubNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=4):\n",
    "        super(ImageSubNetworkWithAttention, self).__init__()\n",
    "        base_model = models.resnet50(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])  # Remove FC layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.cross_attention = CrossAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x, text_features):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        x = self.cross_attention(x.unsqueeze(1), text_features, text_features).squeeze(1)\n",
    "        return x\n",
    "\n",
    "class TextSubNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_heads=4):\n",
    "        super(TextSubNetworkWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.cross_attention = CrossAttention(hidden_dim, num_heads)\n",
    "\n",
    "    def forward(self, x, image_features):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        hidden = self.cross_attention(hidden.unsqueeze(1), image_features, image_features).squeeze(1)\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "ca74e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Siamese Network with Cross-Attention:\n",
    "class SiameseNetworkWithCrossAttention(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SiameseNetworkWithCrossAttention, self).__init__()\n",
    "        self.image_net = ImageSubNetworkWithAttention()\n",
    "        self.text_net = TextSubNetworkWithAttention(vocab_size)\n",
    "\n",
    "    def forward(self, img1, img2, text1, text2):\n",
    "        text_features1 = self.text_net(text1, None)\n",
    "        text_features2 = self.text_net(text2, None)\n",
    "        \n",
    "        img_embedding1 = self.image_net(img1, text_features1)\n",
    "        img_embedding2 = self.image_net(img2, text_features2)\n",
    "\n",
    "        text_embedding1 = self.text_net(text1, img_embedding1)\n",
    "        text_embedding2 = self.text_net(text2, img_embedding2)\n",
    "        combined_embedding1 = torch.cat([img_embedding1, text_embedding1], dim=1)\n",
    "        combined_embedding2 = torch.cat([img_embedding2, text_embedding2], dim=1)\n",
    "\n",
    "        return combined_embedding1, combined_embedding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc5287-0d83-485c-a700-4f9cd88e7113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Contrastive Loss:\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                          label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eee69e-2fde-4b5e-8672-317e5e3cfd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Training and Evaluation:\n",
    "#Train the model and track performance using metrics like ROC AUC.\n",
    "# Training and Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, ColorJitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "# # Load data\n",
    "train_loader, val_loader, test_loader = load_data(DATASET_PATH, IMAGE_DIR)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "def train(model, device, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        valid_batch = [sample for sample in batch if sample is not None]\n",
    "        if len(valid_batch) == 5:  # Check if all elements are valid\n",
    "            img1, text1, img2, text2, labels = valid_batch\n",
    "            img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "            \n",
    "            combined_embedding1, combined_embedding2 = model(img1, img2, text1, text2)\n",
    "            labels = labels.unsqueeze(1).float()\n",
    "            \n",
    "            loss = criterion(combined_embedding1, combined_embedding2, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            print(f\"Warning: Invalid batch encountered. Skipping this iteration.\")\n",
    "    \n",
    "    return total_loss / len(loader)  # Calculate average loss per batch\n",
    "\n",
    "def evaluate(model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            img1, text1, img2, text2 = batch\n",
    "            img1, text1, img2, text2 = img1.to(device), text1.to(device), img2.to(device), text2.to(device)\n",
    "            \n",
    "            combined_embedding1, combined_embedding2 = model(img1, img2, text1, text2)\n",
    "            similarity = F.cosine_similarity(combined_embedding1, combined_embedding2)\n",
    "            predictions.extend(similarity.cpu().numpy())\n",
    "            true_labels.extend(batch[-1].cpu().numpy())\n",
    "    \n",
    "    return accuracy_score(true_labels, (predictions > 0.5).astype(int)) * 100  # Return accuracy as percentage\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_accuracy = evaluate(model, device, val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_accuracy = evaluate(model, device, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79e615-8623-40c1-90b0-cd4c89797b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# import torch.nn.functional as F\n",
    "# import torch.nn.functional as F\n",
    "# from torchvision.transforms import Compose, ColorJitter\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # # Load data\n",
    "# train_loader, val_loader, test_loader = load_data(DATASET_PATH, IMAGE_DIR)\n",
    "\n",
    "# # Training loop\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "# # # Main execution block\n",
    "# # # Load data\n",
    "# # train_loader, val_loader, test_loader = load_data(DATASET_PATH, IMAGE_DIR)\n",
    "\n",
    "# # if train_loader is None or val_loader is None or test_loader is None:\n",
    "# #     print(\"Failed to load data. Please check your dataset path and permissions.\")\n",
    "# # else:\n",
    "# #     # Training loop\n",
    "# #     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# #     model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "# #     optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# #     criterion = nn.CosineEmbeddingLoss()\n",
    "# def train(model, device, loader, optimizer, criterion):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     total_cosine_loss = 0\n",
    "#     total_triplet_loss = 0\n",
    "#     total_contrastive_loss = 0\n",
    "#     total_correct = 0\n",
    "    \n",
    "#     for batch in loader:\n",
    "#         valid_batch = [sample for sample in batch if sample is not None]\n",
    "#         if len(valid_batch) == 5:  # Check if all elements are valid\n",
    "#             img1, text1, img2, text2, labels = valid_batch\n",
    "#             img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "            \n",
    "#             combined_embedding1, combined_embedding2 = model(img1, img2, text1, text2)\n",
    "            \n",
    "#             # Cosine Embedding Loss\n",
    "#             cosine_loss = criterion(combined_embedding1, combined_embedding2, labels)\n",
    "            \n",
    "#             # Triplet Loss\n",
    "#             triplet_loss = F.triplet_margin_loss(combined_embedding1, combined_embedding2, labels, reduction='mean')\n",
    "            \n",
    "#             # Contrastive Loss\n",
    "#             contrastive_loss = F.cosine_embedding_loss(combined_embedding1, combined_embedding2, labels)\n",
    "            \n",
    "#             # Total Loss\n",
    "#             loss = cosine_loss + 0.5 * triplet_loss + 0.5 * contrastive_loss\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "#             total_cosine_loss += cosine_loss.item()\n",
    "#             total_triplet_loss += triplet_loss.item()\n",
    "#             total_contrastive_loss += contrastive_loss.item()\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             # Calculate accuracy\n",
    "#             similarity = F.cosine_similarity(combined_embedding1, combined_embedding2)\n",
    "#             predictions = (similarity > 0.5).float()\n",
    "#             total_correct += (predictions == labels).sum().item()\n",
    "    \n",
    "#     accuracy = total_correct / (len(loader) * BATCH_SIZE)\n",
    "    \n",
    "#     return {\n",
    "#         'total_loss': total_loss / len(loader),\n",
    "#         'cosine_loss': total_cosine_loss / len(loader),\n",
    "#         'triplet_loss': total_triplet_loss / len(loader),\n",
    "#         'contrastive_loss': total_contrastive_loss / len(loader),\n",
    "#         'accuracy': accuracy\n",
    "#     }\n",
    "\n",
    "# def evaluate(model, device, loader):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     true_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in loader:\n",
    "#             img1, text1, img2, text2 = batch\n",
    "#             img1, text1, img2, text2 = img1.to(device), text1.to(device), img2.to(device), text2.to(device)\n",
    "            \n",
    "#             combined_embedding1, combined_embedding2 = model(img1, img2, text1, text2)\n",
    "#             similarity = F.cosine_similarity(combined_embedding1, combined_embedding2)\n",
    "#             predictions.extend(similarity.cpu().numpy())\n",
    "#             true_labels.extend(batch[-1].cpu().numpy())\n",
    "    \n",
    "#     predictions_binary = (predictions > 0.5).astype(int)\n",
    "    \n",
    "#     accuracy = accuracy_score(true_labels, predictions_binary)\n",
    "#     auc = roc_auc_score(true_labels, predictions)\n",
    "#     precision = precision_score(true_labels, predictions_binary)\n",
    "#     recall = recall_score(true_labels, predictions_binary)\n",
    "#     f1 = f1_score(true_labels, predictions_binary)\n",
    "    \n",
    "#     return {\n",
    "#         'accuracy': accuracy * 100,\n",
    "#         'auc': auc * 100,\n",
    "#         'precision': precision * 100,\n",
    "#         'recall': recall * 100,\n",
    "#         'f1_score': f1 * 100\n",
    "#     }\n",
    "\n",
    "# # Training loop\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     train_results = train(model, device, train_loader, optimizer, criterion)\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_results['total_loss']:.4f}\")\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Train Accuracy: {train_results['accuracy']:.2f}%\")\n",
    "    \n",
    "#     # Evaluate on validation set\n",
    "#     val_results = evaluate(model, device, val_loader)\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Val Accuracy: {val_results['accuracy']:.2f}%\")\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Val AUC: {val_results['auc']:.2f}%\")\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Val Precision: {val_results['precision']:.2f}%\")\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Val Recall: {val_results['recall']:.2f}%\")\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Val F1 Score: {val_results['f1_score']:.2f}%\")\n",
    "\n",
    "# # Final evaluation on test set\n",
    "# test_results = evaluate(model, device, test_loader)\n",
    "# print(f\"Test Accuracy: {test_results['accuracy']:.2f}%\")\n",
    "# print(f\"Test AUC: {test_results['auc']:.2f}%\")\n",
    "# print(f\"Test Precision: {test_results['precision']:.2f}%\")\n",
    "# print(f\"Test Recall: {test_results['recall']:.2f}%\")\n",
    "# print(f\"Test F1 Score: {test_results['f1_score']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76db11-09c3-4a3a-ab20-1114c70d7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Training and Evaluation:\n",
    "# #Train the model and track performance using metrics like ROC AUC.\n",
    "# # Training and Evaluation\n",
    "\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Assuming other required variables (e.g., VOCAB_SIZE, LEARNING_RATE, etc.) are defined elsewhere\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)  # Ensure the model is initialized properly\n",
    "# criterion = ContrastiveLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     for img1, text1, img2, text2, label in train_loader:\n",
    "#         img1, text1, img2, text2, label = img1.to(device), text1.to(device), img2.to(device), text2.to(device), label.to(device)\n",
    "\n",
    "#         # Forward pass through the Siamese network\n",
    "#         img_embed1, img_embed2 = model.forward_once(img1), model.forward_once(img2)\n",
    "#         text_embed1, text_embed2 = text_encoder(text1), text_encoder(text2)\n",
    "\n",
    "#         # Combine embeddings\n",
    "#         combined_embed1 = torch.cat([img_embed1, text_embed1], dim=1)\n",
    "#         combined_embed2 = torch.cat([img_embed2, text_embed2], dim=1)\n",
    "\n",
    "#         # Compute similarity and loss\n",
    "#         loss = criterion(combined_embed1, combined_embed2, label)\n",
    "\n",
    "#         # Backpropagation and optimizer step\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()    \n",
    "        \n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#     train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "#     # Validation Phase\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     all_labels = []\n",
    "#     all_preds = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for img1, text1, label in test_loader:\n",
    "#             img1, text1, label = img1.to(device), text1.to(device), label.to(device)\n",
    "#             output1, output2 = model(img1, img1, text1, text1)\n",
    "#             loss = criterion(output1, output2, label)\n",
    "#             val_loss += loss.item()\n",
    "            \n",
    "#             # Calculate pairwise distance between outputs for prediction\n",
    "#             preds = torch.nn.functional.pairwise_distance(output1, output2)  # Smaller distance => closer similarity\n",
    "#             all_labels.extend(label.cpu().numpy())  # Collect actual labels\n",
    "#             all_preds.extend(preds.cpu().numpy())  # Collect predicted distances\n",
    "\n",
    "#     val_losses.append(val_loss / len(test_loader))\n",
    "\n",
    "#     # Calculate ROC AUC score based on predicted distances and true labels\n",
    "#     roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "#     print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# # Plotting the training and validation loss curves\n",
    "# plt.figure()\n",
    "# plt.plot(range(1, EPOCHS + 1), train_losses, label=\"Train Loss\")\n",
    "# plt.plot(range(1, EPOCHS + 1), val_losses, label=\"Validation Loss\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb897d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, loss):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss,\n",
    "    }, f'model_epoch_{epoch}.pth')\n",
    "\n",
    "def load_model(model, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['loss']\n",
    "\n",
    "# Save model after training\n",
    "save_model(model, EPOCHS, train_loss)\n",
    "\n",
    "# Load model for inference\n",
    "checkpoint_path = f'model_epoch_{EPOCHS}.pth'\n",
    "epoch, loss = load_model(model, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c5801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cc7231b-0602-4a7c-9bc2-4a5a70008612",
   "metadata": {},
   "source": [
    "### END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab2c7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
