{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd4516a-7cc7-47fd-b481-597c06f239b5",
   "metadata": {},
   "source": [
    "# An overview\n",
    "Siamese networks consist of two identical sub-networks that share weights and learn to compute the similarity between two input samples. The goal is to learn embeddings such that similar inputs are close in the embedding space, while dissimilar inputs are far apart. For the WikiDiverse dataset, where we have image-caption pairs, we can build a Siamese network that processes text and image data (or just one modality like text or image) and learns to compute similarity between two entities from the knowledge base.\n",
    "* Siamese Network Structure: Two identical sub-networks that compute embeddings for input pairs and learn their similarity\n",
    "* Application: For WikiDiverse, compute similarity between image-caption pairs to link knowledge-base entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746e43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "1e5c4395-eaa0-4786-9053-fcbd93a81654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dataset Path and Hyperparameters:\n",
    "DATASET_PATH = r'C:\\Users\\Min Dator\\aics-project\\wikidiverse_w_cands'\n",
    "IMAGE_DIR = os.path.join(DATASET_PATH, \"wikinewsImgs\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "EMBED_DIM = 256\n",
    "VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "d3f97be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Helper Functions:\n",
    "def get_image_path(url, img_dir):\n",
    "    filename = url.split('/')[-1]\n",
    "    prefix = hashlib.md5(filename.encode()).hexdigest()\n",
    "    suffix = re.sub(r'(\\S+(?=\\.(jpg|jpeg|png|svg)))', '', filename, flags=re.IGNORECASE)\n",
    "    local_path = os.path.join(img_dir, f\"{prefix}{suffix}\".replace('.svg', '.png'))\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "074161e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. WikiDiverse Dataset Class:\n",
    "class WikiDiverseDataset(Dataset):\n",
    "    def __init__(self, json_path, img_dir, transform=None, text_tokenizer=None):\n",
    "        self.data = []\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        \n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                for entry in json.load(f):\n",
    "                    try:\n",
    "                        img1_path = os.path.join(img_dir, entry['image1'])\n",
    "                        img2_path = os.path.join(img_dir, entry['image2'])\n",
    "                        \n",
    "                        if os.path.exists(img1_path) and os.path.exists(img2_path):\n",
    "                            self.data.append(entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing entry: {e}\")\n",
    "        except PermissionError:\n",
    "            print(f\"Permission denied for file: {json_path}. Please ensure you have read permissions.\")\n",
    "        \n",
    "        print(f\"Total valid entries: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "\n",
    "        # Load images\n",
    "        img1_path = os.path.join(self.img_dir, entry['image1'])\n",
    "        img2_path = os.path.join(self.img_dir, entry['image2'])\n",
    "        img1 = Image.open(img1_path).convert('RGB')\n",
    "        img2 = Image.open(img2_path).convert('RGB')\n",
    "\n",
    "        # Apply image transformations\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        # Tokenize text\n",
    "        text1 = entry['text1']\n",
    "        text2 = entry['text2']\n",
    "        if self.text_tokenizer:\n",
    "            text1 = self.text_tokenizer(text1, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "            text2 = self.text_tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "        label = torch.tensor(entry['label'], dtype=torch.float32)\n",
    "\n",
    "        return img1, text1, img2, text2, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [sample for sample in batch if sample is not None]\n",
    "    return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a57c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(json_path, img_dir):\n",
    "    dataset = WikiDiverseDataset(json_path, img_dir)\n",
    "    train_dataset, val_test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    val_dataset, test_dataset = train_test_split(val_test_dataset, test_size=0.5, random_state=42)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Load data\n",
    "#train_loader, val_loader, test_loader = load_data(DATASET_PATH, IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99f3577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data Augmentation:\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import Compose, ColorJitter\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "\n",
    "transform = Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "701962c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Cross-Attention Mechanism:\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, dropout=0.1):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return self.layer_norm(attn_output + attn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "1af278cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Image and Text Sub-Networks:\n",
    "class ImageSubNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=4):\n",
    "        super(ImageSubNetworkWithAttention, self).__init__()\n",
    "        base_model = models.resnet50(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])  # Remove FC layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.cross_attention = CrossAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x, text_features):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        x = self.cross_attention(x.unsqueeze(1), text_features, text_features).squeeze(1)\n",
    "        return x\n",
    "\n",
    "class TextSubNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_heads=4):\n",
    "        super(TextSubNetworkWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.cross_attention = CrossAttention(hidden_dim, num_heads)\n",
    "\n",
    "    def forward(self, x, image_features):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        hidden = self.cross_attention(hidden.unsqueeze(1), image_features, image_features).squeeze(1)\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "ca74e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Siamese Network with Cross-Attention:\n",
    "class SiameseNetworkWithCrossAttention(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SiameseNetworkWithCrossAttention, self).__init__()\n",
    "        self.image_net = ImageSubNetworkWithAttention()\n",
    "        self.text_net = TextSubNetworkWithAttention(vocab_size)\n",
    "\n",
    "    def forward(self, img1, img2, text1, text2):\n",
    "        text_features1 = self.text_net(text1, None)\n",
    "        text_features2 = self.text_net(text2, None)\n",
    "        \n",
    "        img_embedding1 = self.image_net(img1, text_features1)\n",
    "        img_embedding2 = self.image_net(img2, text_features2)\n",
    "\n",
    "        text_embedding1 = self.text_net(text1, img_embedding1)\n",
    "        text_embedding2 = self.text_net(text2, img_embedding2)\n",
    "        combined_embedding1 = torch.cat([img_embedding1, text_embedding1], dim=1)\n",
    "        combined_embedding2 = torch.cat([img_embedding2, text_embedding2], dim=1)\n",
    "\n",
    "        return combined_embedding1, combined_embedding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "7924f78c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\\\\\Users\\\\\\\\Min Dator\\\\\\\\AI project\\\\\\\\aics-project\\\\\\\\wikidiverse_w_cands\\\\wikinewsImgs\\\\abc77ae74d5b046c8b569191ccf39a3b.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[544], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCosineEmbeddingLoss()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m---> 52\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[544], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      6\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m      8\u001b[0m     valid_batch \u001b[38;5;241m=\u001b[39m [sample \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m batch \u001b[38;5;28;01mif\u001b[39;00m sample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(valid_batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m:  \u001b[38;5;66;03m# Check if all elements are valid\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[487], line 16\u001b[0m, in \u001b[0;36mWikiDiverseDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 16\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     18\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\PIL\\Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\\\\\Users\\\\\\\\Min Dator\\\\\\\\AI project\\\\\\\\aics-project\\\\\\\\wikidiverse_w_cands\\\\wikinewsImgs\\\\abc77ae74d5b046c8b569191ccf39a3b.jpg'"
     ]
    }
   ],
   "source": [
    "#9. Training and Evaluation:\n",
    "#Train the model and track performance using metrics like ROC AUC.\n",
    "# Training and Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, ColorJitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "# # Load data\n",
    "train_loader, val_loader, test_loader = load_data(DATASET_PATH, IMAGE_DIR)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "def train(model, device, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        valid_batch = [sample for sample in batch if sample is not None]\n",
    "        if len(valid_batch) == 5:  # Check if all elements are valid\n",
    "            img1, text1, img2, text2, labels = valid_batch\n",
    "            img1, text1, img2, text2, labels = img1.to(device), text1.to(device), img2.to(device), text2.to(device), labels.to(device)\n",
    "            \n",
    "            combined_embedding1, combined_embedding2 = model(img1, img2, text1, text2)\n",
    "            labels = labels.unsqueeze(1).float()\n",
    "            \n",
    "            loss = criterion(combined_embedding1, combined_embedding2, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            print(f\"Warning: Invalid batch encountered. Skipping this iteration.\")\n",
    "    \n",
    "    return total_loss / len(loader)  # Calculate average loss per batch\n",
    "\n",
    "def evaluate(model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            img1, text1, img2, text2 = batch\n",
    "            img1, text1, img2, text2 = img1.to(device), text1.to(device), img2.to(device), text2.to(device)\n",
    "            \n",
    "            combined_embedding1, combined_embedding2 = model(img1, img2, text1, text2)\n",
    "            similarity = F.cosine_similarity(combined_embedding1, combined_embedding2)\n",
    "            predictions.extend(similarity.cpu().numpy())\n",
    "            true_labels.extend(batch[-1].cpu().numpy())\n",
    "    \n",
    "    return accuracy_score(true_labels, (predictions > 0.5).astype(int)) * 100  # Return accuracy as percentage\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_accuracy = evaluate(model, device, val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_accuracy = evaluate(model, device, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76db11-09c3-4a3a-ab20-1114c70d7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Training and Evaluation:\n",
    "# #Train the model and track performance using metrics like ROC AUC.\n",
    "# # Training and Evaluation\n",
    "\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Assuming other required variables (e.g., VOCAB_SIZE, LEARNING_RATE, etc.) are defined elsewhere\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)  # Ensure the model is initialized properly\n",
    "# criterion = ContrastiveLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     for img1, text1, img2, text2, label in train_loader:\n",
    "#         img1, text1, img2, text2, label = img1.to(device), text1.to(device), img2.to(device), text2.to(device), label.to(device)\n",
    "\n",
    "#         # Forward pass through the Siamese network\n",
    "#         img_embed1, img_embed2 = model.forward_once(img1), model.forward_once(img2)\n",
    "#         text_embed1, text_embed2 = text_encoder(text1), text_encoder(text2)\n",
    "\n",
    "#         # Combine embeddings\n",
    "#         combined_embed1 = torch.cat([img_embed1, text_embed1], dim=1)\n",
    "#         combined_embed2 = torch.cat([img_embed2, text_embed2], dim=1)\n",
    "\n",
    "#         # Compute similarity and loss\n",
    "#         loss = criterion(combined_embed1, combined_embed2, label)\n",
    "\n",
    "#         # Backpropagation and optimizer step\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()    \n",
    "        \n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#     train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "#     # Validation Phase\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     all_labels = []\n",
    "#     all_preds = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for img1, text1, label in test_loader:\n",
    "#             img1, text1, label = img1.to(device), text1.to(device), label.to(device)\n",
    "#             output1, output2 = model(img1, img1, text1, text1)\n",
    "#             loss = criterion(output1, output2, label)\n",
    "#             val_loss += loss.item()\n",
    "            \n",
    "#             # Calculate pairwise distance between outputs for prediction\n",
    "#             preds = torch.nn.functional.pairwise_distance(output1, output2)  # Smaller distance => closer similarity\n",
    "#             all_labels.extend(label.cpu().numpy())  # Collect actual labels\n",
    "#             all_preds.extend(preds.cpu().numpy())  # Collect predicted distances\n",
    "\n",
    "#     val_losses.append(val_loss / len(test_loader))\n",
    "\n",
    "#     # Calculate ROC AUC score based on predicted distances and true labels\n",
    "#     roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "#     print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# # Plotting the training and validation loss curves\n",
    "# plt.figure()\n",
    "# plt.plot(range(1, EPOCHS + 1), train_losses, label=\"Train Loss\")\n",
    "# plt.plot(range(1, EPOCHS + 1), val_losses, label=\"Validation Loss\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb897d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, loss):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss,\n",
    "    }, f'model_epoch_{epoch}.pth')\n",
    "\n",
    "def load_model(model, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['loss']\n",
    "\n",
    "# Save model after training\n",
    "save_model(model, EPOCHS, train_loss)\n",
    "\n",
    "# Load model for inference\n",
    "checkpoint_path = f'model_epoch_{EPOCHS}.pth'\n",
    "epoch, loss = load_model(model, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c5801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cc7231b-0602-4a7c-9bc2-4a5a70008612",
   "metadata": {},
   "source": [
    "### END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab2c7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
