{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd4516a-7cc7-47fd-b481-597c06f239b5",
   "metadata": {},
   "source": [
    "# 1. An overview\n",
    "Siamese networks consist of two identical sub-networks that share weights and learn to compute the similarity between two input samples. The goal is to learn embeddings such that similar inputs are close in the embedding space, while dissimilar inputs are far apart. For the WikiDiverse dataset, where we have image-caption pairs, we can build a Siamese network that processes text and image data (or just one modality like text or image) and learns to compute similarity between two entities from the knowledge base.\n",
    "* Siamese Network Structure: Two identical sub-networks that compute embeddings for input pairs and learn their similarity\n",
    "* Application: For WikiDiverse, compute similarity between image-caption pairs to link knowledge-base entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42592d16-a2d0-4eda-8830-3e653e42ae57",
   "metadata": {},
   "source": [
    "# 2. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5c4395-eaa0-4786-9053-fcbd93a81654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score  \n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589cf95c-8dfd-4824-bc71-ff9b907c612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "DATASET_PATH = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_w_cands\\wikidiverse_w_cands\"\n",
    "IMAGE_DIR = os.path.join(DATASET_PATH, \"images\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "EMBED_DIM = 256\n",
    "VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb558c-3888-4d6f-beda-01233f737292",
   "metadata": {},
   "source": [
    "# 3. Data Class\n",
    "Prepare the dataset and dataloader for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b70ba9-7088-4613-b734-cc95181f21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDiverseDataset(Dataset):\n",
    "    def __init__(self, image_paths, captions, labels, tokenizer, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.captions = captions\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        text = self.tokenizer(self.captions[idx], truncation=True, padding=\"max_length\", max_length=100, return_tensors=\"pt\")\n",
    "        text_input = text[\"input_ids\"].squeeze(0)\n",
    "        \n",
    "        return image, text_input, torch.tensor(self.labels[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba25a3-53e1-4996-863d-be79f0ef2b2d",
   "metadata": {},
   "source": [
    "# 4. Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa1dfb-2a28-4bbc-9f4b-a98aeb776e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data \n",
    "data_file = os.path.join(DATASET_PATH, \"captions_and_labels.csv\")  \n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "# Generate full paths to images\n",
    "image_paths = [os.path.join(IMAGE_DIR, filename) for filename in df['image_filename']]\n",
    "\n",
    "# Captions and labels\n",
    "captions = df['caption'].tolist()\n",
    "labels = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef32551-9cca-402d-8d63-d769555af040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Train-test split\n",
    "train_paths, test_paths, train_captions, test_captions, train_labels, test_labels = train_test_split(\n",
    "    image_paths, captions, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab958b-e3a9-4b4a-ac65-417196e7e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and Dataloaders\n",
    "train_dataset = WikiDiverseDataset(train_paths, train_captions, train_labels, tokenizer, transform)\n",
    "test_dataset = WikiDiverseDataset(test_paths, test_captions, test_labels, tokenizer, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f7a15-c1de-43a6-9c25-7c9fda86afcb",
   "metadata": {},
   "source": [
    "# 5. Cross Attention Module\n",
    "The cross-attention mechanism allows one modality (e.g., text) to attend to another (e.g., image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a551a171-7229-467d-8f2d-5d831aa0c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, dropout=0.1):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return self.layer_norm(query + attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40bc2f1-cef9-42b4-ab51-3a2bc93df697",
   "metadata": {},
   "source": [
    "# 6. Sub-Networks with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78301d42-26c7-431f-a53d-cd32f0d62cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSubNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=4):\n",
    "        super(ImageSubNetworkWithAttention, self).__init__()\n",
    "        base_model = models.resnet50(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])  # Remove FC layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.cross_attention = CrossAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x, text_features):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        x = self.cross_attention(x.unsqueeze(1), text_features, text_features).squeeze(1)\n",
    "        return x\n",
    "\n",
    "class TextSubNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_heads=4):\n",
    "        super(TextSubNetworkWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.cross_attention = CrossAttention(hidden_dim, num_heads)\n",
    "\n",
    "    def forward(self, x, image_features):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        hidden = self.cross_attention(hidden.unsqueeze(1), image_features, image_features).squeeze(1)\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8891675c-ffec-4383-a24f-6309cd7a2e36",
   "metadata": {},
   "source": [
    "# 7. Siamese Network with Cross-Attention\n",
    "This combines image and text embeddings using cross-attention and computes similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3ad21c-8254-471c-af48-2e8815d8b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkWithCrossAttention(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SiameseNetworkWithCrossAttention, self).__init__()\n",
    "        self.image_net = ImageSubNetworkWithAttention()\n",
    "        self.text_net = TextSubNetworkWithAttention(vocab_size)\n",
    "\n",
    "    def forward(self, img1, img2, text1, text2):\n",
    "        text_features1 = self.text_net(text1, None)\n",
    "        text_features2 = self.text_net(text2, None)\n",
    "        \n",
    "        img_embedding1 = self.image_net(img1, text_features1)\n",
    "        img_embedding2 = self.image_net(img2, text_features2)\n",
    "\n",
    "        text_embedding1 = self.text_net(text1, img_embedding1)\n",
    "        text_embedding2 = self.text_net(text2, img_embedding2)\n",
    "\n",
    "        combined_embedding1 = torch.cat([img_embedding1, text_embedding1], dim=1)\n",
    "        combined_embedding2 = torch.cat([img_embedding2, text_embedding2], dim=1)\n",
    "        \n",
    "        return combined_embedding1, combined_embedding2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b4f026-58c6-4420-b309-abd3ee98600d",
   "metadata": {},
   "source": [
    "# 8. Loss Function\n",
    "The contrastive loss function encourages similar pairs to have closer embeddings and dissimilar pairs to have distant embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6d62c8-f1fd-4b5e-86f7-ae68ca139cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                          label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a291d73-2869-4e08-bc45-d68dae02c82a",
   "metadata": {},
   "source": [
    "# 9. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4519b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for img1, text1, label in train_loader:\n",
    "        img1, text1, label = img1.to(device), text1.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(img1, img1, text1, text1)\n",
    "        loss = criterion(output1, output2, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for img1, text1, label in test_loader:\n",
    "            img1, text1, label = img1.to(device), text1.to(device), label.to(device)\n",
    "            output1, output2 = model(img1, img1, text1, text1)\n",
    "            val_loss += criterion(output1, output2, label).item()\n",
    "            preds = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    val_losses.append(val_loss / len(test_loader))\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure()\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, EPOCHS + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65077a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\nStarting Evaluation...\")\n",
    "#accuracy, precision, recall, f1 = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f5b932-7a10-4acf-9207-5d9f9dd69764",
   "metadata": {},
   "source": [
    "##### END #### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
