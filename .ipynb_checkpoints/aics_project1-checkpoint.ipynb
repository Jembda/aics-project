{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd4516a-7cc7-47fd-b481-597c06f239b5",
   "metadata": {},
   "source": [
    "# An overview\n",
    "Siamese networks consist of two identical sub-networks that share weights and learn to compute the similarity between two input samples. The goal is to learn embeddings such that similar inputs are close in the embedding space, while dissimilar inputs are far apart. For the WikiDiverse dataset, where we have image-caption pairs, we can build a Siamese network that processes text and image data (or just one modality like text or image) and learns to compute similarity between two entities from the knowledge base.\n",
    "* Siamese Network Structure: Two identical sub-networks that compute embeddings for input pairs and learn their similarity\n",
    "* Application: For WikiDiverse, compute similarity between image-caption pairs to link knowledge-base entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42592d16-a2d0-4eda-8830-3e653e42ae57",
   "metadata": {},
   "source": [
    "# 1. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e5c4395-eaa0-4786-9053-fcbd93a81654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score  # Adjustment\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589cf95c-8dfd-4824-bc71-ff9b907c612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "DATASET_PATH = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_w_cands\\wikidiverse_w_cands\"\n",
    "IMAGE_DIR = os.path.join(DATASET_PATH, \"images\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "EMBED_DIM = 256\n",
    "VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb558c-3888-4d6f-beda-01233f737292",
   "metadata": {},
   "source": [
    "# 2. Data Class\n",
    "Prepare the dataset and dataloader for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b70ba9-7088-4613-b734-cc95181f21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDiverseDataset(Dataset):\n",
    "    def __init__(self, image_paths, captions, labels, tokenizer, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.captions = captions\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        text = self.tokenizer(self.captions[idx], truncation=True, padding=\"max_length\", max_length=100, return_tensors=\"pt\")\n",
    "        text_input = text[\"input_ids\"].squeeze(0)\n",
    "        \n",
    "        return image, text_input, torch.tensor(self.labels[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba25a3-53e1-4996-863d-be79f0ef2b2d",
   "metadata": {},
   "source": [
    "# 3. Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa1dfb-2a28-4bbc-9f4b-a98aeb776e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data \n",
    "data_file = os.path.join(DATASET_PATH, \"captions_and_labels.csv\")  # Your CSV file with captions and labels\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "# Generate full paths to images\n",
    "image_paths = [os.path.join(IMAGE_DIR, filename) for filename in df['image_filename']]\n",
    "\n",
    "# Captions and labels\n",
    "captions = df['caption'].tolist()\n",
    "labels = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef32551-9cca-402d-8d63-d769555af040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Train-test split\n",
    "train_paths, test_paths, train_captions, test_captions, train_labels, test_labels = train_test_split(\n",
    "    image_paths, captions, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab958b-e3a9-4b4a-ac65-417196e7e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and Dataloaders\n",
    "train_dataset = WikiDiverseDataset(train_paths, train_captions, train_labels, tokenizer, transform)\n",
    "test_dataset = WikiDiverseDataset(test_paths, test_captions, test_labels, tokenizer, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f7a15-c1de-43a6-9c25-7c9fda86afcb",
   "metadata": {},
   "source": [
    "# 4. Cross Attention Module\n",
    "The cross-attention mechanism allows one modality (e.g., text) to attend to another (e.g., image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a551a171-7229-467d-8f2d-5d831aa0c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, dropout=0.1):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return self.layer_norm(query + attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40bc2f1-cef9-42b4-ab51-3a2bc93df697",
   "metadata": {},
   "source": [
    "# 5. Sub-Networks with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78301d42-26c7-431f-a53d-cd32f0d62cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSubNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=4):\n",
    "        super(ImageSubNetworkWithAttention, self).__init__()\n",
    "        base_model = models.resnet50(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])  # Remove FC layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.cross_attention = CrossAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x, text_features):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        x = self.cross_attention(x.unsqueeze(1), text_features, text_features).squeeze(1)\n",
    "        return x\n",
    "\n",
    "class TextSubNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_heads=4):\n",
    "        super(TextSubNetworkWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.cross_attention = CrossAttention(hidden_dim, num_heads)\n",
    "\n",
    "    def forward(self, x, image_features):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        hidden = self.cross_attention(hidden.unsqueeze(1), image_features, image_features).squeeze(1)\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8891675c-ffec-4383-a24f-6309cd7a2e36",
   "metadata": {},
   "source": [
    "# 6. Siamese Network with Cross-Attention\n",
    "This combines image and text embeddings using cross-attention and computes similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3ad21c-8254-471c-af48-2e8815d8b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkWithCrossAttention(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SiameseNetworkWithCrossAttention, self).__init__()\n",
    "        self.image_net = ImageSubNetworkWithAttention()\n",
    "        self.text_net = TextSubNetworkWithAttention(vocab_size)\n",
    "\n",
    "    def forward(self, img1, img2, text1, text2):\n",
    "        text_features1 = self.text_net(text1, None)\n",
    "        text_features2 = self.text_net(text2, None)\n",
    "        \n",
    "        img_embedding1 = self.image_net(img1, text_features1)\n",
    "        img_embedding2 = self.image_net(img2, text_features2)\n",
    "\n",
    "        text_embedding1 = self.text_net(text1, img_embedding1)\n",
    "        text_embedding2 = self.text_net(text2, img_embedding2)\n",
    "\n",
    "        combined_embedding1 = torch.cat([img_embedding1, text_embedding1], dim=1)\n",
    "        combined_embedding2 = torch.cat([img_embedding2, text_embedding2], dim=1)\n",
    "        \n",
    "        return combined_embedding1, combined_embedding2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b4f026-58c6-4420-b309-abd3ee98600d",
   "metadata": {},
   "source": [
    "# 7. Loss Function\n",
    "The contrastive loss function encourages similar pairs to have closer embeddings and dissimilar pairs to have distant embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6d62c8-f1fd-4b5e-86f7-ae68ca139cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                          label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a291d73-2869-4e08-bc45-d68dae02c82a",
   "metadata": {},
   "source": [
    "# 8. Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c3dd5-6a2d-4b5e-816f-8ed1a05e5430",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e142463-e59c-4107-a2a8-117f75c93b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for img1, text1, label in train_loader:\n",
    "        img1, text1, label = img1.to(device), text1.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(img1, img1, text1, text1)\n",
    "        loss = criterion(output1, output2, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss / len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac11cda-c4ba-4e85-9e8d-489e704b6779",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7cea3-e277-4c5e-84e9-40d14657c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for img1, text1, label in test_loader:\n",
    "            img1, text1, label = img1.to(device), text1.to(device), label.to(device)\n",
    "            output1, output2 = model(img1, img1, text1, text1)\n",
    "            val_loss += criterion(output1, output2, label).item()\n",
    "            preds = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    val_losses.append(val_loss / len(test_loader))\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure()\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, EPOCHS + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e06cab6-70c7-4f96-885d-61156595a089",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df24cc6-b566-47fb-9cb4-294e39c3edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting Evaluation...\")\n",
    "accuracy, precision, recall, f1 = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e868056d-a91f-4d16-acda-252b307e51b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8f5b932-7a10-4acf-9207-5d9f9dd69764",
   "metadata": {},
   "source": [
    "##### END #### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b691c97-e1af-44c5-81aa-d0869539e774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5accc619-8544-487d-a19b-6855c834c8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00700b-cca6-4692-b367-be788ce47067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632e889-2ef7-4e5b-8e5b-60c7d09f1054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaadf85-df2f-4eee-9832-5e50ced760f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4413935-bbcb-44b0-b555-bd79212a672d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af42023-7894-4c36-aeeb-9bf9be84ed0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "936e68c5-b2bd-4eb2-8d7c-c177630a5fcf",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a601dc1-5b3a-45ba-9a4f-97dfbee72a1d",
   "metadata": {},
   "source": [
    "# Image Preprocessing\n",
    "We use a standard ResNet50 model for extracting features from images. Images are resized and normalized before feeding into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0eab6-21ea-4b75-9294-0551485136c5",
   "metadata": {},
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    return image_transforms(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f642fd20-e342-49ab-8ede-c9245167b875",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "We use a pretrained BERT tokenizer to tokenize and encode text inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18055f1c-bf5e-4d99-b1de-b587c517e719",
   "metadata": {},
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess_text(text, max_length=100):\n",
    "    encoded = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    return encoded['input_ids'].squeeze(0), encoded['attention_mask'].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc68f88-6855-4eba-ba5a-ff1b6f815b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36f38959-4e26-4fa9-9d8a-13033c551217",
   "metadata": {},
   "source": [
    "# 3. Sub-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e65e9-90ab-4088-947d-73de4e66e0a2",
   "metadata": {},
   "source": [
    "# Image Embedding Network\n",
    "This network uses ResNet50 to extract image features and reduces them to a fixed-size embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1fd864-9918-47fc-93a3-fd261e99d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim=256):\n",
    "        super(ImageEmbedding, self).__init__()\n",
    "        base_model = models.resnet50(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])  # Remove the final classification layer\n",
    "        self.fc = nn.Linear(2048, embedding_dim)  # Reduce to embedding dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        embedding = self.fc(features)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6b7661-0b63-4372-a1d3-a4f2181beb0d",
   "metadata": {},
   "source": [
    "# Text Embedding Network\n",
    "This network uses BERT to extract text embeddings, followed by a linear layer to reduce dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b57b9-7493-4488-9818-4b334c6124e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim=256):\n",
    "        super(TextEmbedding, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(768, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.pooler_output  # CLS token representation\n",
    "        embedding = self.fc(cls_embedding)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6925d5f9-5616-4a38-ab90-860e54012830",
   "metadata": {},
   "source": [
    "# 4. Cross-Attention Mechanism\n",
    "The cross-attention mechanism allows one modality (e.g., text) to attend to another (e.g., image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a31b4-54e5-4298-8d73-82aa01a1a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        output = self.fc(attn_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141aab1-7a5c-4bfa-850c-3125496f6a47",
   "metadata": {},
   "source": [
    "# 5. Siamese Network with Cross-Attention\n",
    "This combines image and text embeddings using cross-attention and computes similarity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52030015-f8b3-4fec-9d43-e9f2efbf334f",
   "metadata": {},
   "source": [
    "class SiameseNetworkWithCrossAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, num_heads=4):\n",
    "        super(SiameseNetworkWithCrossAttention, self).__init__()\n",
    "        self.image_embedding = ImageEmbedding(embedding_dim)\n",
    "        self.text_embedding = TextEmbedding(embedding_dim)\n",
    "        self.cross_attention = CrossAttention(embed_dim=embedding_dim, num_heads=num_heads)\n",
    "        self.fc = nn.Linear(embedding_dim * 2, embedding_dim)  # Final layer after concatenation\n",
    "\n",
    "    def forward(self, img1, img2, text1, mask1, text2, mask2):\n",
    "        # Extract embeddings\n",
    "        img_emb1 = self.image_embedding(img1)\n",
    "        img_emb2 = self.image_embedding(img2)\n",
    "        text_emb1 = self.text_embedding(text1, mask1)\n",
    "        text_emb2 = self.text_embedding(text2, mask2)\n",
    "\n",
    "        # Apply cross-attention (optional but useful for alignment)\n",
    "        img_text_emb1 = self.cross_attention(text_emb1.unsqueeze(1), img_emb1.unsqueeze(1), img_emb1.unsqueeze(1)).squeeze(1)\n",
    "        img_text_emb2 = self.cross_attention(text_emb2.unsqueeze(1), img_emb2.unsqueeze(1), img_emb2.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        ## Combine embeddings\n",
    "        combined_emb1 = torch.cat((img_emb1, text_emb1), dim=1)  # text component included\n",
    "        combined_emb2 = torch.cat((img_emb2, text_emb2), dim=1)\n",
    "\n",
    "        # Pass the combined embeddings through a final fully connected layer\n",
    "        combined_emb1 = self.fc(combined_emb1)\n",
    "        combined_emb2 = self.fc(combined_emb2)\n",
    "\n",
    "        return combined_emb1, combined_emb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f7a26-5062-4c37-b783-7396edad0018",
   "metadata": {},
   "source": [
    "class SiameseNetworkWithCrossAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, num_heads=4):\n",
    "        super(SiameseNetworkWithCrossAttention, self).__init__()\n",
    "        self.image_embedding = ImageEmbedding(embedding_dim)\n",
    "        self.text_embedding = TextEmbedding(embedding_dim)\n",
    "        self.cross_attention = CrossAttention(embed_dim=embedding_dim, num_heads=num_heads)\n",
    "        self.fc = nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "\n",
    "    def forward(self, img1, img2, text1, mask1, text2, mask2):\n",
    "        # Extract embeddings\n",
    "        img_emb1 = self.image_embedding(img1)\n",
    "        img_emb2 = self.image_embedding(img2)\n",
    "        text_emb1 = self.text_embedding(text1, mask1)\n",
    "        text_emb2 = self.text_embedding(text2, mask2)\n",
    "\n",
    "        # Apply cross-attention\n",
    "        img_text_emb1 = self.cross_attention(text_emb1.unsqueeze(1), img_emb1.unsqueeze(1), img_emb1.unsqueeze(1)).squeeze(1)\n",
    "        img_text_emb2 = self.cross_attention(text_emb2.unsqueeze(1), img_emb2.unsqueeze(1), img_emb2.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Combine embeddings\n",
    "        combined_emb1 = torch.cat((img_emb1, img_text_emb1), dim=1) #\n",
    "        combined_emb2 = torch.cat((img_emb2, img_text_emb2), dim=1)\n",
    "\n",
    "        # Reduce to a single embedding\n",
    "        combined_emb1 = self.fc(combined_emb1)\n",
    "        combined_emb2 = self.fc(combined_emb2)\n",
    "\n",
    "        return combined_emb1, combined_emb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7de09d-6583-45b4-afc2-61d371a5101f",
   "metadata": {},
   "source": [
    "# 6. Loss Function\n",
    "The contrastive loss function encourages similar pairs to have closer embeddings and dissimilar pairs to have distant embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5356950-17d6-4135-93c8-909c146319e0",
   "metadata": {},
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, emb1, emb2, label):\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(emb1, emb2)\n",
    "        loss = (label * torch.square(euclidean_distance)) + \\\n",
    "               ((1 - label) * torch.square(torch.clamp(self.margin - euclidean_distance, min=0.0)))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26c25c-9871-4848-ad63-b5f8b934385c",
   "metadata": {},
   "source": [
    "# 7. Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a720f821-5d86-46c2-9c40-ad6871147327",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c274efa0-0ffe-46c4-b7ca-f7f88b2f50ea",
   "metadata": {},
   "source": [
    "model = SiameseNetworkWithCrossAttention()\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for img1, img2, text1, mask1, text2, mask2, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        emb1, emb2 = model(img1, img2, text1, mask1, text2, mask2)\n",
    "        loss = criterion(emb1, emb2, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b31926-38a5-4f77-87d9-7758876d4942",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9646c4-d829-4b43-a4a2-c725b1244895",
   "metadata": {},
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, test_loader, threshold=0.5):\n",
    "    model.eval()  \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for img1, img2, text1, mask1, text2, mask2, labels in test_loader:\n",
    "            # Move data to the same device as the model\n",
    "            img1, img2 = img1.to(device), img2.to(device)\n",
    "            text1, mask1 = text1.to(device), mask1.to(device)\n",
    "            text2, mask2 = text2.to(device), mask2.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass to compute embeddings\n",
    "            emb1, emb2 = model(img1, img2, text1, mask1, text2, mask2)\n",
    "\n",
    "            # Compute Euclidean distance between embeddings\n",
    "            euclidean_distance = torch.nn.functional.pairwise_distance(emb1, emb2)\n",
    "\n",
    "            # Convert distances to binary predictions using the threshold\n",
    "            predictions = (euclidean_distance < threshold).long()\n",
    "\n",
    "            # Append predictions and labels to lists\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Evaluation Metrics:\\n\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Assuming `test_loader` is your DataLoader for the test dataset\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = model.to(device)  # Move model to appropriate device\n",
    "\n",
    "# Evaluate the model\n",
    "#evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab2e25-ed63-466e-a60b-8b9180e83f9f",
   "metadata": {},
   "source": [
    "# 8. DataLoader \n",
    "Prepare the dataset and dataloader for training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9961a-f000-4503-bd1c-e56619cdacf8",
   "metadata": {},
   "source": [
    "class WikiDiverseDataset(Dataset):\n",
    "    def __init__(self, image_paths, texts, labels, tokenizer, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        img = preprocess_image(img_path)\n",
    "        input_ids, attention_mask = preprocess_text(text)\n",
    "\n",
    "        return img, input_ids, attention_mask, label\n",
    "\n",
    "# Usage\n",
    "train_dataset = WikiDiverseDataset(train_image_paths, train_texts, train_labels, tokenizer, image_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692f13c-8559-41aa-baf8-b01241cb13ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314ef0c-5ec8-4270-8e5d-eba92c05e679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d6bcf5f-41cf-481c-af56-676a689113d6",
   "metadata": {},
   "source": [
    "  #### END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009066f8-06cc-4bb5-ba96-feffa7cfdee2",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Dataset paths\n",
    "DATASET_PATH = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_w_cands\\wikidiverse_w_cands\"\n",
    "TRAIN_PATH = os.path.join(DATASET_PATH, \"train_w_10cands\")\n",
    "VALID_PATH = os.path.join(DATASET_PATH, \"valid_w_10cands\")\n",
    "TEST_PATH = os.path.join(DATASET_PATH, \"test_w_10cands\")\n",
    "\n",
    "# Verify that the directories exist\n",
    "for path in [TRAIN_PATH, VALID_PATH, TEST_PATH]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset directory not found: {path}\")\n",
    "print(\"All dataset directories found.\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "EMBED_DIM = 256\n",
    "VOCAB_SIZE = 10000  # For tokenizer\n",
    "\n",
    "# Dataset class\n",
    "class WikiDiverseDataset(Dataset):\n",
    "    def __init__(self, data_dir, tokenizer, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.captions = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load dataset from the directory\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        # Assuming data_dir contains files with images and captions\n",
    "        for file_name in os.listdir(self.data_dir):\n",
    "            if file_name.endswith(\".jpg\"):  # Process images\n",
    "                image_path = os.path.join(self.data_dir, file_name)\n",
    "                caption_path = image_path.replace(\".jpg\", \".txt\")\n",
    "                label_path = image_path.replace(\".jpg\", \".label\")\n",
    "\n",
    "                # Check for corresponding caption and label files\n",
    "                if os.path.exists(caption_path) and os.path.exists(label_path):\n",
    "                    self.image_paths.append(image_path)\n",
    "                    with open(caption_path, \"r\") as f:\n",
    "                        self.captions.append(f.read().strip())\n",
    "                    with open(label_path, \"r\") as f:\n",
    "                        self.labels.append(int(f.read().strip()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load and preprocess image\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Tokenize text\n",
    "        text = self.tokenizer(self.captions[idx], truncation=True, padding=\"max_length\", max_length=100, return_tensors=\"pt\")\n",
    "        text_input = text[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        # Return data\n",
    "        return image, text_input, torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Datasets and Dataloaders\n",
    "train_dataset = WikiDiverseDataset(TRAIN_PATH, tokenizer, transform)\n",
    "valid_dataset = WikiDiverseDataset(VALID_PATH, tokenizer, transform)\n",
    "test_dataset = WikiDiverseDataset(TEST_PATH, tokenizer, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(valid_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76445d8-5b33-4aae-b964-87c6ae2ce2f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfae9a8c-6594-45e3-a733-448f4fdd352d",
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# File paths\n",
    "DESC_FILE = r\"C:\\Users\\Min Dator\\aics-project\\wikipedia_entity2desc_filtered.tsv\"\n",
    "IMGS_FILE = r\"C:\\Users\\Min Dator\\aics-project\\wikipedia_entity2imgs.tsv\"\n",
    "DATASET_PATH = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_w_cands\\wikidiverse_w_cands\"\n",
    "\n",
    "# Check if files exist\n",
    "for file in [DESC_FILE, IMGS_FILE]:\n",
    "    if not os.path.exists(file):\n",
    "        raise FileNotFoundError(f\"File not found: {file}\")\n",
    "print(\"All files found.\")\n",
    "\n",
    "# Load TSV files\n",
    "desc_df = pd.read_csv(DESC_FILE, sep=\"\\t\", names=[\"entity_id\", \"description\"], header=None)\n",
    "imgs_df = pd.read_csv(IMGS_FILE, sep=\"\\t\", names=[\"entity_id\", \"image_path\"], header=None)\n",
    "\n",
    "# Merge the data\n",
    "data_df = pd.merge(imgs_df, desc_df, on=\"entity_id\")\n",
    "data_df[\"image_path\"] = data_df[\"image_path\"].apply(lambda x: os.path.join(DATASET_PATH, x))\n",
    "\n",
    "# Filter out entries where the image files don't exist\n",
    "data_df = data_df[data_df[\"image_path\"].apply(os.path.exists)]\n",
    "print(f\"Dataset loaded with {len(data_df)} samples.\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "EMBED_DIM = 256\n",
    "VOCAB_SIZE = 10000  # For tokenizer\n",
    "\n",
    "# Dataset class\n",
    "class WikiDiverseDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = row[\"image_path\"]\n",
    "        description = row[\"description\"]\n",
    "\n",
    "        # Load and preprocess image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Tokenize text\n",
    "        text = self.tokenizer(description, truncation=True, padding=\"max_length\", max_length=100, return_tensors=\"pt\")\n",
    "        text_input = text[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        # Dummy label for now (you can modify this to load actual labels)\n",
    "        label = torch.tensor(1.0, dtype=torch.float32)\n",
    "\n",
    "        return image, text_input, label\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_df, valid_df, test_df = (\n",
    "    data_df.iloc[:int(0.7 * len(data_df))],\n",
    "    data_df.iloc[int(0.7 * len(data_df)):int(0.85 * len(data_df))],\n",
    "    data_df.iloc[int(0.85 * len(data_df)):],\n",
    ")\n",
    "\n",
    "# Datasets and Dataloaders\n",
    "train_dataset = WikiDiverseDataset(train_df, tokenizer, transform)\n",
    "valid_dataset = WikiDiverseDataset(valid_df, tokenizer, transform)\n",
    "test_dataset = WikiDiverseDataset(test_df, tokenizer, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(valid_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11edb3-7775-4094-8d1d-b757416303bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b90facfa-68b0-47d7-83fc-5f9e16f78b6a",
   "metadata": {},
   "source": [
    "### END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9570f899-8c0b-42ee-82d5-81fb9e300512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32375490-89ba-4dcc-9314-b8c926c50baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db32a80a-6e60-4401-a258-5dbb84601755",
   "metadata": {},
   "source": [
    "# 1. Overview\n",
    "This implementation is a multimodal entity linking system that leverages both text and image information to perform entity disambiguation. The model uses BERT for text encoding and CLIP for image encoding. A cross-attention mechanism fuses text and image features, and a disambiguation head predicts the correct entity from a list of candidates. The system includes training and validation loops to optimize the model.\n",
    "# Key Components\n",
    "-Text Encoder: BERT-based textual feature extraction\n",
    "- Image Encoder: CLIP-based image feature extraction\n",
    "- Cross-Attention: Fuse text and image representations\n",
    "- Entity Disambiguation Head: Predict the correct entity\n",
    "- Training and Validation: Dataset handling, loss computation, and optimizer setup\n",
    "# Data Requiremnt (each sample in the data includes) \n",
    "- 'text': The textual description or context\n",
    "- 'image_path': Path to the corresponding image file\n",
    "- 'label': Index of the correct entity in the candidate list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9a43b-505c-4844-b073-416c2b69fb1e",
   "metadata": {},
   "source": [
    "# 2. Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31642bdd-308d-4e56-a919-c4aada296d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "#from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel, get_scheduler\n",
    "#import torch.nn.functional as F\n",
    "#from PIL import Image\n",
    "#from tqdm import tqdm\n",
    "#from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d2ad2c-565e-4686-9654-2efe510905c3",
   "metadata": {},
   "source": [
    "# 3. Textual Encoder (BERT-based) and Image Encoder (CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a82bc2-0e9f-4185-9fab-cc494aaa8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT tokenizer and model for textual encoding\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7fd9ac-2d92-4d5e-b740-c64f6d3dcf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CLIP processor and model for image encoding\n",
    "#clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "#clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75556716-2437-448d-9dab-e50a8c57c709",
   "metadata": {},
   "source": [
    "# 4. Multimodal Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84105e0-28e2-401f-b7f6-4ef61bb8065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class CrossAttentionLayer(nn.Module):\n",
    "#    def __init__(self, hidden_size, num_attention_heads):\n",
    "#        super(CrossAttentionLayer, self).__init__()\n",
    "#        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_attention_heads)\n",
    "#        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "#\n",
    "#    def forward(self, text_features, image_features):\n",
    "#        text_features = text_features.unsqueeze(0)\n",
    "#        image_features = image_features.unsqueeze(0)\n",
    "#\n",
    "#        # Perform attention between text and image features\n",
    "#        attn_output_text, _ = self.attention(text_features, image_features, image_features)\n",
    "#        attn_output_image, _ = self.attention(image_features, text_features, text_features)\n",
    "#        \n",
    "#        combined_output = attn_output_text + attn_output_image\n",
    "#        combined_output = self.fc(combined_output)\n",
    "#        return combined_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191c70b-a83f-4a52-beae-4ed3aed06481",
   "metadata": {},
   "source": [
    "# 5. Entity Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca8f125-cbfb-4604-a820-d0cfd8926d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class EntityDisambiguationHead(nn.Module):\n",
    " #   def __init__(self, hidden_size, num_candidates):\n",
    "  #      super(EntityDisambiguationHead, self).__init__()\n",
    "   #     self.fc1 = nn.Linear(hidden_size, 512)\n",
    "    #    self.fc2 = nn.Linear(512, num_candidates)\n",
    "     #   self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    #def forward(self, features):\n",
    "     #   x = F.relu(self.fc1(features))\n",
    "      #  x = self.fc2(x)\n",
    "       # return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8286d5c1-0574-47ea-a71a-23a1b9612093",
   "metadata": {},
   "source": [
    "# 6. Multimodal Entity Linking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1dd38e0-d1eb-457e-b60a-287d27368cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MultimodalEntityLinkingModel(nn.Module):\n",
    "#    def __init__(self, hidden_size, num_attention_heads, num_candidates):\n",
    "#        super(MultimodalEntityLinkingModel, self).__init__()\n",
    "#        self.text_encoder = bert_model\n",
    "#        self.image_encoder = clip_model\n",
    "#        self.cross_attention_layer = CrossAttentionLayer(hidden_size, num_attention_heads)\n",
    "#        self.disambiguation_head = EntityDisambiguationHead(hidden_size, num_candidates)\n",
    "#\n",
    "#    def forward(self, text_input, image_input):\n",
    "#        # Textual feature extraction using BERT\n",
    "#        encoded_input = tokenizer(text_input, return_tensors='pt', padding=True, truncation=True)\n",
    "#        text_output = self.text_encoder(**encoded_input).last_hidden_state  # shape: (batch_size, seq_len, hidden_size)\n",
    "#        \n",
    "#        # Image feature extraction using CLIP\n",
    "#        inputs = clip_processor(text=[text_input], images=image_input, return_tensors=\"pt\", padding=True)\n",
    "#        outputs = self.image_encoder(**inputs)\n",
    "#        image_features = outputs.image_embeds  # shape: (batch_size, embedding_dim)\n",
    "#\n",
    "#        # Apply cross-attention to fuse text and image features\n",
    "#        combined_features = self.cross_attention_layer(text_output, image_features)\n",
    "\n",
    "#        # Disambiguate and predict the entity\n",
    "#        entity_scores = self.disambiguation_head(combined_features.squeeze(0))  # Removing batch dimension\n",
    "#        return entity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4684801-e81d-45e7-b9de-225290da4e22",
   "metadata": {},
   "source": [
    "# 7. Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e8afa-bdfb-4659-8b2f-7b7b7c76819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class WikiDiverseDataset(Dataset):\n",
    "#    def __init__(self, data, tokenizer, clip_processor):\n",
    "#        self.data = data\n",
    "#        self.tokenizer = tokenizer\n",
    "#        self.clip_processor = clip_processor\n",
    "#\n",
    "#    def __len__(self):\n",
    "#        return len(self.data)\n",
    "#\n",
    "#    def __getitem__(self, idx):\n",
    "#        sample = self.data[idx]\n",
    "#        text = sample['text']\n",
    "#        image_path = sample['image_path']\n",
    "#        label = sample['label']\n",
    "#\n",
    "#        # Process text\n",
    "#        encoded_text = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "#\n",
    "        # Process image\n",
    " #       image = Image.open(image_path).convert(\"RGB\")\n",
    " #       processed_image = self.clip_processor(images=image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "\n",
    " #       return encoded_text, processed_image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d7e63-038d-45d2-83e6-81f47da3a52e",
   "metadata": {},
   "source": [
    "# 8. Training and Validation Function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f9adcf-1025-4314-99ce-29c9c9d4346b",
   "metadata": {},
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        encoded_text, processed_image, labels = batch\n",
    "        encoded_text = {key: val.squeeze(0).to(device) for key, val in encoded_text.items()}\n",
    "        processed_image = processed_image.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        entity_scores = model(encoded_text, processed_image)\n",
    "        loss = criterion(entity_scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predictions = torch.argmax(entity_scores, dim=1)\n",
    "        accuracy = (predictions == labels).float().mean()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "\n",
    "    return total_loss / len(dataloader), total_accuracy / len(dataloader)\n",
    "\n",
    "def validate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            encoded_text, processed_image, labels = batch\n",
    "            encoded_text = {key: val.squeeze(0).to(device) for key, val in encoded_text.items()}\n",
    "            processed_image = processed_image.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            entity_scores = model(encoded_text, processed_image)\n",
    "            loss = criterion(entity_scores, labels)\n",
    "\n",
    "            predictions = torch.argmax(entity_scores, dim=1)\n",
    "            accuracy = (predictions == labels).float().mean()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy.item()\n",
    "\n",
    "    return total_loss / len(dataloader), total_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ce7b6-0d08-48b8-9600-369000902b0a",
   "metadata": {},
   "source": [
    "# 9. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9109784-ca83-4d49-927b-9ae10636e472",
   "metadata": {},
   "source": [
    "hidden_size = 768\n",
    "num_attention_heads = 8\n",
    "num_candidates = 10\n",
    "num_epochs = 5\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_dataset = WikiDiverseDataset(train_data, tokenizer, clip_processor)\n",
    "val_dataset = WikiDiverseDataset(val_data, tokenizer, clip_processor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "model = MultimodalEntityLinkingModel(hidden_size, num_attention_heads, num_candidates).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    train_loss, train_acc = train_model(model, train_dataloader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = validate_model(model, val_dataloader, criterion, device)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "    lr_scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
