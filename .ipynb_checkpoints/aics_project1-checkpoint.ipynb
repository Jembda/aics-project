{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6795e1f2-d955-408e-b706-a330fd891e5d",
   "metadata": {},
   "source": [
    "# 1. Overview\n",
    "This implementation is a multimodal entity linking system that leverages both text and image information to perform entity disambiguation. The model uses BERT for text encoding and CLIP for image encoding. A cross-attention mechanism fuses text and image features, and a disambiguation head predicts the correct entity from a list of candidates. The system includes training and validation loops to optimize the model.\n",
    "# Key Components\n",
    "-Text Encoder: BERT-based textual feature extraction\n",
    "- Image Encoder: CLIP-based image feature extraction\n",
    "- Cross-Attention: Fuse text and image representations\n",
    "- Entity Disambiguation Head: Predict the correct entity\n",
    "- Training and Validation: Dataset handling, loss computation, and optimizer setup\n",
    "# Data Requiremnt (each sample in the data includes) \n",
    "- 'text': The textual description or context\n",
    "- 'image_path': Path to the corresponding image file\n",
    "- 'label': Index of the correct entity in the candidate list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9a43b-505c-4844-b073-416c2b69fb1e",
   "metadata": {},
   "source": [
    "# 2. Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d926c67e-1539-49dd-bcb0-3fe29896cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel, get_scheduler\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d2ad2c-565e-4686-9654-2efe510905c3",
   "metadata": {},
   "source": [
    "# 3. Textual Encoder (BERT-based) and Image Encoder (CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f070ed74-a68d-4a1b-8ece-b44f870ab507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT tokenizer and model for textual encoding\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7fd9ac-2d92-4d5e-b740-c64f6d3dcf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CLIP processor and model for image encoding\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75556716-2437-448d-9dab-e50a8c57c709",
   "metadata": {},
   "source": [
    "# 4. Multimodal Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84105e0-28e2-401f-b7f6-4ef61bb8065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_attention_heads)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, text_features, image_features):\n",
    "        text_features = text_features.unsqueeze(0)\n",
    "        image_features = image_features.unsqueeze(0)\n",
    "\n",
    "        # Perform attention between text and image features\n",
    "        attn_output_text, _ = self.attention(text_features, image_features, image_features)\n",
    "        attn_output_image, _ = self.attention(image_features, text_features, text_features)\n",
    "        \n",
    "        combined_output = attn_output_text + attn_output_image\n",
    "        combined_output = self.fc(combined_output)\n",
    "        return combined_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191c70b-a83f-4a52-beae-4ed3aed06481",
   "metadata": {},
   "source": [
    "# 5. Entity Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca8f125-cbfb-4604-a820-d0cfd8926d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDisambiguationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_candidates):\n",
    "        super(EntityDisambiguationHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_candidates)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = F.relu(self.fc1(features))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8286d5c1-0574-47ea-a71a-23a1b9612093",
   "metadata": {},
   "source": [
    "# 6. Multimodal Entity Linking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1dd38e0-d1eb-457e-b60a-287d27368cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalEntityLinkingModel(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, num_candidates):\n",
    "        super(MultimodalEntityLinkingModel, self).__init__()\n",
    "        self.text_encoder = bert_model\n",
    "        self.image_encoder = clip_model\n",
    "        self.cross_attention_layer = CrossAttentionLayer(hidden_size, num_attention_heads)\n",
    "        self.disambiguation_head = EntityDisambiguationHead(hidden_size, num_candidates)\n",
    "\n",
    "    def forward(self, text_input, image_input):\n",
    "        # Textual feature extraction using BERT\n",
    "        encoded_input = tokenizer(text_input, return_tensors='pt', padding=True, truncation=True)\n",
    "        text_output = self.text_encoder(**encoded_input).last_hidden_state  # shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Image feature extraction using CLIP\n",
    "        inputs = clip_processor(text=[text_input], images=image_input, return_tensors=\"pt\", padding=True)\n",
    "        outputs = self.image_encoder(**inputs)\n",
    "        image_features = outputs.image_embeds  # shape: (batch_size, embedding_dim)\n",
    "\n",
    "        # Apply cross-attention to fuse text and image features\n",
    "        combined_features = self.cross_attention_layer(text_output, image_features)\n",
    "\n",
    "        # Disambiguate and predict the entity\n",
    "        entity_scores = self.disambiguation_head(combined_features.squeeze(0))  # Removing batch dimension\n",
    "        return entity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4684801-e81d-45e7-b9de-225290da4e22",
   "metadata": {},
   "source": [
    "# 7. Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e8afa-bdfb-4659-8b2f-7b7b7c76819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDiverseDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, clip_processor):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.clip_processor = clip_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        text = sample['text']\n",
    "        image_path = sample['image_path']\n",
    "        label = sample['label']\n",
    "\n",
    "        # Process text\n",
    "        encoded_text = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        # Process image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        processed_image = self.clip_processor(images=image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "\n",
    "        return encoded_text, processed_image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d7e63-038d-45d2-83e6-81f47da3a52e",
   "metadata": {},
   "source": [
    "# 8. Training and Validation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be0ec1-ff8c-490e-9a5e-bf232075291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        encoded_text, processed_image, labels = batch\n",
    "        encoded_text = {key: val.squeeze(0).to(device) for key, val in encoded_text.items()}\n",
    "        processed_image = processed_image.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        entity_scores = model(encoded_text, processed_image)\n",
    "        loss = criterion(entity_scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predictions = torch.argmax(entity_scores, dim=1)\n",
    "        accuracy = (predictions == labels).float().mean()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "\n",
    "    return total_loss / len(dataloader), total_accuracy / len(dataloader)\n",
    "\n",
    "def validate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            encoded_text, processed_image, labels = batch\n",
    "            encoded_text = {key: val.squeeze(0).to(device) for key, val in encoded_text.items()}\n",
    "            processed_image = processed_image.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            entity_scores = model(encoded_text, processed_image)\n",
    "            loss = criterion(entity_scores, labels)\n",
    "\n",
    "            predictions = torch.argmax(entity_scores, dim=1)\n",
    "            accuracy = (predictions == labels).float().mean()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy.item()\n",
    "\n",
    "    return total_loss / len(dataloader), total_accuracy / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ce7b6-0d08-48b8-9600-369000902b0a",
   "metadata": {},
   "source": [
    "# 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617cd1f5-6527-4652-ad53-c3139b1d2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 768\n",
    "num_attention_heads = 8\n",
    "num_candidates = 10\n",
    "num_epochs = 5\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_dataset = WikiDiverseDataset(train_data, tokenizer, clip_processor)\n",
    "val_dataset = WikiDiverseDataset(val_data, tokenizer, clip_processor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "model = MultimodalEntityLinkingModel(hidden_size, num_attention_heads, num_candidates).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    train_loss, train_acc = train_model(model, train_dataloader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = validate_model(model, val_dataloader, criterion, device)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "    lr_scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
