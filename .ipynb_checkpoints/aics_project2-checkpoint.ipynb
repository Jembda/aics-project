{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9597cbf7-2d3b-4ea3-af54-bf145e8e2a0f",
   "metadata": {},
   "source": [
    "# Overview \n",
    "An implementation of the WikiDiverse entity linking system using attention mechanisms, thereby leveraging a multimodal attention-based approach to fuse both text and image information for entity disambiguation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd3d75-c7ac-4810-bd67-ba95267e4d02",
   "metadata": {},
   "source": [
    "# 1. Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea53da2-9a17-4813-a644-05e621999b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel\n",
    "import torch.nn.functional as F\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8822bf08-dd76-4451-8d30-2f1dd0290ee7",
   "metadata": {},
   "source": [
    "# 2. Textual Encoder (BERT-based) \n",
    "* BERT will process the textual input, including the context and entity descriptions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e13c83-0261-4ffc-8dc3-0ef160c8d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input text\n",
    "input_text = \"The Lions versus the Packers (2007).\"\n",
    "encoded_input = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Get the output of BERT (last hidden state)\n",
    "text_output = bert_model(**encoded_input).last_hidden_state  # shape: (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4de8a-0298-49af-8bb6-391e717ff18b",
   "metadata": {},
   "source": [
    "# 3. Image Encoder (CLIP or ResNet)\n",
    "*We can extract image features using a pretrained model like CLIP or ResNet, which can take both text and images as input and project them into a shared space.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a607db-1a72-435c-a193-0e9bfefc6957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CLIP processor and model\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Preprocess and extract image features\n",
    "image = PIL.Image.open(\"path_to_image.jpg\")\n",
    "inputs = clip_processor(text=[\"The Lions versus the Packers (2007).\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Extract image and text features\n",
    "outputs = clip_model(**inputs)\n",
    "image_features = outputs.image_embeds  # shape: (batch_size, embedding_dim)\n",
    "text_features = outputs.text_embeds  # shape: (batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd35cc5-70a9-4eee-842c-caae4d35fcb3",
   "metadata": {},
   "source": [
    "# 4. Multimodal Attention Layer\n",
    "* The core of multimodal fusion will be the cross-attention mechanism. Cross-attention will help the model attend to relevant parts of the text and image, guiding the prediction towards the correct entity. * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e032588-3b73-417a-bb1a-08c21d873fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads):\n",
    "        super(MultimodalAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_attention_heads)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, text_features, image_features):\n",
    "        # Cross attention between text and image features\n",
    "        # Attention of text features to image features (or vice versa)\n",
    "        text_features = text_features.unsqueeze(0)  # Add batch dimension\n",
    "        image_features = image_features.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Cross-attention: text to image (or reverse)\n",
    "        attn_output, attn_weights = self.attention(text_features, image_features, image_features)\n",
    "\n",
    "        # Pass through a feedforward layer\n",
    "        attn_output = self.fc(attn_output)\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "# Initialize attention layer\n",
    "multimodal_attention = MultimodalAttention(hidden_size=768, num_attention_heads=8)\n",
    "\n",
    "# Apply attention to the text and image features\n",
    "attn_output, attn_weights = multimodal_attention(text_features, image_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23d942-1e4b-4395-ac2f-0de1f84c104f",
   "metadata": {},
   "source": [
    "# 5. Final Entity Disambiguation\n",
    "*After obtaining the fused multimodal features from attention, we use a simple MLP (multilayer perceptron) or a classification head to output the predicted entity from the candidate list.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947dedd6-c4d9-4bde-beb4-df66c9bb569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDisambiguationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_candidates):\n",
    "        super(EntityDisambiguationHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_candidates)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = F.relu(self.fc1(features))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Initialize the entity disambiguation head\n",
    "num_candidates = 10  # Example: 10 candidates\n",
    "disambiguation_head = EntityDisambiguationHead(hidden_size=768, num_candidates=num_candidates)\n",
    "\n",
    "# Predict the entity using the multimodal features\n",
    "entity_scores = disambiguation_head(attn_output.squeeze(0))  # Squeeze batch dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59cceb2-8eda-4b22-8fce-f51cd435af4e",
   "metadata": {},
   "source": [
    "# 6. Loss and Optimization\n",
    "*For training, use cross-entropy loss to optimize the model for entity disambiguation.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dd3af8-6c0f-4dbf-b2d0-41f6fea215cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss for entity disambiguation\n",
    "labels = torch.tensor([correct_entity_index])  # The index of the correct entity in the candidate list\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(entity_scores, labels)\n",
    "\n",
    "# Backpropagate and update the model\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c859ee1-4552-4ff1-8500-40846e1464df",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f509fe0-0246-4718-ad97-68081df17c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel\n",
    "import torch.nn.functional as F\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize BERT tokenizer and model for textual encoding\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize CLIP processor and model for image encoding\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Cross-Attention Layer\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_attention_heads)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, text_features, image_features):\n",
    "        # Cross-attention: text to image and image to text\n",
    "        text_features = text_features.unsqueeze(0)  # Add batch dimension\n",
    "        image_features = image_features.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Perform attention between text and image features\n",
    "        attn_output_text, _ = self.attention(text_features, image_features, image_features)\n",
    "        attn_output_image, _ = self.attention(image_features, text_features, text_features)\n",
    "        \n",
    "        # Combine the outputs (you can experiment with different strategies like sum, concat, etc.)\n",
    "        combined_output = attn_output_text + attn_output_image\n",
    "        combined_output = self.fc(combined_output)  # Feedforward layer after attention\n",
    "        return combined_output\n",
    "\n",
    "# Entity Disambiguation Head\n",
    "class EntityDisambiguationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_candidates):\n",
    "        super(EntityDisambiguationHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_candidates)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = F.relu(self.fc1(features))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Model combining Text and Image features\n",
    "class MultimodalEntityLinkingModel(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, num_candidates):\n",
    "        super(MultimodalEntityLinkingModel, self).__init__()\n",
    "        self.text_encoder = bert_model\n",
    "        self.image_encoder = clip_model\n",
    "        self.cross_attention_layer = CrossAttentionLayer(hidden_size, num_attention_heads)\n",
    "        self.disambiguation_head = EntityDisambiguationHead(hidden_size, num_candidates)\n",
    "\n",
    "    def forward(self, text_input, image_input):\n",
    "        # Textual feature extraction using BERT\n",
    "        encoded_input = tokenizer(text_input, return_tensors='pt', padding=True, truncation=True)\n",
    "        text_output = self.text_encoder(**encoded_input).last_hidden_state  # shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Image feature extraction using CLIP\n",
    "        inputs = clip_processor(text=[text_input], images=image_input, return_tensors=\"pt\", padding=True)\n",
    "        outputs = self.image_encoder(**inputs)\n",
    "        image_features = outputs.image_embeds  # shape: (batch_size, embedding_dim)\n",
    "\n",
    "        # Apply cross-attention to fuse text and image features\n",
    "        combined_features = self.cross_attention_layer(text_output, image_features)\n",
    "\n",
    "        # Disambiguate and predict the entity\n",
    "        entity_scores = self.disambiguation_head(combined_features.squeeze(0))  # Removing batch dimension\n",
    "        return entity_scores\n",
    "\n",
    "# Helper function for calculating loss and accuracy\n",
    "def calculate_loss_and_accuracy(model, text_input, image_input, correct_entity_index):\n",
    "    # Forward pass through the model\n",
    "    entity_scores = model(text_input, image_input)\n",
    "\n",
    "    # Compute the cross-entropy loss for entity disambiguation\n",
    "    labels = torch.tensor([correct_entity_index])  # The index of the correct entity in the candidate list\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(entity_scores, labels)\n",
    "    \n",
    "    # Get the predicted entity\n",
    "    predicted_entity = torch.argmax(entity_scores, dim=1)\n",
    "    accuracy = (predicted_entity == labels).float().mean()\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "# Example Usage\n",
    "# Initialize model parameters\n",
    "hidden_size = 768  # BERT hidden size\n",
    "num_attention_heads = 8\n",
    "num_candidates = 10  # Number of candidates for entity linking\n",
    "\n",
    "# Initialize the model\n",
    "model = MultimodalEntityLinkingModel(hidden_size, num_attention_heads, num_candidates)\n",
    "\n",
    "# Example input (text and image)\n",
    "text_input = \"The Lions versus the Packers (2007).\"\n",
    "image_path = \"path_to_image.jpg\"\n",
    "image_input = Image.open(image_path)\n",
    "\n",
    "# Assume we have the index of the correct entity (for example purposes)\n",
    "correct_entity_index = 0\n",
    "\n",
    "# Calculate loss and accuracy\n",
    "loss, accuracy = calculate_loss_and_accuracy(model, text_input, image_input, correct_entity_index)\n",
    "print(f\"Loss: {loss.item()}, Accuracy: {accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b3cbc-3a56-4fcc-9e37-ced8637b515f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8c2e59-240d-4441-a3e2-927a75cab345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1996,  7212,  6431,  1996, 15285,  1006,  2289,  1007,  1012,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example text (from the WikiDiverse dataset)\n",
    "text_input = \"The Lions versus the Packers (2007).\"\n",
    "\n",
    "# Tokenize the text input\n",
    "encoded_input = tokenizer(text_input, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Output the tokenized input\n",
    "print(encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d4bf54-6fca-4f03-8bfc-7d723770e22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve image. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Path to store images\n",
    "DATASET_PATH = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_data\\images\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    os.makedirs(DATASET_PATH)\n",
    "\n",
    "# Function to download and process the image\n",
    "def download_image(url):\n",
    "    try:\n",
    "        # Get the image content\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            m_img = url.split('/')[-1]\n",
    "            \n",
    "            # Create a unique file name using MD5 hash\n",
    "            prefix = hashlib.md5(m_img.encode()).hexdigest()\n",
    "            suffix = re.sub(r'(\\S+(?=\\.(jpg|JPG|png|PNG|svg|SVG)))|(\\S+(?=\\.(jpeg|JPEG)))', '', m_img)\n",
    "            \n",
    "            # Construct the file path for the image\n",
    "            file_path = os.path.join(DATASET_PATH, prefix + suffix)\n",
    "            file_path = file_path.replace('.svg', '.png').replace('.SVG', '.png')  # Replace .svg with .png\n",
    "\n",
    "            # Open the image and save it\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "            image.save(file_path)\n",
    "\n",
    "            print(f\"Image saved at {file_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve image. Status code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "\n",
    "# Example usage with data (replace 'data' with the actual dataset)\n",
    "data = [\n",
    "    [\"The Lions versus the Packers (2007).\", \"https://upload.wikimedia.org/wikipedia/commons/0/06/DetroitLionsRunningPlay-2007.jpg\", \"sports\", [\n",
    "        [\"Lions\", \"Organization\", 4, 9, \"https://en.wikipedia.org/wiki/Detroit_Lions\"],\n",
    "        [\"Packers\", \"Organization\", 21, 28, \"https://en.wikipedia.org/wiki/Green_Bay_Packers\"]\n",
    "    ]]\n",
    "]\n",
    "\n",
    "# Iterate over the data and download images\n",
    "for item in data:\n",
    "    image_url = item[1]  # Get the image URL (second element in the data)\n",
    "    download_image(image_url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8bad0-711e-4c5f-93fa-36e42e6083c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
