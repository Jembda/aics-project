{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e675b7-0cb5-4e4c-b3b5-daac76e9c3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a74ce-1b13-4a01-86fb-c794222f40af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ab991-0410-40b2-979b-285b25d79372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "DATASET_PATH = r\"C:\\Users\\Min Dator\\aics_project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf790d-2988-46f2-adc5-29e07ad6532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "EMBED_DIM = 256\n",
    "VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6654d1-2b02-4590-9aea-549eb67cad0d",
   "metadata": {},
   "source": [
    "# 1. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61addbb2-bd0d-4417-895c-f3c322d2ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDiverseDataset(Dataset):\n",
    "    def __init__(self, image_paths, captions, labels, tokenizer, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.captions = captions\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        text = self.tokenizer(self.captions[idx], truncation=True, padding=\"max_length\", max_length=100, return_tensors=\"pt\")\n",
    "        text_input = text[\"input_ids\"].squeeze(0)\n",
    "        return image, text_input, torch.tensor(self.labels[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f08be-9e68-424c-8d9d-7e041ad0fd38",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818fda59-b205-4ed5-9711-8bc1c50c786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [os.path.join(DATASET_PATH, f\"image_{i}.jpg\") for i in range(100)]\n",
    "captions = [f\"This is a caption for image {i}.\" for i in range(100)]\n",
    "labels = [1 if i % 2 == 0 else 0 for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283788f-291f-4e50-be93-083d81bf20b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819bc25f-24fe-4305-83c4-223fba965f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_paths, test_paths, train_captions, test_captions, train_labels, test_labels = train_test_split(\n",
    "    image_paths, captions, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979b674-c30d-4c75-a328-0058e75d8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a97a89-407c-4374-8aa2-e8a6ae436435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and Dataloaders\n",
    "train_dataset = WikiDiverseDataset(train_paths, train_captions, train_labels, tokenizer, transform)\n",
    "test_dataset = WikiDiverseDataset(test_paths, test_captions, test_labels, tokenizer, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ffb90-8535-4c2b-934b-31824fe09b5f",
   "metadata": {},
   "source": [
    "# 2. Cross Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f0cb8-d7a1-44af-a73c-429ae911a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, dropout=0.1):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return self.layer_norm(query + attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a766e-7cc7-4576-8b62-8ee454449173",
   "metadata": {},
   "source": [
    "# 3. Sub-Networks with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a7dc1-704b-4b82-9442-ae0dce51b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSubNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=4):\n",
    "        super(ImageSubNetworkWithAttention, self).__init__()\n",
    "        base_model = models.resnet50(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.cross_attention = CrossAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x, text_features):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.cross_attention(x.unsqueeze(1), text_features, text_features).squeeze(1)\n",
    "        return x\n",
    "\n",
    "class TextSubNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_heads=4):\n",
    "        super(TextSubNetworkWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.cross_attention = CrossAttention(hidden_dim, num_heads)\n",
    "\n",
    "    def forward(self, x, image_features):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        hidden = self.cross_attention(hidden.unsqueeze(1), image_features, image_features).squeeze(1)\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c2600-ddd7-4201-9d87-91ea08ceac9f",
   "metadata": {},
   "source": [
    "# 4. Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9c6b4-965f-4e31-ab71-05721ce44f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkWithCrossAttention(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SiameseNetworkWithCrossAttention, self).__init__()\n",
    "        self.image_net = ImageSubNetworkWithAttention()\n",
    "        self.text_net = TextSubNetworkWithAttention(vocab_size)\n",
    "\n",
    "    def forward(self, img1, img2, text1, text2):\n",
    "        text_features1 = self.text_net(text1, None)\n",
    "        text_features2 = self.text_net(text2, None)\n",
    "        \n",
    "        img_embedding1 = self.image_net(img1, text_features1)\n",
    "        img_embedding2 = self.image_net(img2, text_features2)\n",
    "\n",
    "        text_embedding1 = self.text_net(text1, img_embedding1)\n",
    "        text_embedding2 = self.text_net(text2, img_embedding2)\n",
    "\n",
    "        combined_embedding1 = torch.cat([img_embedding1, text_embedding1], dim=1)\n",
    "        combined_embedding2 = torch.cat([img_embedding2, text_embedding2], dim=1)\n",
    "        \n",
    "        return combined_embedding1, combined_embedding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f12e7b-4cbf-4b5e-b23d-3e974e4449b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive Loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                          label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591da554-b12f-41c7-a23a-c3a140c6e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetworkWithCrossAttention(VOCAB_SIZE).to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for img1, text1, label in train_loader:\n",
    "        img1, text1, label = img1.to(device), text1.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(img1, img1, text1, text1)\n",
    "        loss = criterion(output1, output2, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for img1, text1, label in test_loader:\n",
    "            img1, text1, label = img1.to(device), text1.to(device), label.to(device)\n",
    "            output1, output2 = model(img1, img1, text1, text1)\n",
    "            val_loss += criterion(output1, output2, label).item()\n",
    "            preds = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    val_losses.append(val_loss / len(test_loader))\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure()\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, EPOCHS + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
