{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460ecd19-ebc4-41b6-b478-eadd09ab5a0b",
   "metadata": {},
   "source": [
    "# 1. An overview\n",
    "Siamese networks consist of two identical sub-networks that share weights and learn to compute the similarity between two input samples. The goal is to learn embeddings such that similar inputs are close in the embedding space, while dissimilar inputs are far apart. For the WikiDiverse dataset, where we have image-caption pairs, we can build a Siamese network that processes text and image data (or just one modality like text or image) and learns to compute similarity between two entities from the knowledge base.\n",
    "* Siamese Network Structure: Two identical sub-networks that compute embeddings for input pairs and learn their similarity\n",
    "* Application: For WikiDiverse, compute similarity between image-caption pairs to link knowledge-base entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f83670-d3fa-4b66-8812-7d0ac5124c79",
   "metadata": {},
   "source": [
    "# 2. Libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "709d7f57-37cd-4ba9-be35-c6a7f80be707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd73e6a-5a16-4ab8-b1c0-e7520f6fc775",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing \n",
    "Assuming we have preprocessed text and image data, we need to encode both image and text inputs for the Siamese network. We will first tokenize and pad the text, and then use a pretrained ResNet50 model (for example) for feature extraction from the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad910af-b2ca-4801-a817-4e3711d1040e",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a67f93f-3800-488e-b7b7-ac412064add6",
   "metadata": {},
   "source": [
    "#Text Processing: Tokenize and pad text to ensure uniform input dimensions.\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "text_sequences = tokenizer.texts_to_sequences(text_data)\n",
    "text_input = pad_sequences(text_sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7cb61bd-963a-4670-9c56-6cf72d06b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_text(text, max_length=100):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoding['input_ids'].squeeze(0), encoding['attention_mask'].squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46ec61-cb05-476b-aa40-41b9b4deea73",
   "metadata": {},
   "source": [
    "# Image processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a122f2f-86cb-47ca-adb6-e32d5a5cdddc",
   "metadata": {},
   "source": [
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224)) \n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "image_input = np.array([preprocess_image(img_path) for img_path in image_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e266682-1a48-4529-8a01-eb54feefbb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = transform(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9939d15-509b-4b42-afca-3c49eef6e511",
   "metadata": {},
   "source": [
    "# 4. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007eab11-581c-4d99-9201-6b83a97cbac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDiverseDataset(Dataset):\n",
    "    def __init__(self, data, img_dir, max_length=100):\n",
    "        self.data = data  # List of image-caption pairs\n",
    "        self.img_dir = img_dir\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        img_path = item['image_path']  # Path to the image\n",
    "        caption = item['caption']  # Text caption\n",
    "        label = item['label']  # 1 for similar, 0 for dissimilar pairs\n",
    "\n",
    "        # Preprocess image and text\n",
    "        img = preprocess_image(img_path)\n",
    "        input_ids, attention_mask = tokenize_text(caption, self.max_length)\n",
    "\n",
    "        return img, input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db1695-105a-4ea3-b486-2159125a8ec7",
   "metadata": {},
   "source": [
    "# 5. Siamese Sub-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c8c881-b400-4513-b5c9-54155aac7d67",
   "metadata": {},
   "source": [
    "# Image Sub-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85cd2964-230a-43a0-9da1-cefa2d27f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim=256):\n",
    "        super(ImageEmbedding, self).__init__()\n",
    "        base_model = models.resnet50(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])  # Remove the final classification layer\n",
    "        self.fc = nn.Linear(base_model.fc.in_features, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a5a3d-1b3f-49e9-ab44-f896a19e3b0c",
   "metadata": {},
   "source": [
    "# Text Sub-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60bd0513-54ce-461d-95c6-e14e3822c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim=256):\n",
    "        super(TextEmbedding, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(768, embedding_dim)  # BERT outputs 768-dimensional embeddings\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token representation\n",
    "        x = self.fc(cls_token)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04babb3-c6c7-4f25-8547-5ad4815d61ce",
   "metadata": {},
   "source": [
    "# 6. Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b586dfb6-f739-448c-9873-8979b5756f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim=256):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.image_embedding = ImageEmbedding(embedding_dim)\n",
    "        self.text_embedding = TextEmbedding(embedding_dim)\n",
    "\n",
    "    def forward(self, img1, img2, text1, mask1, text2, mask2):\n",
    "        img_emb1 = self.image_embedding(img1)\n",
    "        img_emb2 = self.image_embedding(img2)\n",
    "        text_emb1 = self.text_embedding(text1, mask1)\n",
    "        text_emb2 = self.text_embedding(text2, mask2)\n",
    "\n",
    "        combined_emb1 = torch.cat((img_emb1, text_emb1), dim=1)\n",
    "        combined_emb2 = torch.cat((img_emb2, text_emb2), dim=1)\n",
    "\n",
    "        return combined_emb1, combined_emb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1346d61-aef4-482e-8392-660110b86986",
   "metadata": {},
   "source": [
    "# 7. Contrastive Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47de1e86-5bc7-452a-b5fd-96afddee95ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, emb1, emb2, label):\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(emb1, emb2)\n",
    "        loss = (label * torch.square(euclidean_distance)) + \\\n",
    "               ((1 - label) * torch.square(torch.clamp(self.margin - euclidean_distance, min=0.0)))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6de4bc-3364-4ba0-8567-8b71de42cf8e",
   "metadata": {},
   "source": [
    "# 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e23b6c3-2bf5-4fc0-8374-ae4c7180bb2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2397490576.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Initialize dataset and dataloader\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataset and dataloader\n",
    "dataset = WikiDiverseDataset(data, img_dir='path_to_images')\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SiameseNetwork(embedding_dim=256)\n",
    "model = model.cuda()  # Move to GPU if available\n",
    "criterion = ContrastiveLoss(margin=1.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for img1, input_ids1, mask1, label in dataloader:\n",
    "        # Load second input from dataset (img2, text2)\n",
    "        img2, input_ids2, mask2, _ = dataloader.dataset[np.random.randint(len(dataset))]\n",
    "\n",
    "        # Move data to GPU\n",
    "        img1, img2 = img1.cuda(), img2.cuda()\n",
    "        input_ids1, mask1, input_ids2, mask2 = input_ids1.cuda(), mask1.cuda(), input_ids2.cuda(), mask2.cuda()\n",
    "        label = label.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        emb1, emb2 = model(img1, img2, input_ids1, mask1, input_ids2, mask2)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(emb1, emb2, label)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb72c0-9ed4-4038-9bd2-31704942ddaa",
   "metadata": {},
   "source": [
    "# 9. Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b5fe95-0579-4d2c-afb7-bccd33585316",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for img1, input_ids1, mask1, label in test_dataloader:\n",
    "        img2, input_ids2, mask2, _ = test_dataloader.dataset[np.random.randint(len(test_dataset))]\n",
    "\n",
    "        img1, img2 = img1.cuda(), img2.cuda()\n",
    "        input_ids1, mask1, input_ids2, mask2 = input_ids1.cuda(), mask1.cuda(), input_ids2.cuda(), mask2.cuda()\n",
    "        label = label.cuda()\n",
    "\n",
    "        emb1, emb2 = model(img1, img2, input_ids1, mask1, input_ids2, mask2)\n",
    "        loss = criterion(emb1, emb2, label)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "print(f\"Test Loss: {total_loss/len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7384d-458a-4f91-8d3c-2a1f500e6440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8643da-03a8-49f3-b2aa-8f9b43f16991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff99e44-5d92-46c2-8b16-b8d742984978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6c74a3f-6f04-4a8b-b436-4dc9def52340",
   "metadata": {},
   "source": [
    "# 1. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2990a4-b0e6-4317-a539-2efe91e3bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202113e-7203-42a5-be8e-a208fae05afd",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e68a3-bd6d-45c7-b4bf-3e2e3d55f87c",
   "metadata": {},
   "source": [
    "# Image Preprocessing\n",
    "We use a standard ResNet50 model for extracting features from images. Images are resized and normalized before feeding into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89784fd9-da1b-4cd1-884e-1e9a525fa35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    return image_transforms(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed34590-cac9-41ac-8fab-1e60dcc7a56f",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "We use a pretrained BERT tokenizer to tokenize and encode text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc5e8c-43a5-45f8-8a24-a1052c95fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess_text(text, max_length=100):\n",
    "    encoded = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    return encoded['input_ids'].squeeze(0), encoded['attention_mask'].squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd1b0e-b221-4127-81e8-21b7fd731155",
   "metadata": {},
   "source": [
    "# 3. Sub-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a375e4-6499-402f-9334-c7093060a27c",
   "metadata": {},
   "source": [
    "# Image Embedding Network\n",
    "This network uses ResNet50 to extract image features and reduces them to a fixed-size embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d6b54e-bd76-41c0-8fb0-616687e33ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim=256):\n",
    "        super(ImageEmbedding, self).__init__()\n",
    "        base_model = models.resnet50(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])  # Remove the final classification layer\n",
    "        self.fc = nn.Linear(2048, embedding_dim)  # Reduce to embedding dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        embedding = self.fc(features)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6536c75-3542-48a1-a924-e258e1ee0888",
   "metadata": {},
   "source": [
    "# Text Embedding Network\n",
    "This network uses BERT to extract text embeddings, followed by a linear layer to reduce dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f75e7-274f-491d-bac1-532c9be91c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim=256):\n",
    "        super(TextEmbedding, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(768, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.pooler_output  # CLS token representation\n",
    "        embedding = self.fc(cls_embedding)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4415da-9688-4169-bafe-c43d3d6c3436",
   "metadata": {},
   "source": [
    "# 4. Cross-Attention Mechanism\n",
    "The cross-attention mechanism allows one modality (e.g., text) to attend to another (e.g., image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5f9ff-1dac-4e61-ba67-02bccd82e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        output = self.fc(attn_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1798c9-1094-4d93-8680-406dac63c7e6",
   "metadata": {},
   "source": [
    "# 5. Siamese Network with Cross-Attention\n",
    "This combines image and text embeddings using cross-attention and computes similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441e460-4ad7-4fd9-a8f7-04020ff51f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkWithCrossAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, num_heads=4):\n",
    "        super(SiameseNetworkWithCrossAttention, self).__init__()\n",
    "        self.image_embedding = ImageEmbedding(embedding_dim)\n",
    "        self.text_embedding = TextEmbedding(embedding_dim)\n",
    "        self.cross_attention = CrossAttention(embed_dim=embedding_dim, num_heads=num_heads)\n",
    "        self.fc = nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "\n",
    "    def forward(self, img1, img2, text1, mask1, text2, mask2):\n",
    "        # Extract embeddings\n",
    "        img_emb1 = self.image_embedding(img1)\n",
    "        img_emb2 = self.image_embedding(img2)\n",
    "        text_emb1 = self.text_embedding(text1, mask1)\n",
    "        text_emb2 = self.text_embedding(text2, mask2)\n",
    "\n",
    "        # Apply cross-attention\n",
    "        img_text_emb1 = self.cross_attention(text_emb1.unsqueeze(1), img_emb1.unsqueeze(1), img_emb1.unsqueeze(1)).squeeze(1)\n",
    "        img_text_emb2 = self.cross_attention(text_emb2.unsqueeze(1), img_emb2.unsqueeze(1), img_emb2.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Combine embeddings\n",
    "        combined_emb1 = torch.cat((img_emb1, img_text_emb1), dim=1)\n",
    "        combined_emb2 = torch.cat((img_emb2, img_text_emb2), dim=1)\n",
    "\n",
    "        # Reduce to a single embedding\n",
    "        combined_emb1 = self.fc(combined_emb1)\n",
    "        combined_emb2 = self.fc(combined_emb2)\n",
    "\n",
    "        return combined_emb1, combined_emb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc563fcc-a7c9-41e4-b0d6-947f17dc1540",
   "metadata": {},
   "source": [
    "# 6. Loss Function\n",
    "The contrastive loss function encourages similar pairs to have closer embeddings and dissimilar pairs to have distant embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f654429-233b-4a0c-b136-ae7a62634e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, emb1, emb2, label):\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(emb1, emb2)\n",
    "        loss = (label * torch.square(euclidean_distance)) + \\\n",
    "               ((1 - label) * torch.square(torch.clamp(self.margin - euclidean_distance, min=0.0)))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aeb33e-1479-4232-8431-caa405090e06",
   "metadata": {},
   "source": [
    "# 7. Training and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dde937-78e2-49e7-8357-10c6d7f5a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "model = SiameseNetworkWithCrossAttention()\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for img1, img2, text1, mask1, text2, mask2, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        emb1, emb2 = model(img1, img2, text1, mask1, text2, mask2)\n",
    "        loss = criterion(emb1, emb2, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79752cb-e8a3-4165-aa71-2f34806b8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img1, img2, text1, mask1, text2, mask2, labels in test_loader:\n",
    "        emb1, emb2 = model(img1, img2, text1, mask1, text2, mask2)\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(emb1, emb2)\n",
    "        # Evaluate accuracy, precision, recall, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78025f4-f342-4f8f-a6f6-f1dc357888f7",
   "metadata": {},
   "source": [
    "# 8. DataLoader Example\n",
    "Prepare the dataset and dataloader for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c42978-83e9-4529-aa7c-0c6646320b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDiverseDataset(Dataset):\n",
    "    def __init__(self, image_paths, texts, labels, tokenizer, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        img = preprocess_image(img_path)\n",
    "        input_ids, attention_mask = preprocess_text(text)\n",
    "\n",
    "        return img, input_ids, attention_mask, label\n",
    "\n",
    "# Usage\n",
    "train_dataset = WikiDiverseDataset(train_image_paths, train_texts, train_labels, tokenizer, image_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
