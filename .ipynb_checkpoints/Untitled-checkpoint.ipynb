{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459d5e7d-2284-4ded-ab16-7d3fad3a7b05",
   "metadata": {},
   "source": [
    "The WikiDiverse dataset offers a comprehensive and rich set of multimodal information for entity linking and related tasks. Below are steps and strategies for effectively using the dataset in your project.\n",
    "Dataset Structure\n",
    "Passage Level:\n",
    "\n",
    "Contains an image URL, a textual description (passage), and annotated entities with:\n",
    "Entity mention text.\n",
    "Entity type (e.g., organization, person).\n",
    "Mention positions in the passage.\n",
    "Wikipedia URL for the entity.\n",
    "Mention Level:\n",
    "\n",
    "Focuses on specific mentions within a sentence, with additional context:\n",
    "Left and right contexts.\n",
    "Mention type and candidates.\n",
    "Topic category.\n",
    "Entity Level:\n",
    "\n",
    "Provides Wikipedia-based descriptions, images, and entity-level annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9578a2-e8c8-4bb6-9f32-0252548327f1",
   "metadata": {},
   "source": [
    "# 1. Image Preprocessing\n",
    "   * Images are retrieved from the URLs provided in the dataset.\n",
    "   Fetching Images: Use the hashing function provided in the dataset documentation to locate the\n",
    "   images in your local storage or download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d555346c-92a3-40bf-8331-ef88c781f3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_to_wikinewsImgs/062ce5e341a566a4208d801e53557538.jpg\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import re\n",
    "import os\n",
    "\n",
    "def get_image_path(url, local_dir=\"path_to_wikinewsImgs\"):\n",
    "    img_name = url.split('/')[-1]\n",
    "    prefix = hashlib.md5(img_name.encode()).hexdigest()\n",
    "    suffix = re.sub(r'(\\S+(?=\\.(jpg|JPG|png|PNG|svg|SVG)))|(\\S+(?=\\.(jpeg|JPEG)))', '', img_name)\n",
    "    file_name = prefix + suffix\n",
    "    file_name = file_name.replace('.svg', '.png').replace('.SVG', '.png')\n",
    "    return os.path.join(local_dir, file_name)\n",
    "\n",
    "# Example usage\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/0/06/DetroitLionsRunningPlay-2007.jpg\"\n",
    "local_path = get_image_path(url)\n",
    "print(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92843d8d-04ca-4387-85a2-ad21e4278b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image already exists at path_to_wikinewsImgs/062ce5e341a566a4208d801e53557538.jpg\n",
      "Preprocessed image shape: (224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import re\n",
    "import requests\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Function to generate the file path\n",
    "def get_image_path(url, local_dir=\"path_to_wikinewsImgs\"):\n",
    "    os.makedirs(local_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    img_name = url.split('/')[-1]\n",
    "    prefix = hashlib.md5(img_name.encode()).hexdigest()\n",
    "    suffix = re.sub(r'(\\S+(?=\\.(jpg|JPG|png|PNG|svg|SVG)))|(\\S+(?=\\.(jpeg|JPEG)))', '', img_name)\n",
    "    file_name = prefix + suffix\n",
    "    file_name = file_name.replace('.svg', '.png').replace('.SVG', '.png')\n",
    "    return os.path.join(local_dir, file_name)\n",
    "\n",
    "# Function to preprocess image\n",
    "def preprocess_image(img_path, target_size=(224, 224)):\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array\n",
    "\n",
    "# Example usage\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/0/06/DetroitLionsRunningPlay-2007.jpg\"\n",
    "local_path = get_image_path(url)\n",
    "\n",
    "# Step 2: Download the image\n",
    "if not os.path.exists(local_path):  # Check if the file already exists\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        with open(local_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Image saved at {local_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download image. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(f\"Image already exists at {local_path}\")\n",
    "\n",
    "# Step 3: Preprocess the image\n",
    "try:\n",
    "    img_array = preprocess_image(local_path)\n",
    "    print(f\"Preprocessed image shape: {img_array.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d61f95-846d-427e-8bcc-4793ea584137",
   "metadata": {},
   "source": [
    "# Preprocessing Images: Convert the images into tensors, resize them to a fixed size, and normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ae4bed1-1c44-4df0-8907-3eb25d3b4ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def preprocess_image(img_path, target_size=(224, 224)):\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5880822-2797-49a3-91e4-3b6e9ae487ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e880fb8-99dd-419d-a286-1b8dffe38ed1",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing\n",
    "   Textual information includes both the passage and mention-level details. Use tokenization and\n",
    "   padding for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbed4586-ca99-4049-be63-2aff26986b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def preprocess_text(texts, max_len=100, vocab_size=10000):\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<UNK>\")\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "    return padded_sequences, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da22cc7-8301-4d16-b0fe-9ebdf6259d57",
   "metadata": {},
   "source": [
    "# 3.  Dataset Parsing\n",
    "    Parse the dataset and split it into inputs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "652a91fa-f60f-4d12-a11f-fd69ea95e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_passage_level(data_path):\n",
    "    with open(data_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    images, texts, entities = [], [], []\n",
    "    for item in data:\n",
    "        text = item[0]\n",
    "        image_url = item[1]\n",
    "        entity_annotations = item[3]\n",
    "\n",
    "        images.append(get_image_path(image_url))\n",
    "        texts.append(text)\n",
    "        entities.append(entity_annotations)\n",
    "\n",
    "    return images, texts, entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54fb36-5bd4-41d4-85bf-d6886f9b22cb",
   "metadata": {},
   "source": [
    "# For mention-level data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3d5e1e8-3bf0-455a-adc9-88424c1b0db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mention_level(data_path):\n",
    "    with open(data_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    mentions, contexts, topics = [], [], []\n",
    "    for item in data:\n",
    "        text = item[0]\n",
    "        mention = item[2]\n",
    "        left_context = item[4]\n",
    "        right_context = item[5]\n",
    "        topic = item[8]\n",
    "\n",
    "        contexts.append((left_context, text, right_context))\n",
    "        mentions.append(mention)\n",
    "        topics.append(topic)\n",
    "\n",
    "    return contexts, mentions, topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6b8d3-57de-461f-bec3-6d1b3ecd64ef",
   "metadata": {},
   "source": [
    "# 4. Pair Generation for Training\n",
    "   Generate positive and negative pairs of inputs for the Siamese network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "877bbf65-a5b4-4fd2-b94f-3d09f2c0a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_pairs(images, texts, labels, num_pairs=1000):\n",
    "    pairs_image1, pairs_image2 = [], []\n",
    "    pairs_text1, pairs_text2 = [], []\n",
    "    pair_labels = []\n",
    "\n",
    "    for _ in range(num_pairs):\n",
    "        # Positive pair\n",
    "        idx = random.randint(0, len(images) - 1)\n",
    "        pairs_image1.append(images[idx])\n",
    "        pairs_text1.append(texts[idx])\n",
    "        pairs_image2.append(images[idx])\n",
    "        pairs_text2.append(texts[idx])\n",
    "        pair_labels.append(1)\n",
    "\n",
    "        # Negative pair\n",
    "        idx1, idx2 = random.sample(range(len(images)), 2)\n",
    "        pairs_image1.append(images[idx1])\n",
    "        pairs_text1.append(texts[idx1])\n",
    "        pairs_image2.append(images[idx2])\n",
    "        pairs_text2.append(texts[idx2])\n",
    "        pair_labels.append(0)\n",
    "\n",
    "    return pairs_image1, pairs_image2, pairs_text1, pairs_text2, pair_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423e598d-5d77-4cdb-8040-b877e0f18e15",
   "metadata": {},
   "source": [
    "# Training the Siamese Network: Use the generated pairs to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4be64-a67f-406b-b019-2ec4e135948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we have preprocessed image and text inputs\n",
    "X_image1, X_image2 = preprocess_images(pairs_image1), preprocess_images(pairs_image2)\n",
    "X_text1, X_text2 = preprocess_texts(pairs_text1), preprocess_texts(pairs_text2)\n",
    "\n",
    "siamese_model.fit(\n",
    "    [X_image1, X_image2, X_text1, X_text2], \n",
    "    pair_labels, \n",
    "    batch_size=32, \n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a2384-d564-4fad-a9d5-bc24c9d1108b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0d024f3-0e4d-4a98-a90a-74315f93aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Flatten, Dropout, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Lambda\n",
    "import tensorflow as tf\n",
    "\n",
    "# Image Branch\n",
    "def build_image_branch(input_shape=(224, 224, 3)):\n",
    "    base_model = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze pre-trained layers\n",
    "\n",
    "    x = Flatten()(base_model.output)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "\n",
    "    return Model(base_model.input, x, name=\"ImageBranch\")\n",
    "\n",
    "# Text Branch\n",
    "def build_text_branch(vocab_size, max_len, embedding_dim=128):\n",
    "    text_input = Input(shape=(max_len,), name=\"TextInput\")\n",
    "    x = Embedding(vocab_size, embedding_dim, input_length=max_len)(text_input)\n",
    "    x = LSTM(128, return_sequences=False)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    return Model(text_input, x, name=\"TextBranch\")\n",
    "\n",
    "# Similarity Layer\n",
    "def cosine_similarity(vectors):\n",
    "    x, y = vectors\n",
    "    x = tf.math.l2_normalize(x, axis=1)\n",
    "    y = tf.math.l2_normalize(y, axis=1)\n",
    "    return tf.reduce_sum(x * y, axis=1, keepdims=True)\n",
    "\n",
    "def euclidean_distance(vectors):\n",
    "    x, y = vectors\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "# Full Siamese Network\n",
    "def build_siamese_network(vocab_size, max_len, input_shape=(224, 224, 3)):\n",
    "    # Inputs\n",
    "    img1_input = Input(shape=input_shape, name=\"Image1Input\")\n",
    "    img2_input = Input(shape=input_shape, name=\"Image2Input\")\n",
    "    text1_input = Input(shape=(max_len,), name=\"Text1Input\")\n",
    "    text2_input = Input(shape=(max_len,), name=\"Text2Input\")\n",
    "\n",
    "    # Branches\n",
    "    image_branch = build_image_branch(input_shape)\n",
    "    text_branch = build_text_branch(vocab_size, max_len)\n",
    "\n",
    "    # Extract Features\n",
    "    img1_features = image_branch(img1_input)\n",
    "    img2_features = image_branch(img2_input)\n",
    "    text1_features = text_branch(text1_input)\n",
    "    text2_features = text_branch(text2_input)\n",
    "\n",
    "    # Concatenate Features\n",
    "    combined_features1 = tf.concat([img1_features, text1_features], axis=-1)\n",
    "    combined_features2 = tf.concat([img2_features, text2_features], axis=-1)\n",
    "\n",
    "    # Distance Calculation\n",
    "    similarity = Lambda(cosine_similarity, name=\"CosineSimilarity\")([combined_features1, combined_features2])\n",
    "\n",
    "    # Final Model\n",
    "    model = Model(inputs=[img1_input, img2_input, text1_input, text2_input], outputs=similarity, name=\"SiameseNetwork\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d0efda-319d-46b9-97ef-b88b565990c0",
   "metadata": {},
   "source": [
    "# The recised version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d845e9e-53f1-42dd-be66-73799dc5f2d4",
   "metadata": {},
   "source": [
    "# 1. Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7773300-c830-449b-9cf0-e04e8270085b",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataset\n",
    "data = {\n",
    "    \"image_path1\": [\"path_to_image1.jpg\", \"path_to_image2.jpg\"],\n",
    "    \"image_path2\": [\"path_to_image3.jpg\", \"path_to_image4.jpg\"],\n",
    "    \"text1\": [\"This is a sample text 1\", \"Another example text 1\"],\n",
    "    \"text2\": [\"This is a sample text 2\", \"Another example text 2\"],\n",
    "    \"label\": [1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the dataset\n",
    "df.to_csv(r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Sample dataset created.\")\n",
    "\n",
    "\n",
    "# Load WikiDiverse Dataset\n",
    "# Assuming the dataset has columns: ['image_path1', 'image_path2', 'text1', 'text2', 'label']\n",
    "data = pd.read_csv(r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_dataset.csv\")\n",
    "\n",
    "\n",
    "# Extract columns\n",
    "image_paths1 = data['image_path1'].values\n",
    "image_paths2 = data['image_path2'].values\n",
    "text_data1 = data['text1'].values\n",
    "text_data2 = data['text2'].values\n",
    "labels = data['label'].values\n",
    "\n",
    "# Tokenize and pad text\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(np.concatenate([text_data1, text_data2]))\n",
    "\n",
    "text_sequences1 = tokenizer.texts_to_sequences(text_data1)\n",
    "text_sequences2 = tokenizer.texts_to_sequences(text_data2)\n",
    "\n",
    "text_input1 = pad_sequences(text_sequences1, maxlen=max_sequence_length)\n",
    "text_input2 = pad_sequences(text_sequences2, maxlen=max_sequence_length)\n",
    "\n",
    "# Preprocess images\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = load_img(img_path, target_size=(224, 224))\n",
    "    img_array = img_to_array(img)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "image_input1 = np.array([preprocess_image(img) for img in image_paths1])\n",
    "image_input2 = np.array([preprocess_image(img) for img in image_paths2])\n",
    "\n",
    "# Train-Test Split\n",
    "X_train_image1, X_test_image1, X_train_image2, X_test_image2, \\\n",
    "X_train_text1, X_test_text1, X_train_text2, X_test_text2, \\\n",
    "y_train, y_test = train_test_split(\n",
    "    image_input1, image_input2, text_input1, text_input2, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2b1d94-83a6-4181-b23f-2d398711e91d",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Create a sample dataset\n",
    "data = {\n",
    "    \"image_path1\": [\"path_to_image1.jpg\", \"path_to_image2.jpg\"],\n",
    "    \"image_path2\": [\"path_to_image3.jpg\", \"path_to_image4.jpg\"],\n",
    "    \"text1\": [\"This is a sample text 1\", \"Another example text 1\"],\n",
    "    \"text2\": [\"This is a sample text 2\", \"Another example text 2\"],\n",
    "    \"label\": [1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the dataset\n",
    "dataset_path = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_dataset.csv\"\n",
    "df.to_csv(dataset_path, index=False)\n",
    "print(\"Sample dataset created.\")\n",
    "\n",
    "# Load WikiDiverse Dataset\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Extract columns\n",
    "image_paths1 = data['image_path1'].values\n",
    "image_paths2 = data['image_path2'].values\n",
    "text_data1 = data['text1'].values\n",
    "text_data2 = data['text2'].values\n",
    "labels = data['label'].values\n",
    "\n",
    "# Tokenize and pad text\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(np.concatenate([text_data1, text_data2]))\n",
    "\n",
    "text_sequences1 = tokenizer.texts_to_sequences(text_data1)\n",
    "text_sequences2 = tokenizer.texts_to_sequences(text_data2)\n",
    "\n",
    "text_input1 = pad_sequences(text_sequences1, maxlen=max_sequence_length)\n",
    "text_input2 = pad_sequences(text_sequences2, maxlen=max_sequence_length)\n",
    "\n",
    "# Generate random image arrays for testing\n",
    "def generate_random_image_array():\n",
    "    return preprocess_input(np.random.rand(224, 224, 3).astype(np.float32) * 255)\n",
    "\n",
    "image_input1 = np.array([generate_random_image_array() for _ in image_paths1])\n",
    "image_input2 = np.array([generate_random_image_array() for _ in image_paths2])\n",
    "\n",
    "# Train-Test Split\n",
    "X_train_image1, X_test_image1, X_train_image2, X_test_image2, \\\n",
    "X_train_text1, X_test_text1, X_train_text2, X_test_text2, \\\n",
    "y_train, y_test = train_test_split(\n",
    "    image_input1, image_input2, text_input1, text_input2, labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Preprocessing completed. Dataset is ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04262c6-1559-43a0-912e-247648433fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Expanded dataset creation\n",
    "data = {\n",
    "    \"image_path1\": [f\"path_to_image_{i}.jpg\" for i in range(1, 21)],\n",
    "    \"image_path2\": [f\"path_to_image_{i+20}.jpg\" for i in range(1, 21)],\n",
    "    \"text1\": [f\"This is a sample text {i}\" for i in range(1, 21)],\n",
    "    \"text2\": [f\"This is another sample text {i}\" for i in range(1, 21)],\n",
    "    \"label\": [1 if i % 2 == 0 else 0 for i in range(1, 21)]  # Alternating labels\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "dataset_path = r\"C:\\Users\\Min Dator\\aics-project\\wikidiverse_dataset.csv\"\n",
    "df.to_csv(dataset_path, index=False)\n",
    "print(\"Dataset saved to:\", dataset_path)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Extract columns\n",
    "image_paths1 = data['image_path1'].values\n",
    "image_paths2 = data['image_path2'].values\n",
    "text_data1 = data['text1'].values\n",
    "text_data2 = data['text2'].values\n",
    "labels = data['label'].values\n",
    "\n",
    "# Tokenize and pad text\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(np.concatenate([text_data1, text_data2]))\n",
    "\n",
    "text_sequences1 = tokenizer.texts_to_sequences(text_data1)\n",
    "text_sequences2 = tokenizer.texts_to_sequences(text_data2)\n",
    "\n",
    "text_input1 = pad_sequences(text_sequences1, maxlen=max_sequence_length)\n",
    "text_input2 = pad_sequences(text_sequences2, maxlen=max_sequence_length)\n",
    "\n",
    "# Image preprocessing function\n",
    "def preprocess_image(img_path):\n",
    "    # Replace this placeholder logic with real image file handling if available\n",
    "    img = np.random.rand(224, 224, 3)  # Simulate random image\n",
    "    img_array = img_to_array(img)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "image_input1 = np.array([preprocess_image(img) for img in image_paths1])\n",
    "image_input2 = np.array([preprocess_image(img) for img in image_paths2])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_image1, X_test_image1, X_train_image2, X_test_image2, \\\n",
    "X_train_text1, X_test_text1, X_train_text2, X_test_text2, \\\n",
    "y_train, y_test = train_test_split(\n",
    "    image_input1, image_input2, text_input1, text_input2, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Preprocessing complete. Training and test data prepared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112b4614-9151-4614-ba6e-85bdd450d78b",
   "metadata": {},
   "source": [
    "# 2. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "552c9373-62a7-45cf-9114-0cb8f6640c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# Image Sub-Network\n",
    "def create_image_model(input_shape=(224, 224, 3)):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    return models.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Text Sub-Network\n",
    "def create_text_model(input_shape=(max_sequence_length,)):\n",
    "    input_text = layers.Input(shape=input_shape)\n",
    "    x = layers.Embedding(input_dim=10000, output_dim=128, input_length=max_sequence_length)(input_text)\n",
    "    x = layers.LSTM(256, return_sequences=True)(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)  # Global max pooling for compact representation\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    return models.Model(inputs=input_text, outputs=x)\n",
    "\n",
    "# Cross-Attention Layer\n",
    "def cross_attention_layer(image_embedding, text_embedding):\n",
    "    \"\"\"\n",
    "    Cross-attention mechanism where text guides the focus on the image embedding.\n",
    "    \"\"\"\n",
    "    text_expanded = tf.expand_dims(text_embedding, axis=1)  # Expand for broadcasting\n",
    "    image_expanded = tf.expand_dims(image_embedding, axis=2)\n",
    "    \n",
    "    # Attention scores\n",
    "    attention_scores = tf.matmul(image_expanded, text_expanded)  # Shape: (batch_size, image_dim, text_dim)\n",
    "    attention_weights = tf.nn.softmax(attention_scores, axis=1)\n",
    "    \n",
    "    # Weighted sum\n",
    "    attended_image = tf.matmul(attention_weights, text_embedding)\n",
    "    return layers.Flatten()(attended_image)\n",
    "\n",
    "# Siamese Network\n",
    "def create_siamese_network(image_shape=(224, 224, 3), text_shape=(max_sequence_length,)):\n",
    "    # Image and Text Models\n",
    "    image_model = create_image_model(input_shape=image_shape)\n",
    "    text_model = create_text_model(input_shape=text_shape)\n",
    "    \n",
    "    # Inputs\n",
    "    input_image1 = layers.Input(shape=image_shape)\n",
    "    input_image2 = layers.Input(shape=image_shape)\n",
    "    input_text1 = layers.Input(shape=text_shape)\n",
    "    input_text2 = layers.Input(shape=text_shape)\n",
    "    \n",
    "    # Get Embeddings\n",
    "    image_embedding1 = image_model(input_image1)\n",
    "    image_embedding2 = image_model(input_image2)\n",
    "    text_embedding1 = text_model(input_text1)\n",
    "    text_embedding2 = text_model(input_text2)\n",
    "    \n",
    "    # Apply Cross-Attention\n",
    "    cross_embedding1 = cross_attention_layer(image_embedding1, text_embedding1)\n",
    "    cross_embedding2 = cross_attention_layer(image_embedding2, text_embedding2)\n",
    "    \n",
    "    # Combine Embeddings\n",
    "    combined_embedding1 = layers.concatenate([cross_embedding1, text_embedding1])\n",
    "    combined_embedding2 = layers.concatenate([cross_embedding2, text_embedding2])\n",
    "    \n",
    "    # Compute Similarity\n",
    "    distance = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([combined_embedding1, combined_embedding2])\n",
    "    output = layers.Dense(1, activation='sigmoid')(distance)\n",
    "    \n",
    "    # Build Model\n",
    "    model = models.Model(inputs=[input_image1, input_image2, input_text1, input_text2], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01927e0-5e16-42a2-8310-c671b4dfb137",
   "metadata": {},
   "source": [
    "# 3. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d509c688-ec7b-4a5f-b54f-ed607c18a482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 07:28:08.023787: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 07:28:16.122640: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-01-02 07:28:16.126308: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-01-02 07:28:16.128411: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-01-02 07:28:16.972269: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-01-02 07:28:16.974606: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-01-02 07:28:16.976783: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-01-02 07:28:17.139265: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-01-02 07:28:17.140741: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-01-02 07:28:17.141973: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.1`. Either provide more data, or a different value for the `validation_split` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m siamese_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m), loss\u001b[38;5;241m=\u001b[39mcontrastive_loss, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train Model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m siamese_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     16\u001b[0m     [X_train_image1, X_train_image2, X_train_text1, X_train_text2], y_train,\n\u001b[1;32m     17\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Evaluate Model\u001b[39;00m\n\u001b[1;32m     21\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m siamese_model\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m     22\u001b[0m     [X_test_image1, X_test_image2, X_test_text1, X_test_text2], y_test\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/engine/data_adapter.py:1687\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1684\u001b[0m split_at \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mfloor(batch_dim \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m validation_split)))\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_at \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m split_at \u001b[38;5;241m==\u001b[39m batch_dim:\n\u001b[0;32m-> 1687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1688\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data contains \u001b[39m\u001b[38;5;132;01m{batch_dim}\u001b[39;00m\u001b[38;5;124m samples, which is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1689\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msufficient to split it into a validation and training set as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1690\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified by `validation_split=\u001b[39m\u001b[38;5;132;01m{validation_split}\u001b[39;00m\u001b[38;5;124m`. Either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1691\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide more data, or a different value for the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1692\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_split` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1693\u001b[0m             batch_dim\u001b[38;5;241m=\u001b[39mbatch_dim, validation_split\u001b[38;5;241m=\u001b[39mvalidation_split\n\u001b[1;32m   1694\u001b[0m         )\n\u001b[1;32m   1695\u001b[0m     )\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split\u001b[39m(t, start, end):\n\u001b[1;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.1`. Either provide more data, or a different value for the `validation_split` argument."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Loss Function\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    square_pred = tf.square(y_pred)\n",
    "    square_true = tf.square(y_true)\n",
    "    loss = (y_true * square_pred) + ((1 - y_true) * tf.maximum(0.0, margin - tf.sqrt(square_pred + 1e-6))**2)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Create and Compile Model\n",
    "siamese_model = create_siamese_network()\n",
    "siamese_model.compile(optimizer=Adam(learning_rate=1e-4), loss=contrastive_loss, metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "history = siamese_model.fit(\n",
    "    [X_train_image1, X_train_image2, X_train_text1, X_train_text2], y_train,\n",
    "    batch_size=32, epochs=10, validation_split=0.1\n",
    ")\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_accuracy = siamese_model.evaluate(\n",
    "    [X_test_image1, X_test_image2, X_test_text1, X_test_text2], y_test\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a102a0-154a-45a5-9301-8dcaac39c297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
