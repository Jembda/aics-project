{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8faf1a-49d1-4274-8287-a3615bc9aeeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b3a51c3-4302-42f4-987d-d4d704664041",
   "metadata": {},
   "source": [
    "# 1 An overview\n",
    "Siamese networks consist of two identical sub-networks that share weights and learn to compute the similarity between two input samples. The goal is to learn embeddings such that similar inputs are close in the embedding space, while dissimilar inputs are far apart. \n",
    "For the WikiDiverse dataset, where we have image-caption pairs, we can build a Siamese network that processes text and image data (or just one modality like text or image) and learns to compute similarity between two entities from the knowledge base.\n",
    "* Siamese Network Structure: Two identical sub-networks that compute embeddings for input pairs and learn their similarity\n",
    "* Application: For WikiDiverse, compute similarity between image-caption pairs to link knowledge-base entities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a164a71-7186-49d9-847d-2e7114303898",
   "metadata": {},
   "source": [
    "#  2. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4aea479-4acf-4b98-a47d-3111cb7c00c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b30570-8a7c-4dcc-a1fe-5f93b072c7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 16:45:20.904120: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-01 16:45:23.175972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86cf035e-7b6c-4b23-b26e-e546bc990e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3c50a81-2375-4801-a485-04e0318f82b9",
   "metadata": {},
   "source": [
    "# 3 Data Preprocessing\n",
    "Assuming we have preprocessed text and image data, we need to encode both image and text inputs for the Siamese network. We will first tokenize and pad the text, and then use a pretrained ResNet50 model (for example) for feature extraction from the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662692e7-75d1-492c-a35f-b57b360928a5",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac3a8c-eb92-4b6f-9c55-7ecb770ec0d7",
   "metadata": {},
   "source": [
    "#Text Processing: Tokenize and pad text to ensure uniform input dimensions.\n",
    "max_sequence_length = 100  # Maximum length of each text sequence\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(text_data)  # `text_data` is a list of text samples\n",
    "\n",
    "# Convert text to sequences\n",
    "text_sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Pad sequences to make them of uniform length\n",
    "text_input = pad_sequences(text_sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b4d6e8-785a-4b73-a330-b95dd4d66aa4",
   "metadata": {},
   "source": [
    "#Text Processing: Tokenize and pad text to ensure uniform input dimensions.\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "text_sequences = tokenizer.texts_to_sequences(text_data)\n",
    "text_input = pad_sequences(text_sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f39c440d-f8d1-4e14-a519-04fb617b6de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  2\n",
      "   6  7  8  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  2\n",
      "  10 11  1 12]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13  3\n",
      "   4 14 15  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16\n",
      "   3  4 17 18]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define text_data\n",
    "text_data = [\n",
    "    \"This is the first example text.\",\n",
    "    \"Here is another sample text data.\",\n",
    "    \"Machine learning with images and text.\",\n",
    "    \"Deep learning with Siamese networks.\"\n",
    "]\n",
    "\n",
    "# Text preprocessing\n",
    "max_sequence_length = 100  # Maximum length of each text sequence\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(text_data)  # `text_data` is a list of text samples\n",
    "\n",
    "# Convert text to sequences\n",
    "text_sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Pad sequences to make them of uniform length\n",
    "text_input = pad_sequences(text_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "print(text_input)  # Verify the padded sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387f4dd-e682-448a-9cff-9a4e96682df3",
   "metadata": {},
   "source": [
    "# Image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae0e21-2250-4c5d-b61e-b51d8d56db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of image preprocessing using ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224)) \n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "image_input = np.array([preprocess_image(img_path) for img_path in image_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621cf58e-0938-4274-b64f-c7404d67a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pretrained word embading Bert\n",
    "# size of the data?\n",
    "#pytorch instead of tf\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ac4562d-9bf3-4265-b34b-9eff1d0b3f04",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     img_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img_array, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preprocess_input(img_array)\n\u001b[0;32m---> 11\u001b[0m image_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([preprocess_image(img_path) \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m image_data])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Image Processing: Use ResNet50 for feature extraction. \n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "image_input = np.array([preprocess_image(img_path) for img_path in image_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "033f5b75-e54c-4baa-ab51-35fc33b38aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "\n",
    "# Generate random images for testing\n",
    "image_data = [np.random.rand(224, 224, 3) for _ in range(5)]  # Simulate 5 random images\n",
    "\n",
    "# Update preprocess_image to accept arrays directly for testing\n",
    "def preprocess_image(img_array):\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "image_input = np.array([preprocess_image(img) for img in image_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db976e-6c77-4f56-8dc4-af474d46f5e4",
   "metadata": {},
   "source": [
    "# 4. Building Siamese Network \n",
    "The Siamese network consists of two sub-networks that process the two inputs (image or text). We'll use a shared ResNet50 for images and an LSTM-based network for text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e3cca-5210-43be-98e6-45cb79cdf76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image sub-network (using ResNet50 for feature extraction)\n",
    "def create_image_model(input_shape=(224, 224, 3)):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    return models.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Text sub-network (using LSTM for sequence processing)\n",
    "def create_text_model(input_shape=(max_sequence_length,)):\n",
    "    input_text = layers.Input(shape=input_shape)\n",
    "    x = layers.Embedding(input_dim=10000, output_dim=128, input_length=max_sequence_length)(input_text)\n",
    "    x = layers.LSTM(256)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    return models.Model(inputs=input_text, outputs=x)\n",
    "\n",
    "# Shared sub-network for both text and image\n",
    "def create_siamese_network(image_shape=(224, 224, 3), text_shape=(max_sequence_length,)):\n",
    "    # Image model\n",
    "    image_model = create_image_model(input_shape=image_shape)\n",
    "    \n",
    "    # Text model\n",
    "    text_model = create_text_model(input_shape=text_shape)\n",
    "    \n",
    "    # Define inputs for the Siamese network\n",
    "    input_image_1 = layers.Input(shape=image_shape)\n",
    "    input_image_2 = layers.Input(shape=image_shape)\n",
    "    input_text_1 = layers.Input(shape=text_shape)\n",
    "    input_text_2 = layers.Input(shape=text_shape)\n",
    "    \n",
    "    # Get embeddings for both image pairs and text pairs\n",
    "    image_embedding_1 = image_model(input_image_1)\n",
    "    image_embedding_2 = image_model(input_image_2)\n",
    "    text_embedding_1 = text_model(input_text_1)\n",
    "    text_embedding_2 = text_model(input_text_2)\n",
    "    \n",
    "    # Combine the embeddings\n",
    "    combined_embedding_1 = layers.concatenate([image_embedding_1, text_embedding_1])\n",
    "    combined_embedding_2 = layers.concatenate([image_embedding_2, text_embedding_2])\n",
    "    \n",
    "    \n",
    "    ##cross attention will be the image providing baseline model one option for cross attention\n",
    "    ##the text tells you where to look in the image concatination is a reaonable baseline.\n",
    "    \n",
    "    # Calculate the absolute difference between embeddings\n",
    "    distance = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([combined_embedding_1, combined_embedding_2]) #check cosin distance or mean squared ??\n",
    "    \n",
    "    # Output layer with sigmoid activation (similarity score)\n",
    "    output = layers.Dense(1, activation='sigmoid')(distance)\n",
    "    \n",
    "    # Create the model\n",
    "    model = models.Model(inputs=[input_image_1, input_image_2, input_text_1, input_text_2], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a410231-e22b-4d64-b043-87af4d6a4d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-networks:\n",
    "# Image: ResNet50 with a dense embedding layer.\n",
    "# Text: LSTM-based embedding network.\n",
    "\n",
    "# Image sub-network\n",
    "def create_image_model(input_shape=(224, 224, 3)):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    return models.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Text sub-network\n",
    "def create_text_model(input_shape=(max_sequence_length,)):\n",
    "    input_text = layers.Input(shape=input_shape)\n",
    "    x = layers.Embedding(input_dim=10000, output_dim=128, input_length=max_sequence_length)(input_text)\n",
    "    x = layers.LSTM(256)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    return models.Model(inputs=input_text, outputs=x)\n",
    "\n",
    "# Siamese Network\n",
    "def create_siamese_network(image_shape=(224, 224, 3), text_shape=(max_sequence_length,)):\n",
    "    image_model = create_image_model(input_shape=image_shape)\n",
    "    text_model = create_text_model(input_shape=text_shape)\n",
    "    \n",
    "    input_image_1 = layers.Input(shape=image_shape)\n",
    "    input_image_2 = layers.Input(shape=image_shape)\n",
    "    input_text_1 = layers.Input(shape=text_shape)\n",
    "    input_text_2 = layers.Input(shape=text_shape)\n",
    "    \n",
    "    image_embedding_1 = image_model(input_image_1)\n",
    "    image_embedding_2 = image_model(input_image_2)\n",
    "    text_embedding_1 = text_model(input_text_1)\n",
    "    text_embedding_2 = text_model(input_text_2)\n",
    "    \n",
    "    combined_embedding_1 = layers.concatenate([image_embedding_1, text_embedding_1])\n",
    "    combined_embedding_2 = layers.concatenate([image_embedding_2, text_embedding_2])\n",
    "    \n",
    "    distance = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([combined_embedding_1, combined_embedding_2])\n",
    "    output = layers.Dense(1, activation='sigmoid')(distance)\n",
    "    \n",
    "    model = models.Model(inputs=[input_image_1, input_image_2, input_text_1, input_text_2], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf0fd2-5d17-4f6c-abda-272cf2c77fa2",
   "metadata": {},
   "source": [
    "# 5. Loss Function\n",
    "In Siamese networks, a common loss function used is contrastive loss, which minimizes the distance between similar pairs and maximizes the distance for dissimilar pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef97485-3bd9-462b-949f-bed16fd758a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contrastive loss is used to measure similarity:\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    square_pred = tf.square(y_pred)\n",
    "    square_true = tf.square(y_true)\n",
    "    loss = (y_true * square_pred) + ((1 - y_true) * tf.maximum(0.0, margin - tf.sqrt(square_pred + 1e-6))**2)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12595815-4bb8-498e-aad7-f7c6a3c417e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11bfb9a7-cc63-4ca7-9f61-4926d84084a0",
   "metadata": {},
   "source": [
    "# 6. Compiling the Model\n",
    "Compile the model with an optimizer (e.g., Adam) and the contrastive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ca9a4-ca3b-408b-98ff-bbc42074d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = create_siamese_network()\n",
    "siamese_model.compile(optimizer=optimizers.Adam(lr=0.0001), loss=contrastive_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b5769-c0dd-4d75-b8c0-34e0efd2df11",
   "metadata": {},
   "source": [
    "# 7. Training\n",
    "Now that the model is built, we can train it using image-caption pairs from the WikiDiverse dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1f377-0555-4822-82a4-0f1457675ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using the prepared dataset:\n",
    "# X_train_image1, X_train_image2: images for the pair\n",
    "# X_train_text1, X_train_text2: text for the pair\n",
    "# y_train: label (1 for similar, 0 for dissimilar)\n",
    "\n",
    "siamese_model.fit([X_train_image1, X_train_image2, X_train_text1, X_train_text2], y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b3be8-e106-4abe-9778-79aa60b942a3",
   "metadata": {},
   "source": [
    "# 8. Evaluation\n",
    "After training, you can evaluate the model on a test set to check its performance in similarity detection tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c3828-5cdd-4437-a1dd-47ee15f426e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model's performance:\n",
    "siamese_model.evaluate([X_test_image1, X_test_image2, X_test_text1, X_test_text2], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c08ec6-636c-42fd-bd4b-ee6d587a1a12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
