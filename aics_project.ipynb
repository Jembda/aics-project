{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8faf1a-49d1-4274-8287-a3615bc9aeeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b3a51c3-4302-42f4-987d-d4d704664041",
   "metadata": {},
   "source": [
    "# 1 An overview\n",
    "Siamese networks consist of two identical sub-networks that share weights and learn to compute the similarity between two input samples. The goal is to learn embeddings such that similar inputs are close in the embedding space, while dissimilar inputs are far apart. \n",
    "For the WikiDiverse dataset, where we have image-caption pairs, we can build a Siamese network that processes text and image data (or just one modality like text or image) and learns to compute similarity between two entities from the knowledge base. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a164a71-7186-49d9-847d-2e7114303898",
   "metadata": {},
   "source": [
    "#  2. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b30570-8a7c-4dcc-a1fe-5f93b072c7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 10:48:03.224162: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-18 10:48:03.598089: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-18 10:48:05.704963: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c50a81-2375-4801-a485-04e0318f82b9",
   "metadata": {},
   "source": [
    "# 3 Data Preprocessing\n",
    "Assuming you have preprocessed text and image data, we need to encode both image and text inputs for the Siamese network. You will first tokenize and pad the text, and then use a pretrained ResNet50 model (for example) for feature extraction from the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662692e7-75d1-492c-a35f-b57b360928a5",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e4c97-9254-4216-992e-12e78259d3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of text preprocessing using Tokenizer\n",
    "max_sequence_length = 100  # Maximum length of each text sequence\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(text_data)  # `text_data` is a list of text samples\n",
    "\n",
    "# Convert text to sequences\n",
    "text_sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Pad sequences to make them of uniform length\n",
    "text_input = pad_sequences(text_sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387f4dd-e682-448a-9cff-9a4e96682df3",
   "metadata": {},
   "source": [
    "# Image processin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae0e21-2250-4c5d-b61e-b51d8d56db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of image preprocessing using ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "image_input = np.array([preprocess_image(img_path) for img_path in image_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db976e-6c77-4f56-8dc4-af474d46f5e4",
   "metadata": {},
   "source": [
    "# 4. Building Siamese Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e3cca-5210-43be-98e6-45cb79cdf76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image sub-network (using ResNet50 for feature extraction)\n",
    "def create_image_model(input_shape=(224, 224, 3)):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    return models.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Text sub-network (using LSTM for sequence processing)\n",
    "def create_text_model(input_shape=(max_sequence_length,)):\n",
    "    input_text = layers.Input(shape=input_shape)\n",
    "    x = layers.Embedding(input_dim=10000, output_dim=128, input_length=max_sequence_length)(input_text)\n",
    "    x = layers.LSTM(256)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    return models.Model(inputs=input_text, outputs=x)\n",
    "\n",
    "# Shared sub-network for both text and image\n",
    "def create_siamese_network(image_shape=(224, 224, 3), text_shape=(max_sequence_length,)):\n",
    "    # Image model\n",
    "    image_model = create_image_model(input_shape=image_shape)\n",
    "    \n",
    "    # Text model\n",
    "    text_model = create_text_model(input_shape=text_shape)\n",
    "    \n",
    "    # Define inputs for the Siamese network\n",
    "    input_image_1 = layers.Input(shape=image_shape)\n",
    "    input_image_2 = layers.Input(shape=image_shape)\n",
    "    input_text_1 = layers.Input(shape=text_shape)\n",
    "    input_text_2 = layers.Input(shape=text_shape)\n",
    "    \n",
    "    # Get embeddings for both image pairs and text pairs\n",
    "    image_embedding_1 = image_model(input_image_1)\n",
    "    image_embedding_2 = image_model(input_image_2)\n",
    "    text_embedding_1 = text_model(input_text_1)\n",
    "    text_embedding_2 = text_model(input_text_2)\n",
    "    \n",
    "    # Combine the embeddings\n",
    "    combined_embedding_1 = layers.concatenate([image_embedding_1, text_embedding_1])\n",
    "    combined_embedding_2 = layers.concatenate([image_embedding_2, text_embedding_2])\n",
    "    \n",
    "    # Calculate the absolute difference between embeddings\n",
    "    distance = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([combined_embedding_1, combined_embedding_2])\n",
    "    \n",
    "    # Output layer with sigmoid activation (similarity score)\n",
    "    output = layers.Dense(1, activation='sigmoid')(distance)\n",
    "    \n",
    "    # Create the model\n",
    "    model = models.Model(inputs=[input_image_1, input_image_2, input_text_1, input_text_2], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf0fd2-5d17-4f6c-abda-272cf2c77fa2",
   "metadata": {},
   "source": [
    "# 5. Loss Function\n",
    "In Siamese networks, a common loss function used is contrastive loss, which minimizes the distance between similar pairs and maximizes the distance for dissimilar pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef97485-3bd9-462b-949f-bed16fd758a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    square_pred = tf.square(y_pred)\n",
    "    square_true = tf.square(y_true)\n",
    "    loss = (y_true * square_pred) + ((1 - y_true) * tf.maximum(0.0, margin - tf.sqrt(square_pred + 1e-6))**2)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bfb9a7-cc63-4ca7-9f61-4926d84084a0",
   "metadata": {},
   "source": [
    "# 6. Compiling the Model\n",
    "Compile the model with an optimizer (e.g., Adam) and the contrastive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ca9a4-ca3b-408b-98ff-bbc42074d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = create_siamese_network()\n",
    "siamese_model.compile(optimizer=optimizers.Adam(lr=0.0001), loss=contrastive_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b5769-c0dd-4d75-b8c0-34e0efd2df11",
   "metadata": {},
   "source": [
    "# 7. Training\n",
    "Now that the model is built, we can train it using image-caption pairs from the WikiDiverse dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1f377-0555-4822-82a4-0f1457675ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training loop\n",
    "# X_train_image1, X_train_image2: images for the pair\n",
    "# X_train_text1, X_train_text2: text for the pair\n",
    "# y_train: label (1 for similar, 0 for dissimilar)\n",
    "\n",
    "siamese_model.fit([X_train_image1, X_train_image2, X_train_text1, X_train_text2], y_train, batch_size=32, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b3be8-e106-4abe-9778-79aa60b942a3",
   "metadata": {},
   "source": [
    "# 8. Evaluation\n",
    "After training, you can evaluate the model on a test set to check its performance in similarity detection tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c3828-5cdd-4437-a1dd-47ee15f426e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.evaluate([X_test_image1, X_test_image2, X_test_text1, X_test_text2], y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
